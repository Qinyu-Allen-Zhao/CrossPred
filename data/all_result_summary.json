{
    "BLIP2-opt-2.7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.25575392557539256
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.18946047678795483
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.252
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.23579893747445851
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23510204081632652
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2574712643678161
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.27976190476190477
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24320742213386348
        },
        "Instance Location (SEED_2)": {
            "acc": 0.23824130879345604
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.25773195876288657
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.23875870804306523
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2681594756963408
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.12658227848101267
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2196969696969697
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.17764471057884232
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.228310502283105
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23104693140794225
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2512218963831867
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.2719033232628399
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.275
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2389937106918239
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.24489795918367346
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9882352941176471,
            "f1": 0.6640316205533597
        },
        "posters (MME)": {
            "acc": 0.4931972789115646,
            "prec": 0.4965277777777778,
            "rec": 0.9727891156462585,
            "f1": 0.6574712643678161
        },
        "position (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5172413793103449,
            "rec": 1.0,
            "f1": 0.6818181818181819
        },
        "scene (MME)": {
            "acc": 0.5225,
            "prec": 0.5118110236220472,
            "rec": 0.975,
            "f1": 0.6712564543889844
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.4857142857142857,
            "prec": 0.49137931034482757,
            "rec": 0.8142857142857143,
            "f1": 0.6129032258064516
        },
        "artwork (MME)": {
            "acc": 0.535,
            "prec": 0.518324607329843,
            "rec": 0.99,
            "f1": 0.6804123711340206
        },
        "landmark (MME)": {
            "acc": 0.5125,
            "prec": 0.5067024128686327,
            "rec": 0.945,
            "f1": 0.6596858638743455
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "existence (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5714285714285714,
            "rec": 0.6666666666666666,
            "f1": 0.6153846153846153
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.4444444444444444,
            "rec": 0.8,
            "f1": 0.5714285714285714
        },
        "count (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5178571428571429,
            "rec": 0.9666666666666667,
            "f1": 0.6744186046511628
        },
        "color (MME)": {
            "acc": 0.65,
            "prec": 0.6,
            "rec": 0.9,
            "f1": 0.7200000000000001
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "code_reasoning (MME)": {
            "acc": 0.175,
            "prec": 0.25925925925925924,
            "rec": 0.35,
            "f1": 0.29787234042553196
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.2558139534883721
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2634920634920635
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.24615384615384617
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2511415525114155
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.24113475177304963
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.19553072625698323
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.24651162790697675
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2297979797979798
        },
        "ocr (MMBench_CN)": {
            "acc": 0.26282051282051283
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.23049645390070922
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.23
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.23026315789473684
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.26704545454545453
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2978723404255319
        },
        "image_style (MMBench_CN)": {
            "acc": 0.23113207547169812
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2916666666666667
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.29333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.23837209302325582
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.26666666666666666
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3013698630136986
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3191489361702128
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2569832402234637
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.28837209302325584
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.26535626535626533
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.27525252525252525
        },
        "ocr (MMBench_EN)": {
            "acc": 0.2564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2768361581920904
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.23049645390070922
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.245
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.26644736842105265
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.26136363636363635
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.25
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.29924242424242425
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2642857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.1
        },
        "Sociology (MMMU)": {
            "acc": 0.3
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.1
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.2
        },
        "Biology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.23333333333333334
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2336065573770492
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24836601307189543
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "State capitals (ScienceQA)": {
            "acc": 0.22115384615384615
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3972602739726027
        },
        "States of matter (ScienceQA)": {
            "acc": 0.5
        },
        "Materials (ScienceQA)": {
            "acc": 0.3776223776223776
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5510204081632653
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.43902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.21875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Classification (ScienceQA)": {
            "acc": 0.5063291139240507
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.28169014084507044
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.22208121827411167
        },
        "3D Distance (CVBench)": {
            "acc": 0.5066666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5138461538461538
        },
        "3D Depth (CVBench)": {
            "acc": 0.5016666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.260412535667419,
            "bert": 0.0
        },
        "Whole dataset (Enrico)": {
            "bart": -7.309470057487488,
            "bert": 0.01627574026584625
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.535636110305786,
            "bert": 0.9252747267484664
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.5734686040878296,
            "bert": 0.8670031785964966
        },
        "Whole dataset (GQA)": {
            "bart": -4.307566496133805,
            "bert": 0.9199207818508148
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.412515314817429,
            "bert": 0.7787222766876221
        },
        "Whole dataset (INAT)": {
            "bart": -6.920969986915589,
            "bert": 0.4019647043943405
        },
        "Whole dataset (IRFL)": {
            "bart": -7.4715980327129365,
            "bert": 0.10935050308704376
        },
        "Whole dataset (MemeCaps)": {
            "bart": -5.153873705863953,
            "bert": 0.4282817542552948
        },
        "Whole dataset (Memotion)": {
            "bart": -6.015220017433166,
            "bert": 0.5328117144107819
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.891787987947464,
            "bert": 0.6192867404222488
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.660757285356522,
            "bert": 0.9810902166366577
        },
        "Whole dataset (NLVR)": {
            "bart": -6.060981931686402,
            "bert": 0.6775667870044708
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.835421342849731,
            "bert": 0.843681902885437
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.893074027299881,
            "bert": 0.8597021853923797
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.6871817171573635,
            "bert": 0.4739289194345474
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.13075836956501,
            "bert": 0.6043100327253341
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.507622842788696,
            "bert": 0.7866365486383438
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.963022561073303,
            "bert": 0.0
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.300856058597565,
            "bert": 0.7917566466331482
        },
        "Whole dataset (Slake)": {
            "bart": -4.121029111146927,
            "bert": 0.9922815215587616
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.024452531337738,
            "bert": 0.6453436410427094
        },
        "Whole dataset (VCR)": {
            "bart": -3.959841883778572,
            "bert": 0.8766030848026276
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.551739227771759,
            "bert": 0.3730859059095383
        },
        "Whole dataset (VQA)": {
            "bart": -5.128818434476853,
            "bert": 0.7601677787303924
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.7308132314682005,
            "bert": 0.6803729307651519
        },
        "Whole dataset (Winoground)": {
            "bart": -6.518855091333389,
            "bert": 0.6882273417711258
        },
        "random (POPE)": {
            "acc": 0.6482084690553745,
            "prec": 0.625,
            "rec": 0.6756756756756757,
            "f1": 0.6493506493506493
        },
        "popular (POPE)": {
            "acc": 0.7133550488599348,
            "prec": 0.7315436241610739,
            "rec": 0.6942675159235668,
            "f1": 0.7124183006535948
        },
        "adversarial (POPE)": {
            "acc": 0.6888111888111889,
            "prec": 0.6287425149700598,
            "rec": 0.7954545454545454,
            "f1": 0.7023411371237457
        }
    },
    "BLIP2-flan-t5-xxl": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5852871585287158
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5948103792415169
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5257214554579673
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.3153692614770459
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.812
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.3992644053943604
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3346938775510204
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6850574712643678
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23015873015873015
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4585818422796554
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5419222903885481
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7419252691576947
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6466411796832332
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5688622754491018
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4642313546423135
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5992779783393501
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.41935483870967744
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7734138972809668
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.30833333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5723270440251572
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6666666666666666
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6030150753768844
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.40816326530612246
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.6,
            "prec": 0.8695652173913043,
            "rec": 0.23529411764705882,
            "f1": 0.3703703703703704
        },
        "posters (MME)": {
            "acc": 0.8027210884353742,
            "prec": 0.7870967741935484,
            "rec": 0.8299319727891157,
            "f1": 0.8079470198675497
        },
        "position (MME)": {
            "acc": 0.5666666666666667,
            "prec": 0.5833333333333334,
            "rec": 0.4666666666666667,
            "f1": 0.5185185185185186
        },
        "scene (MME)": {
            "acc": 0.82,
            "prec": 0.9507042253521126,
            "rec": 0.675,
            "f1": 0.7894736842105263
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6727272727272727,
            "rec": 0.5285714285714286,
            "f1": 0.5920000000000001
        },
        "artwork (MME)": {
            "acc": 0.7775,
            "prec": 0.7381974248927039,
            "rec": 0.86,
            "f1": 0.7944572748267897
        },
        "landmark (MME)": {
            "acc": 0.715,
            "prec": 0.9056603773584906,
            "rec": 0.48,
            "f1": 0.6274509803921569
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.8333333333333334,
            "prec": 1.0,
            "rec": 0.6666666666666666,
            "f1": 0.8
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.7419354838709677,
            "rec": 0.7666666666666667,
            "f1": 0.7540983606557377
        },
        "color (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7741935483870968,
            "rec": 0.8,
            "f1": 0.7868852459016393
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.59375,
            "rec": 0.95,
            "f1": 0.7307692307692308
        },
        "code_reasoning (MME)": {
            "acc": 0.575,
            "prec": 0.6363636363636364,
            "rec": 0.35,
            "f1": 0.45161290322580644
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.25
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.24444444444444444
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2876712328767123
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2553191489361702
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.24581005586592178
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.21395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.25552825552825553
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2702020202020202
        },
        "ocr (MMBench_CN)": {
            "acc": 0.27564102564102566
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24293785310734464
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3049645390070922
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.26644736842105265
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2727272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_CN)": {
            "acc": 0.3018867924528302
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.24666666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.30714285714285716
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5206349206349207
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5769230769230769
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5388127853881278
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6312056737588653
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6983240223463687
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8964646464646465
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6631205673758865
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.81
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8848684210526315
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6808510638297872
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8632075471698113
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8333333333333334
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.35333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9214285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Math (MMMU)": {
            "acc": 0.5
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3333333333333333
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.5
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.4
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.26666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.20491803278688525
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.0873015873015873
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3411764705882353
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18137254901960784
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9711538461538461
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.8811188811188811
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8775510204081632
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.41935483870967744
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.43902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.8095238095238095
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6724137931034483
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6923076923076923
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.9873417721518988
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.8873239436619719
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.6052631578947368
        },
        "2D Count (CVBench)": {
            "acc": 0.3616751269035533
        },
        "3D Distance (CVBench)": {
            "acc": 0.5
        },
        "2D Relation (CVBench)": {
            "acc": 0.5892307692307692
        },
        "3D Depth (CVBench)": {
            "acc": 0.5
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.266708421707153,
            "bert": 0.710745250582695
        },
        "Whole dataset (Enrico)": {
            "bart": -6.2922413182258605,
            "bert": 0.9435857731103897
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.8688180446624756,
            "bert": 0.9993851798772811
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.470824022293091,
            "bert": 0.8914496111869812
        },
        "Whole dataset (GQA)": {
            "bart": -4.6463653910160065,
            "bert": 0.9933547365665436
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.64659078836441,
            "bert": 0.9825113660097122
        },
        "Whole dataset (INAT)": {
            "bart": -6.4646954917907715,
            "bert": 0.7664094406366349
        },
        "Whole dataset (IRFL)": {
            "bart": -4.261036313772202,
            "bert": 0.9994130575656891
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.876314764022827,
            "bert": 0.8501056832075119
        },
        "Whole dataset (Memotion)": {
            "bart": -4.698782134056091,
            "bert": 0.9015576118230819
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.649250710308552,
            "bert": 0.8845706415176392
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.738089096546173,
            "bert": 0.9968848818540573
        },
        "Whole dataset (NLVR)": {
            "bart": -3.0324696254730226,
            "bert": 0.9995549392700195
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.047919497489929,
            "bert": 0.9996383517980576
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5425145697593687,
            "bert": 0.9222145015001297
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.032153385281563,
            "bert": 0.9542226344347
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.214697129130363,
            "bert": 0.8122423309087753
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.59440492272377,
            "bert": 0.8992161923646926
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.8254222190380096,
            "bert": 0.9530149519443512
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.0364494395256045,
            "bert": 0.8649293261766434
        },
        "Whole dataset (Slake)": {
            "bart": -4.569750102758408,
            "bert": 0.9943889319896698
        },
        "Whole dataset (UCMerced)": {
            "bart": -3.498147349357605,
            "bert": 0.9539522290229797
        },
        "Whole dataset (VCR)": {
            "bart": -2.99621396958828,
            "bert": 0.93396981716156
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6620376920700073,
            "bert": 0.9051890468597412
        },
        "Whole dataset (VQA)": {
            "bart": -5.3718592119216915,
            "bert": 0.9685070604085922
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.449449899196625,
            "bert": 0.9393434423208237
        },
        "Whole dataset (Winoground)": {
            "bart": -4.319196064472198,
            "bert": 0.9976523840427398
        },
        "random (POPE)": {
            "acc": 0.762214983713355,
            "prec": 1.0,
            "rec": 0.5067567567567568,
            "f1": 0.672645739910314
        },
        "popular (POPE)": {
            "acc": 0.7752442996742671,
            "prec": 0.9680851063829787,
            "rec": 0.5796178343949044,
            "f1": 0.7250996015936254
        },
        "adversarial (POPE)": {
            "acc": 0.8041958041958042,
            "prec": 0.9871794871794872,
            "rec": 0.5833333333333334,
            "f1": 0.7333333333333333
        }
    },
    "BLIP2-opt-6.7b-coco": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.2817810281781028
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2440401505646173
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2934131736526946
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.232
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2627707396812423
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.22122448979591836
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.271264367816092
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23511904761904762
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24254473161033796
        },
        "Instance Location (SEED_2)": {
            "acc": 0.2985685071574642
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.32989690721649484
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.26789107029765674
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2659748771163299
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.4936708860759494
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.20454545454545456
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2937595129375951
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2490974729241877
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.28347996089931576
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.23867069486404835
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2909090909090909
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6088235294117647,
            "prec": 0.635036496350365,
            "rec": 0.5117647058823529,
            "f1": 0.5667752442996742
        },
        "posters (MME)": {
            "acc": 0.5544217687074829,
            "prec": 0.5555555555555556,
            "rec": 0.54421768707483,
            "f1": 0.5498281786941581
        },
        "position (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.49122807017543857,
            "rec": 0.9333333333333333,
            "f1": 0.6436781609195402
        },
        "scene (MME)": {
            "acc": 0.635,
            "prec": 0.5894039735099338,
            "rec": 0.89,
            "f1": 0.7091633466135459
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5071428571428571,
            "prec": 0.5052631578947369,
            "rec": 0.6857142857142857,
            "f1": 0.5818181818181819
        },
        "artwork (MME)": {
            "acc": 0.55,
            "prec": 0.5282485875706214,
            "rec": 0.935,
            "f1": 0.6750902527075812
        },
        "landmark (MME)": {
            "acc": 0.565,
            "prec": 0.540625,
            "rec": 0.865,
            "f1": 0.6653846153846154
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.5454545454545454,
            "rec": 0.6,
            "f1": 0.5714285714285713
        },
        "existence (MME)": {
            "acc": 0.75,
            "prec": 0.8571428571428571,
            "rec": 0.6,
            "f1": 0.7058823529411764
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9666666666666667,
            "f1": 0.6590909090909091
        },
        "color (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5192307692307693,
            "rec": 0.9,
            "f1": 0.6585365853658537
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5135135135135135,
            "rec": 0.95,
            "f1": 0.6666666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.425,
            "prec": 0.42857142857142855,
            "rec": 0.45,
            "f1": 0.4390243902439024
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22674418604651161
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.23174603174603176
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2602739726027397
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.26256983240223464
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.25116279069767444
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.26044226044226043
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.23232323232323232
        },
        "ocr (MMBench_CN)": {
            "acc": 0.1794871794871795
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24858757062146894
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.24113475177304963
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2565789473684211
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.23863636363636365
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.2358490566037736
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.26666666666666666
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.24285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.25
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.25396825396825395
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2923076923076923
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3013698630136986
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3262411347517731
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2569832402234637
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.24186046511627907
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.26515151515151514
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2978723404255319
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2631578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2840909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_EN)": {
            "acc": 0.24528301886792453
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2878787878787879
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Math (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.2
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.16666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.22950819672131148
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.20915032679738563
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.26136363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.410958904109589
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.2564102564102564
        },
        "State capitals (ScienceQA)": {
            "acc": 0.23397435897435898
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.5342465753424658
        },
        "States of matter (ScienceQA)": {
            "acc": 0.39285714285714285
        },
        "Materials (ScienceQA)": {
            "acc": 0.5104895104895105
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5612244897959183
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3064516129032258
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.2619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.13725490196078433
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.46551724137931033
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.41304347826086957
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.3125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.46296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.2608695652173913
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Classification (ScienceQA)": {
            "acc": 0.4430379746835443
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.2676056338028169
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.27918781725888325
        },
        "3D Distance (CVBench)": {
            "acc": 0.5333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.5076923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.4166666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.249656844139099,
            "bert": 0.03787132263183594
        },
        "Whole dataset (Enrico)": {
            "bart": -8.643479137420654,
            "bert": 0.9517783743143081
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.738909885883332,
            "bert": 0.6853240311145783
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.6668424510955813,
            "bert": 0.7799259918928146
        },
        "Whole dataset (GQA)": {
            "bart": -5.936856892108917,
            "bert": 0.5957183855772018
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.750525026321411,
            "bert": 0.6521205073595047
        },
        "Whole dataset (INAT)": {
            "bart": -7.010714225769043,
            "bert": 0.3839819085597992
        },
        "Whole dataset (IRFL)": {
            "bart": -4.779674061536789,
            "bert": 0.9677016574144364
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.626107249259949,
            "bert": 0.7381813091039657
        },
        "Whole dataset (Memotion)": {
            "bart": -5.698493962287903,
            "bert": 0.7297727519273758
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.433705832958221,
            "bert": 0.6730308473110199
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.474527488946915,
            "bert": 0.9983574068546295
        },
        "Whole dataset (NLVR)": {
            "bart": -6.189759254455566,
            "bert": 0.6533534526824951
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.723340749740601,
            "bert": 0.7172279870510101
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.719349293708801,
            "bert": 0.8663414919376373
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.725790718793869,
            "bert": 0.36549199163913726
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.527438616752624,
            "bert": 0.8153475427627563
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.537272337675095,
            "bert": 0.2454436230659485
        },
        "Whole dataset (Resisc45)": {
            "bart": -7.020511898994446,
            "bert": 0.6207675486803055
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.3324124407768245,
            "bert": 0.8066256421804429
        },
        "Whole dataset (Slake)": {
            "bart": -4.219825990200043,
            "bert": 0.8941505229473115
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.272594463825226,
            "bert": 0.8398934352397919
        },
        "Whole dataset (VCR)": {
            "bart": -4.7366800910234454,
            "bert": 0.782873705625534
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.873926103115082,
            "bert": 0.22028350412845613
        },
        "Whole dataset (VQA)": {
            "bart": -6.040063333511353,
            "bert": 0.4942370384931564
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.733789248466492,
            "bert": 0.830861554145813
        },
        "Whole dataset (Winoground)": {
            "bart": -7.059910790920258,
            "bert": 0.39994773209095
        },
        "random (POPE)": {
            "acc": 0.3973941368078176,
            "prec": 0.4106280193236715,
            "rec": 0.5743243243243243,
            "f1": 0.4788732394366197
        },
        "popular (POPE)": {
            "acc": 0.4527687296416938,
            "prec": 0.47417840375586856,
            "rec": 0.643312101910828,
            "f1": 0.5459459459459459
        },
        "adversarial (POPE)": {
            "acc": 0.3916083916083916,
            "prec": 0.3894736842105263,
            "rec": 0.5606060606060606,
            "f1": 0.45962732919254656
        }
    },
    "BLIP2-opt-6.7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.2703807270380727
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.22521957340025095
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.29740518962075846
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.226
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.23743359215365753
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.22938775510204082
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.26436781609195403
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23412698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25115970841616964
        },
        "Instance Location (SEED_2)": {
            "acc": 0.28732106339468305
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.3402061855670103
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.2564914502849905
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2725286728563626
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.6582278481012658
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.20454545454545456
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2831050228310502
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22382671480144403
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.18475073313782991
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.26586102719033233
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.32075471698113206
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3090909090909091
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23115577889447236
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.5852941176470589,
            "prec": 0.7101449275362319,
            "rec": 0.28823529411764703,
            "f1": 0.4100418410041841
        },
        "posters (MME)": {
            "acc": 0.5986394557823129,
            "prec": 0.5960264900662252,
            "rec": 0.6122448979591837,
            "f1": 0.6040268456375839
        },
        "position (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5084745762711864,
            "rec": 1.0,
            "f1": 0.6741573033707865
        },
        "scene (MME)": {
            "acc": 0.6275,
            "prec": 0.5770392749244713,
            "rec": 0.955,
            "f1": 0.71939736346516
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5071428571428571,
            "prec": 0.504424778761062,
            "rec": 0.8142857142857143,
            "f1": 0.6229508196721312
        },
        "artwork (MME)": {
            "acc": 0.54,
            "prec": 0.5216216216216216,
            "rec": 0.965,
            "f1": 0.6771929824561403
        },
        "landmark (MME)": {
            "acc": 0.5325,
            "prec": 0.5181058495821727,
            "rec": 0.93,
            "f1": 0.6654740608228981
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.55,
            "rec": 0.55,
            "f1": 0.55
        },
        "existence (MME)": {
            "acc": 0.75,
            "prec": 0.7777777777777778,
            "rec": 0.7,
            "f1": 0.7368421052631577
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.15,
            "f1": 0.23076923076923075
        },
        "count (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.4915254237288136,
            "rec": 0.9666666666666667,
            "f1": 0.651685393258427
        },
        "color (MME)": {
            "acc": 0.6166666666666667,
            "prec": 0.5813953488372093,
            "rec": 0.8333333333333334,
            "f1": 0.684931506849315
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5172413793103449,
            "rec": 0.75,
            "f1": 0.6122448979591838
        },
        "code_reasoning (MME)": {
            "acc": 0.25,
            "prec": 0.32142857142857145,
            "rec": 0.45,
            "f1": 0.375
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.20348837209302326
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2698412698412698
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.19230769230769232
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2557077625570776
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2978723404255319
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2849162011173184
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.24186046511627907
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24078624078624078
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.21717171717171718
        },
        "ocr (MMBench_CN)": {
            "acc": 0.1858974358974359
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24858757062146894
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2801418439716312
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.21
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.27631578947368424
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.22727272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2872340425531915
        },
        "image_style (MMBench_CN)": {
            "acc": 0.2169811320754717
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.2558139534883721
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.23809523809523808
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2968036529680365
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2346368715083799
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.25116279069767444
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.25307125307125306
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2601010101010101
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.31560283687943264
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.245
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2631578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.23863636363636365
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.24528301886792453
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3466666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.25
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.1
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.11904761904761904
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2222222222222222
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1715686274509804
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4794520547945205
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "State capitals (ScienceQA)": {
            "acc": 0.21794871794871795
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.5616438356164384
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.5034965034965035
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5612244897959183
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3064516129032258
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3620689655172414
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.21875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4074074074074074
        },
        "Maps (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Classification (ScienceQA)": {
            "acc": 0.4050632911392405
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3448275862068966
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.2112676056338028
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "2D Count (CVBench)": {
            "acc": 0.26776649746192893
        },
        "3D Distance (CVBench)": {
            "acc": 0.515
        },
        "2D Relation (CVBench)": {
            "acc": 0.5107692307692308
        },
        "3D Depth (CVBench)": {
            "acc": 0.465
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.260412535667419,
            "bert": 0.0
        },
        "Whole dataset (Enrico)": {
            "bart": -8.582415208816528,
            "bert": 0.9135199874639511
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.66216118812561,
            "bert": 0.7300645542144776
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.511706147193909,
            "bert": 0.8847218805551529
        },
        "Whole dataset (GQA)": {
            "bart": -6.465157231092453,
            "bert": 0.4415537852048874
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.533982164859772,
            "bert": 0.7242774891853333
        },
        "Whole dataset (INAT)": {
            "bart": -6.659377388954162,
            "bert": 0.6715194004774093
        },
        "Whole dataset (IRFL)": {
            "bart": -4.82381441950798,
            "bert": 0.9472058409452438
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.7511664438247685,
            "bert": 0.6919377422332764
        },
        "Whole dataset (Memotion)": {
            "bart": -5.031316404342651,
            "bert": 0.7886568301916123
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.317062549591064,
            "bert": 0.6746337151527405
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.479040695428848,
            "bert": 0.9977989089488983
        },
        "Whole dataset (NLVR)": {
            "bart": -6.170588207244873,
            "bert": 0.6602641272544861
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.022187271118164,
            "bert": 0.9137997341156006
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.738457008600235,
            "bert": 0.9068356579542161
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.472329697608948,
            "bert": 0.33071056127548215
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.943505884408951,
            "bert": 0.8152126741409301
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.312871277332306,
            "bert": 0.8241433471441268
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.959090719223022,
            "bert": 0.006514801383018493
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.387485949993134,
            "bert": 0.7695188707113266
        },
        "Whole dataset (Slake)": {
            "bart": -3.8412043380737306,
            "bert": 0.9862396508455277
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.9616916680336,
            "bert": 0.8267673772573471
        },
        "Whole dataset (VCR)": {
            "bart": -3.963764083981514,
            "bert": 0.8515271943807602
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.014053249359131,
            "bert": 0.37822321593761443
        },
        "Whole dataset (VQA)": {
            "bart": -5.67989339351654,
            "bert": 0.6901689660549164
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.010751188993454,
            "bert": 0.8977348756790161
        },
        "Whole dataset (Winoground)": {
            "bart": -5.4199279749393465,
            "bert": 0.7509807986021042
        },
        "random (POPE)": {
            "acc": 0.5700325732899023,
            "prec": 0.5465116279069767,
            "rec": 0.6351351351351351,
            "f1": 0.5874999999999999
        },
        "popular (POPE)": {
            "acc": 0.6026058631921825,
            "prec": 0.5879396984924623,
            "rec": 0.7452229299363057,
            "f1": 0.6573033707865169
        },
        "adversarial (POPE)": {
            "acc": 0.5699300699300699,
            "prec": 0.5257142857142857,
            "rec": 0.696969696969697,
            "f1": 0.5993485342019544
        }
    },
    "BLIP2-flan-t5-xl": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5504409550440955
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5868263473053892
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5294855708908407
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.35528942115768464
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.75
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4303228442991418
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.35918367346938773
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.696551724137931
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24206349206349206
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4300861497680583
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5204498977505112
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.727042431918936
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.642818132168214
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5109780439121756
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4490106544901065
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5992779783393501
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.35777126099706746
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7673716012084593
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.3
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.3710691823899371
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.592964824120603
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.16049382716049382
        },
        "celebrity (MME)": {
            "acc": 0.5323529411764706,
            "prec": 1.0,
            "rec": 0.06470588235294118,
            "f1": 0.12154696132596686
        },
        "posters (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.9622641509433962,
            "rec": 0.3469387755102041,
            "f1": 0.51
        },
        "position (MME)": {
            "acc": 0.55,
            "prec": 0.6666666666666666,
            "rec": 0.2,
            "f1": 0.30769230769230765
        },
        "scene (MME)": {
            "acc": 0.7675,
            "prec": 1.0,
            "rec": 0.535,
            "f1": 0.6970684039087948
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5928571428571429,
            "prec": 0.7241379310344828,
            "rec": 0.3,
            "f1": 0.42424242424242425
        },
        "artwork (MME)": {
            "acc": 0.6925,
            "prec": 0.9529411764705882,
            "rec": 0.405,
            "f1": 0.5684210526315789
        },
        "landmark (MME)": {
            "acc": 0.5025,
            "prec": 1.0,
            "rec": 0.005,
            "f1": 0.009950248756218907
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.8333333333333334,
            "prec": 1.0,
            "rec": 0.6666666666666666,
            "f1": 0.8
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6944444444444444,
            "rec": 0.8333333333333334,
            "f1": 0.7575757575757577
        },
        "color (MME)": {
            "acc": 0.75,
            "prec": 0.7419354838709677,
            "rec": 0.7666666666666667,
            "f1": 0.7540983606557377
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.64,
            "rec": 0.8,
            "f1": 0.7111111111111111
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.1,
            "f1": 0.16666666666666669
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.2441860465116279
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.23492063492063492
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.13076923076923078
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.228310502283105
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2765957446808511
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.26256983240223464
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.20465116279069767
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.24494949494949494
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23717948717948717
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23728813559322035
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2978723404255319
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.255
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.29605263157894735
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2897727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.19148936170212766
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24056603773584906
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2727272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.23333333333333334
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.25
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5015873015873016
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.7
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4931506849315068
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8813131313131313
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7115384615384616
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6134751773049646
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.785
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9177631578947368
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.7021276595744681
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8679245283018868
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.7575757575757576
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9214285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4
        },
        "History (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5
        },
        "Literature (MMMU)": {
            "acc": 0.7
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.4
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2540983606557377
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.09523809523809523
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24836601307189543
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3522727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9166666666666666
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8163265306122449
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.15555555555555556
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8260869565217391
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.5
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.8028169014084507
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.6578947368421053
        },
        "2D Count (CVBench)": {
            "acc": 0.42385786802030456
        },
        "3D Distance (CVBench)": {
            "acc": 0.5016666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5723076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.515
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.217363073825836,
            "bert": 0.740151263475418
        },
        "Whole dataset (Enrico)": {
            "bart": -6.13874390244484,
            "bert": 0.9830271887779236
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.958056983947754,
            "bert": 0.9993937820196152
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.486234759092331,
            "bert": 0.8820110726356506
        },
        "Whole dataset (GQA)": {
            "bart": -4.819745020866394,
            "bert": 0.9939017522335053
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.710334405899048,
            "bert": 0.9981489759683609
        },
        "Whole dataset (INAT)": {
            "bart": -6.366613259315491,
            "bert": 0.7748703688383103
        },
        "Whole dataset (IRFL)": {
            "bart": -4.718669824600219,
            "bert": 0.9989164352416993
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.649097783565521,
            "bert": 0.8544546812772751
        },
        "Whole dataset (Memotion)": {
            "bart": -4.559603543281555,
            "bert": 0.9010154551267624
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9050068318843842,
            "bert": 0.7609189260005951
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.459598881006241,
            "bert": 0.9983608186244964
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6206137990951537,
            "bert": 0.999545584321022
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9995548486709595
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5446760681271554,
            "bert": 0.9128637927770614
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.135339782834053,
            "bert": 0.9418205505609513
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.214697129130363,
            "bert": 0.8122423309087753
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.572040686607361,
            "bert": 0.9118767434358597
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.26986701130867,
            "bert": 0.937683430314064
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.047156472206115,
            "bert": 0.8671222275495529
        },
        "Whole dataset (Slake)": {
            "bart": -3.7602982902526856,
            "bert": 0.9933174550533295
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.026254618763923,
            "bert": 0.9448883777856827
        },
        "Whole dataset (VCR)": {
            "bart": -3.0320584684610368,
            "bert": 0.9333203381299973
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.842327587604523,
            "bert": 0.9099763441085815
        },
        "Whole dataset (VQA)": {
            "bart": -5.498141409158706,
            "bert": 0.9617846035957336
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.799918657541275,
            "bert": 0.9413445651531219
        },
        "Whole dataset (Winoground)": {
            "bart": -4.855510189533233,
            "bert": 0.9980586725473404
        },
        "random (POPE)": {
            "acc": 0.755700325732899,
            "prec": 1.0,
            "rec": 0.49324324324324326,
            "f1": 0.6606334841628959
        },
        "popular (POPE)": {
            "acc": 0.7687296416938111,
            "prec": 0.9479166666666666,
            "rec": 0.5796178343949044,
            "f1": 0.7193675889328063
        },
        "adversarial (POPE)": {
            "acc": 0.8041958041958042,
            "prec": 0.9634146341463414,
            "rec": 0.5984848484848485,
            "f1": 0.7383177570093457
        }
    },
    "InstructBLIP-Vicuna-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.43772854377285436
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.32934131736526945
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.37327478042659973
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.602
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2909685328974254
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.28408163265306124
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.45057471264367815
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2251984126984127
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.30881378396288933
        },
        "Instance Location (SEED_2)": {
            "acc": 0.3323108384458078
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.29896907216494845
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.4401519949335022
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.4609503003823048
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.1518987341772152
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2785388127853881
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4151624548736462
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.31085043988269795
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.45317220543806647
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.23333333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5878787878787879
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.3165829145728643
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.30864197530864196
        },
        "celebrity (MME)": {
            "acc": 0.6058823529411764,
            "prec": 0.5592105263157895,
            "rec": 1.0,
            "f1": 0.7172995780590717
        },
        "posters (MME)": {
            "acc": 0.8027210884353742,
            "prec": 0.7405405405405405,
            "rec": 0.9319727891156463,
            "f1": 0.8253012048192772
        },
        "position (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9333333333333333,
            "f1": 0.6511627906976745
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8356807511737089,
            "rec": 0.89,
            "f1": 0.8619854721549637
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.65,
            "prec": 0.6,
            "rec": 0.9,
            "f1": 0.7200000000000001
        },
        "artwork (MME)": {
            "acc": 0.635,
            "prec": 0.5794117647058824,
            "rec": 0.985,
            "f1": 0.7296296296296296
        },
        "landmark (MME)": {
            "acc": 0.8625,
            "prec": 0.84688995215311,
            "rec": 0.885,
            "f1": 0.8655256723716381
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 1.0,
            "rec": 0.9,
            "f1": 0.9473684210526316
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.45454545454545453,
            "rec": 0.75,
            "f1": 0.5660377358490566
        },
        "count (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5172413793103449,
            "rec": 1.0,
            "f1": 0.6818181818181819
        },
        "color (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6521739130434783,
            "rec": 1.0,
            "f1": 0.7894736842105263
        },
        "OCR (MME)": {
            "acc": 0.55,
            "prec": 0.5263157894736842,
            "rec": 1.0,
            "f1": 0.6896551724137931
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.47368421052631576,
            "rec": 0.9,
            "f1": 0.6206896551724138
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.436046511627907
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2761904761904762
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3013698630136986
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2765957446808511
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2681564245810056
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.40930232558139534
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.5036855036855037
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.31313131313131315
        },
        "ocr (MMBench_CN)": {
            "acc": 0.30128205128205127
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2624113475177305
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.295
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.34210526315789475
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.42613636363636365
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2765957446808511
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25943396226415094
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.3712121212121212
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.30666666666666664
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.4142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.5
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.3111111111111111
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3013698630136986
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3475177304964539
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.3240223463687151
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.31627906976744186
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.5601965601965602
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.5025252525252525
        },
        "ocr (MMBench_EN)": {
            "acc": 0.4935897435897436
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.23728813559322035
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3262411347517731
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.52
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.5361842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.5795454545454546
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.32978723404255317
        },
        "image_style (MMBench_EN)": {
            "acc": 0.5518867924528302
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.6174242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.24666666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.4785714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.1
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.1
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.0
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1111111111111111
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18137254901960784
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3522727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.47435897435897434
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.4520547945205479
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.5734265734265734
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5408163265306123
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.22580645161290322
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4878048780487805
        },
        "Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.5454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.75
        },
        "Solutions (ScienceQA)": {
            "acc": 0.46296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.43137254901960786
        },
        "Classification (ScienceQA)": {
            "acc": 0.46835443037974683
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.2535211267605634
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5526315789473685
        },
        "2D Count (CVBench)": {
            "acc": 0.28553299492385786
        },
        "3D Distance (CVBench)": {
            "acc": 0.495
        },
        "2D Relation (CVBench)": {
            "acc": 0.4969230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.49833333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.247931151390076,
            "bert": 0.7524582272768021
        },
        "Whole dataset (Enrico)": {
            "bart": -7.458817967176437,
            "bert": 0.89186896443367
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.8028169631958,
            "bert": 0.8302133464813233
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.7009020113945006,
            "bert": 0.8971405190229416
        },
        "Whole dataset (GQA)": {
            "bart": -7.788251557350159,
            "bert": 0.982860403060913
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.647488602399826,
            "bert": 0.9749012148380279
        },
        "Whole dataset (INAT)": {
            "bart": -7.237679619789123,
            "bert": 0.770797986984253
        },
        "Whole dataset (IRFL)": {
            "bart": -4.465999250411987,
            "bert": 0.9976523661613464
        },
        "Whole dataset (MemeCaps)": {
            "bart": -5.173456523418427,
            "bert": 0.8239322447776795
        },
        "Whole dataset (Memotion)": {
            "bart": -6.453422951698303,
            "bert": 0.8957456028461457
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.312567756175995,
            "bert": 0.7649321395158768
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.925425353050232,
            "bert": 0.988997728228569
        },
        "Whole dataset (NLVR)": {
            "bart": -7.733592619895935,
            "bert": 0.9822450172901154
        },
        "Whole dataset (NLVR2)": {
            "bart": -7.391578884124756,
            "bert": 0.9941350448131562
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.9038985991477966,
            "bert": 0.9307724946737289
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.025131850242615,
            "bert": 0.9090565019845962
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.364226151704788,
            "bert": 0.824335908293724
        },
        "Whole dataset (PathVQA)": {
            "bart": -7.40686493396759,
            "bert": 0.8934411996603012
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.406998915672302,
            "bert": 0.8192465549707413
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.137735095024109,
            "bert": 0.8692782747745514
        },
        "Whole dataset (Slake)": {
            "bart": -7.743272967338562,
            "bert": 0.9887160950899124
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.9355297899246215,
            "bert": 0.7511878883838654
        },
        "Whole dataset (VCR)": {
            "bart": -6.199783961772919,
            "bert": 0.832353031039238
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.488010048866272,
            "bert": 0.8968205451965332
        },
        "Whole dataset (VQA)": {
            "bart": -6.913658361434937,
            "bert": 0.9642208415269852
        },
        "Whole dataset (VQARAD)": {
            "bart": -7.705760922431946,
            "bert": 0.9189763092994689
        },
        "Whole dataset (Winoground)": {
            "bart": -7.645412051677704,
            "bert": 0.9704032880067825
        },
        "random (POPE)": {
            "acc": 0.8599348534201955,
            "prec": 0.9565217391304348,
            "rec": 0.7432432432432432,
            "f1": 0.8365019011406843
        },
        "popular (POPE)": {
            "acc": 0.8175895765472313,
            "prec": 0.8531468531468531,
            "rec": 0.7770700636942676,
            "f1": 0.8133333333333334
        },
        "adversarial (POPE)": {
            "acc": 0.8636363636363636,
            "prec": 0.8604651162790697,
            "rec": 0.8409090909090909,
            "f1": 0.8505747126436781
        }
    },
    "InstructBLIP-Vicuna-13B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5528070552807055
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.35129740518962077
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.4880803011292346
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.30538922155688625
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.616
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.34572946465059257
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.33714285714285713
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4804597701149425
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3260437375745527
        },
        "Instance Location (SEED_2)": {
            "acc": 0.42535787321063395
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.4742268041237113
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6285623812539582
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5461496450027308
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.32575757575757575
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.3546423135464231
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4693140794223827
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.25024437927663734
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.5649546827794562
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.30833333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.29559748427672955
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6757575757575758
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.49246231155778897
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.8676470588235294,
            "prec": 0.8531073446327684,
            "rec": 0.888235294117647,
            "f1": 0.8703170028818444
        },
        "posters (MME)": {
            "acc": 0.8231292517006803,
            "prec": 0.8231292517006803,
            "rec": 0.8231292517006803,
            "f1": 0.8231292517006803
        },
        "position (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5208333333333334,
            "rec": 0.8333333333333334,
            "f1": 0.6410256410256411
        },
        "scene (MME)": {
            "acc": 0.8425,
            "prec": 0.8277511961722488,
            "rec": 0.865,
            "f1": 0.8459657701711492
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6428571428571429,
            "prec": 0.6111111111111112,
            "rec": 0.7857142857142857,
            "f1": 0.6875000000000001
        },
        "artwork (MME)": {
            "acc": 0.7625,
            "prec": 0.6895306859205776,
            "rec": 0.955,
            "f1": 0.80083857442348
        },
        "landmark (MME)": {
            "acc": 0.87,
            "prec": 0.8425925925925926,
            "rec": 0.91,
            "f1": 0.8749999999999999
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.5142857142857142,
            "rec": 0.9,
            "f1": 0.6545454545454545
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.35,
            "prec": 0.2857142857142857,
            "rec": 0.2,
            "f1": 0.23529411764705882
        },
        "count (MME)": {
            "acc": 0.65,
            "prec": 0.6,
            "rec": 0.9,
            "f1": 0.7200000000000001
        },
        "color (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.25,
            "f1": 0.3333333333333333
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.313953488372093
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.18461538461538463
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2328767123287671
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.22695035460992907
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2346368715083799
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26976744186046514
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.2678132678132678
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "ocr (MMBench_CN)": {
            "acc": 0.27564102564102566
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2033898305084746
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.25886524822695034
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.285
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.26644736842105265
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.4318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2872340425531915
        },
        "image_style (MMBench_CN)": {
            "acc": 0.29245283018867924
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.26515151515151514
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.29333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.75
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.37777777777777777
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3769230769230769
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3378995433789954
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.46099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4022346368715084
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.4697674418604651
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.8771498771498771
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8560606060606061
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5641025641025641
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.24293785310734464
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.77
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.5986842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8693181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.40425531914893614
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.6893939393939394
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.29333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.6285714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.2
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.2
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.1
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29508196721311475
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20588235294117646
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6301369863013698
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.6698717948717948
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3424657534246575
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.7762237762237763
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.5952380952380952
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5925925925925926
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6274509803921569
        },
        "Classification (ScienceQA)": {
            "acc": 0.5316455696202531
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7241379310344828
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.19718309859154928
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "2D Count (CVBench)": {
            "acc": 0.44416243654822335
        },
        "3D Distance (CVBench)": {
            "acc": 0.505
        },
        "2D Relation (CVBench)": {
            "acc": 0.5
        },
        "3D Depth (CVBench)": {
            "acc": 0.5
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.215803725719452,
            "bert": 0.7601405519247055
        },
        "Whole dataset (Enrico)": {
            "bart": -7.442999610900879,
            "bert": 0.8132492411136627
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.934589066505432,
            "bert": 0.8382966989278793
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.73033796787262,
            "bert": 0.8825812411308288
        },
        "Whole dataset (GQA)": {
            "bart": -7.774313793182373,
            "bert": 0.9841825312376022
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.778074522018432,
            "bert": 0.989801543354988
        },
        "Whole dataset (INAT)": {
            "bart": -7.463322558403015,
            "bert": 0.7307545989751816
        },
        "Whole dataset (IRFL)": {
            "bart": -4.638820484876633,
            "bert": 0.99674944460392
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.8949397492408755,
            "bert": 0.8469370347261429
        },
        "Whole dataset (Memotion)": {
            "bart": -6.017084078788757,
            "bert": 0.8643329977989197
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.045704998970032,
            "bert": 0.8535632705688476
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.918303308486938,
            "bert": 0.9897470039129257
        },
        "Whole dataset (NLVR)": {
            "bart": -7.649746894836426,
            "bert": 0.9959747177362442
        },
        "Whole dataset (NLVR2)": {
            "bart": -7.586302709579468,
            "bert": 0.9890784478187561
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.949563680887222,
            "bert": 0.9076203233003617
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.056010028123856,
            "bert": 0.9247719782590866
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.608685922622681,
            "bert": 0.8500134384632111
        },
        "Whole dataset (PathVQA)": {
            "bart": -7.333625407218933,
            "bert": 0.9120377027988433
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.383161344528198,
            "bert": 0.8175181984901428
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.3193918585777284,
            "bert": 0.8636416715383529
        },
        "Whole dataset (Slake)": {
            "bart": -7.728555288314819,
            "bert": 0.9893223869800568
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.930123624801635,
            "bert": 0.7618834275007248
        },
        "Whole dataset (VCR)": {
            "bart": -6.174794099330902,
            "bert": 0.8269679862260818
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.515312900543213,
            "bert": 0.9013552749156952
        },
        "Whole dataset (VQA)": {
            "bart": -6.929527640342712,
            "bert": 0.9667247623205185
        },
        "Whole dataset (VQARAD)": {
            "bart": -7.65270299911499,
            "bert": 0.9369827461242676
        },
        "Whole dataset (Winoground)": {
            "bart": -7.8125273752212525,
            "bert": 0.9806416314840317
        },
        "random (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9256198347107438,
            "rec": 0.7567567567567568,
            "f1": 0.8327137546468402
        },
        "popular (POPE)": {
            "acc": 0.8110749185667753,
            "prec": 0.8413793103448276,
            "rec": 0.7770700636942676,
            "f1": 0.8079470198675496
        },
        "adversarial (POPE)": {
            "acc": 0.8566433566433567,
            "prec": 0.8527131782945736,
            "rec": 0.8333333333333334,
            "f1": 0.842911877394636
        }
    },
    "InstructBLIP-flan-t5-xl": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6687459668745966
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6107784431137725
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5200752823086575
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.33532934131736525
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.65
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4822231303637107
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3485714285714286
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.671264367816092
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24702380952380953
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.42611000662690524
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5316973415132924
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7330588980367321
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6728563626433642
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8987341772151899
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.18181818181818182
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5808383233532934
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4581430745814307
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5595667870036101
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3225806451612903
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7854984894259819
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.225
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.41509433962264153
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6484848484848484
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6080402010050251
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.14814814814814814
        },
        "celebrity (MME)": {
            "acc": 0.7588235294117647,
            "prec": 0.8859649122807017,
            "rec": 0.5941176470588235,
            "f1": 0.7112676056338029
        },
        "posters (MME)": {
            "acc": 0.7755102040816326,
            "prec": 0.7425149700598802,
            "rec": 0.8435374149659864,
            "f1": 0.7898089171974522
        },
        "position (MME)": {
            "acc": 0.6,
            "prec": 0.5882352941176471,
            "rec": 0.6666666666666666,
            "f1": 0.625
        },
        "scene (MME)": {
            "acc": 0.8175,
            "prec": 0.9568345323741008,
            "rec": 0.665,
            "f1": 0.784660766961652
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7357142857142858,
            "prec": 0.7538461538461538,
            "rec": 0.7,
            "f1": 0.725925925925926
        },
        "artwork (MME)": {
            "acc": 0.7075,
            "prec": 0.7485029940119761,
            "rec": 0.625,
            "f1": 0.6811989100817438
        },
        "landmark (MME)": {
            "acc": 0.5375,
            "prec": 1.0,
            "rec": 0.075,
            "f1": 0.13953488372093023
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.325,
            "prec": 0.1111111111111111,
            "rec": 0.05,
            "f1": 0.06896551724137932
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.725,
            "rec": 0.9666666666666667,
            "f1": 0.8285714285714285
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8125,
            "rec": 0.8666666666666667,
            "f1": 0.8387096774193549
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5428571428571428,
            "rec": 0.95,
            "f1": 0.6909090909090908
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.3,
            "f1": 0.37499999999999994
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.0755813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.26031746031746034
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.07692307692307693
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2054794520547945
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.15602836879432624
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.07262569832402235
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.19069767441860466
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.13513513513513514
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2222222222222222
        },
        "ocr (MMBench_CN)": {
            "acc": 0.21153846153846154
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.11864406779661017
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2872340425531915
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.11842105263157894
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.17613636363636365
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.1595744680851064
        },
        "image_style (MMBench_CN)": {
            "acc": 0.21226415094339623
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2159090909090909
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.05333333333333334
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.07142857142857142
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5301587301587302
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4520547945205479
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5886524822695035
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5586592178770949
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8686868686868687
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5961538461538461
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6063829787234043
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9144736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6914893617021277
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8679245283018868
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.7803030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.35333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.1
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1885245901639344
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.27058823529411763
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14705882352941177
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9006410256410257
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.8741258741258742
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4032258064516129
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.43902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.08888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.7605633802816901
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.4530456852791878
        },
        "3D Distance (CVBench)": {
            "acc": 0.53
        },
        "2D Relation (CVBench)": {
            "acc": 0.5523076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.55
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.066941423416138,
            "bert": 0.786591163277626
        },
        "Whole dataset (Enrico)": {
            "bart": -6.404915322065353,
            "bert": 0.9766215354204177
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.7710738229751586,
            "bert": 0.9993128454685212
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.374401571750641,
            "bert": 0.903994927406311
        },
        "Whole dataset (GQA)": {
            "bart": -4.100738903284073,
            "bert": 0.9964894735813141
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.135556517839432,
            "bert": 0.9982300680875779
        },
        "Whole dataset (INAT)": {
            "bart": -6.599530816078186,
            "bert": 0.7684190434217453
        },
        "Whole dataset (IRFL)": {
            "bart": -4.715800975561142,
            "bert": 0.9990518724918366
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.60562237739563,
            "bert": 0.8480598515272141
        },
        "Whole dataset (Memotion)": {
            "bart": -4.692514081001281,
            "bert": 0.9012510454654694
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.655599871873855,
            "bert": 0.8453988146781921
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.459598881006241,
            "bert": 0.9968397480249405
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5645350980758668,
            "bert": 0.9995641297101975
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6367765283584594,
            "bert": 0.9995455795526504
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.292408220767975,
            "bert": 0.9434504675865173
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.4667556077241897,
            "bert": 0.9677393609285354
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.214697129130363,
            "bert": 0.8122423309087753
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.366942317485809,
            "bert": 0.8996500968933105
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.124551348686218,
            "bert": 0.8246296149492264
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.071377549171448,
            "bert": 0.8675789117813111
        },
        "Whole dataset (Slake)": {
            "bart": -3.5141399562358857,
            "bert": 0.992704005241394
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.7242481136322025,
            "bert": 0.8309024930000305
        },
        "Whole dataset (VCR)": {
            "bart": -3.0975395005941393,
            "bert": 0.9308104407787323
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6981389832496645,
            "bert": 0.9072406953573227
        },
        "Whole dataset (VQA)": {
            "bart": -4.363086705207825,
            "bert": 0.980549293756485
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.470278738737107,
            "bert": 0.9419868904352188
        },
        "Whole dataset (Winoground)": {
            "bart": -4.759032118320465,
            "bert": 0.9973363453149795
        },
        "random (POPE)": {
            "acc": 0.8045602605863192,
            "prec": 0.9888888888888889,
            "rec": 0.6013513513513513,
            "f1": 0.7478991596638654
        },
        "popular (POPE)": {
            "acc": 0.8078175895765473,
            "prec": 0.9298245614035088,
            "rec": 0.6751592356687898,
            "f1": 0.7822878228782287
        },
        "adversarial (POPE)": {
            "acc": 0.8496503496503497,
            "prec": 0.9405940594059405,
            "rec": 0.7196969696969697,
            "f1": 0.815450643776824
        }
    },
    "InstructBLIP-flan-t5-xxl": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6498171649817165
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.7085828343313373
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5357590966122961
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2834331337325349
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.798
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5226808336738864
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3575510204081633
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6781609195402298
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24702380952380953
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4671968190854871
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5736196319018405
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7590246991766941
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6892408519934462
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8734177215189873
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5828343313373253
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.471841704718417
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.6137184115523465
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.40078201368523947
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.325
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.46540880503144655
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6757575757575758
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5778894472361809
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.19753086419753085
        },
        "celebrity (MME)": {
            "acc": 0.6735294117647059,
            "prec": 0.9836065573770492,
            "rec": 0.35294117647058826,
            "f1": 0.5194805194805194
        },
        "posters (MME)": {
            "acc": 0.7721088435374149,
            "prec": 0.9651162790697675,
            "rec": 0.564625850340136,
            "f1": 0.7124463519313305
        },
        "position (MME)": {
            "acc": 0.55,
            "prec": 0.5405405405405406,
            "rec": 0.6666666666666666,
            "f1": 0.5970149253731343
        },
        "scene (MME)": {
            "acc": 0.825,
            "prec": 0.9577464788732394,
            "rec": 0.68,
            "f1": 0.7953216374269005
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.75,
            "prec": 0.7272727272727273,
            "rec": 0.8,
            "f1": 0.761904761904762
        },
        "artwork (MME)": {
            "acc": 0.75,
            "prec": 0.7747252747252747,
            "rec": 0.705,
            "f1": 0.7382198952879582
        },
        "landmark (MME)": {
            "acc": 0.5425,
            "prec": 0.9473684210526315,
            "rec": 0.09,
            "f1": 0.1643835616438356
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6,
            "rec": 0.15,
            "f1": 0.24
        },
        "existence (MME)": {
            "acc": 0.9333333333333333,
            "prec": 1.0,
            "rec": 0.8666666666666667,
            "f1": 0.9285714285714286
        },
        "numerical_calculation (MME)": {
            "acc": 0.325,
            "prec": 0.23076923076923078,
            "rec": 0.15,
            "f1": 0.18181818181818185
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7073170731707317,
            "rec": 0.9666666666666667,
            "f1": 0.8169014084507042
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8125,
            "rec": 0.8666666666666667,
            "f1": 0.8387096774193549
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5405405405405406,
            "rec": 1.0,
            "f1": 0.7017543859649124
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5384615384615384,
            "rec": 0.35,
            "f1": 0.4242424242424242
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.27906976744186046
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.24444444444444444
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2557077625570776
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2695035460992908
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2737430167597765
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26046511627906976
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.26044226044226043
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.24242424242424243
        },
        "ocr (MMBench_CN)": {
            "acc": 0.2692307692307692
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.32269503546099293
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.23684210526315788
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.23863636363636365
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_CN)": {
            "acc": 0.22641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2765151515151515
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.22
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.25
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6076923076923076
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6595744680851063
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6871508379888268
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9116161616161617
        },
        "ocr (MMBench_EN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6879432624113475
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9046052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6808510638297872
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8962264150943396
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.37333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.1
        },
        "History (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.19607843137254902
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.8811188811188811
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9183673469387755
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.27419354838709675
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7692307692307693
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Maps (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7254901960784313
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.8732394366197183
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5789473684210527
        },
        "2D Count (CVBench)": {
            "acc": 0.4682741116751269
        },
        "3D Distance (CVBench)": {
            "acc": 0.5016666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5584615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.51
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.263595385551453,
            "bert": 0.7463453191518784
        },
        "Whole dataset (Enrico)": {
            "bart": -6.12348380446434,
            "bert": 0.9588596117496491
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.8826256680488584,
            "bert": 0.999356922507286
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3759159433841703,
            "bert": 0.90352166056633
        },
        "Whole dataset (GQA)": {
            "bart": -4.172869024276733,
            "bert": 0.9967326641082763
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.102515786886215,
            "bert": 0.9980111241340637
        },
        "Whole dataset (INAT)": {
            "bart": -6.6504180002212525,
            "bert": 0.7799820977449418
        },
        "Whole dataset (IRFL)": {
            "bart": -4.147345148324966,
            "bert": 0.9994488543272019
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.659070155620575,
            "bert": 0.8513059550523758
        },
        "Whole dataset (Memotion)": {
            "bart": -4.704370069503784,
            "bert": 0.9015944963693618
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.807320012450218,
            "bert": 0.8914758569002151
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.501515855789185,
            "bert": 0.9971092420816422
        },
        "Whole dataset (NLVR)": {
            "bart": -2.9144253444671633,
            "bert": 0.9996476423740387
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.1704249429702758,
            "bert": 0.9996568709611893
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.2637440475821493,
            "bert": 0.9448841315507889
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.4550268131494524,
            "bert": 0.9717508834600449
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.214697129130363,
            "bert": 0.8122423309087753
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.561135491132736,
            "bert": 0.9022712761163711
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.883669216632843,
            "bert": 0.882120543718338
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.238541841506958,
            "bert": 0.8636443215608597
        },
        "Whole dataset (Slake)": {
            "bart": -4.137699919939041,
            "bert": 0.9943461614847183
        },
        "Whole dataset (UCMerced)": {
            "bart": -3.9336629515886306,
            "bert": 0.8995545744895935
        },
        "Whole dataset (VCR)": {
            "bart": -3.0937080347537993,
            "bert": 0.9271471083164216
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.9204445666074754,
            "bert": 0.9050394207239151
        },
        "Whole dataset (VQA)": {
            "bart": -4.657954993247986,
            "bert": 0.9826387804746628
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.988869730234146,
            "bert": 0.9457706022262573
        },
        "Whole dataset (Winoground)": {
            "bart": -4.4748042047023775,
            "bert": 0.9975620859861374
        },
        "random (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.9611650485436893,
            "rec": 0.668918918918919,
            "f1": 0.7888446215139443
        },
        "popular (POPE)": {
            "acc": 0.8013029315960912,
            "prec": 0.8809523809523809,
            "rec": 0.7070063694267515,
            "f1": 0.7844522968197879
        },
        "adversarial (POPE)": {
            "acc": 0.8356643356643356,
            "prec": 0.8695652173913043,
            "rec": 0.7575757575757576,
            "f1": 0.8097165991902834
        }
    },
    "MiniGPT4-LLaMA2-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.35491503549150355
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.20159680638722555
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.3670012547051443
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.32934131736526945
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.424
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2958724969350225
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.18693877551020407
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.432183908045977
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23015873015873015
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24519549370444002
        },
        "Instance Location (SEED_2)": {
            "acc": 0.37014314928425357
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.29896907216494845
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.3983533882203927
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.3582741671217914
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.6962025316455697
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.33532934131736525
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.3272450532724505
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.3176895306859206
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.1573802541544477
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.4350453172205438
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.23333333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18238993710691823
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.41515151515151516
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.1708542713567839
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.12244897959183673
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2716049382716049
        },
        "celebrity (MME)": {
            "acc": 0.5911764705882353,
            "prec": 0.5701357466063348,
            "rec": 0.7411764705882353,
            "f1": 0.6445012787723785
        },
        "posters (MME)": {
            "acc": 0.3469387755102041,
            "prec": 0.3469387755102041,
            "rec": 0.3469387755102041,
            "f1": 0.3469387755102041
        },
        "position (MME)": {
            "acc": 0.45,
            "prec": 0.46808510638297873,
            "rec": 0.7333333333333333,
            "f1": 0.5714285714285714
        },
        "scene (MME)": {
            "acc": 0.69,
            "prec": 0.7159090909090909,
            "rec": 0.63,
            "f1": 0.6702127659574468
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.2,
            "prec": 0.171875,
            "rec": 0.15714285714285714,
            "f1": 0.16417910447761194
        },
        "artwork (MME)": {
            "acc": 0.4675,
            "prec": 0.41975308641975306,
            "rec": 0.17,
            "f1": 0.2419928825622776
        },
        "landmark (MME)": {
            "acc": 0.7,
            "prec": 0.7985074626865671,
            "rec": 0.535,
            "f1": 0.6407185628742514
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 0.5714285714285714,
            "rec": 0.6,
            "f1": 0.5853658536585366
        },
        "existence (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.6170212765957447,
            "rec": 0.9666666666666667,
            "f1": 0.7532467532467533
        },
        "numerical_calculation (MME)": {
            "acc": 0.025,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.43333333333333335,
            "prec": 0.46296296296296297,
            "rec": 0.8333333333333334,
            "f1": 0.5952380952380952
        },
        "color (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5135135135135135,
            "rec": 0.95,
            "f1": 0.6666666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.3333333333333333,
            "rec": 0.05,
            "f1": 0.08695652173913045
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.37790697674418605
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.3047619047619048
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.19230769230769232
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2785388127853881
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.1773049645390071
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.30726256983240224
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.6046511627906976
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.4914004914004914
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.30808080808080807
        },
        "ocr (MMBench_CN)": {
            "acc": 0.3269230769230769
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.24822695035460993
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.32
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.34868421052631576
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.5852272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.3632075471698113
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.39015151515151514
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.20666666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.55
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.3023255813953488
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.326984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3242009132420091
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3404255319148936
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2737430167597765
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.6
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.5872235872235873
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.49242424242424243
        },
        "ocr (MMBench_EN)": {
            "acc": 0.33974358974358976
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.288135593220339
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.63
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.5032894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2872340425531915
        },
        "image_style (MMBench_EN)": {
            "acc": 0.5566037735849056
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.6287878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.32
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.3357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.0
        },
        "Pharmacy (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.1
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2540983606557377
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3058823529411765
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.22549019607843138
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4931506849315068
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "State capitals (ScienceQA)": {
            "acc": 0.5288461538461539
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.6164383561643836
        },
        "States of matter (ScienceQA)": {
            "acc": 0.5357142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.6363636363636364
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5306122448979592
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.21951219512195122
        },
        "Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.46875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Classification (ScienceQA)": {
            "acc": 0.569620253164557
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.24873096446700507
        },
        "3D Distance (CVBench)": {
            "acc": 0.465
        },
        "2D Relation (CVBench)": {
            "acc": 0.5215384615384615
        },
        "3D Depth (CVBench)": {
            "acc": 0.505
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.810799815654755,
            "bert": 0.7921458530426025
        },
        "Whole dataset (Enrico)": {
            "bart": -7.218411245346069,
            "bert": 0.8051163291931153
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.799822578430176,
            "bert": 0.8201032638549804
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.8710048353672026,
            "bert": 0.8448650795221329
        },
        "Whole dataset (GQA)": {
            "bart": -4.1552281975746155,
            "bert": 0.9773452669382096
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.96235303401947,
            "bert": 0.790307171344757
        },
        "Whole dataset (INAT)": {
            "bart": -6.825651849508286,
            "bert": 0.7646924144029618
        },
        "Whole dataset (IRFL)": {
            "bart": -4.574841538667679,
            "bert": 0.9897291702032089
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.873610830307007,
            "bert": 0.8569518208503724
        },
        "Whole dataset (Memotion)": {
            "bart": -5.277067141532898,
            "bert": 0.8228661280870437
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.3403023833036425,
            "bert": 0.8202530813217163
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -8.091323857307435,
            "bert": 0.7939045590162277
        },
        "Whole dataset (NLVR)": {
            "bart": -5.877051365375519,
            "bert": 0.805886237025261
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.1515439820289615,
            "bert": 0.8238165557384491
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.9783543288707732,
            "bert": 0.8665001231431961
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.6731561911106105,
            "bert": 0.8785367274284362
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.207258453369141,
            "bert": 0.8024875289201736
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.147256712913514,
            "bert": 0.8897154021263123
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.339061088562012,
            "bert": 0.8090033787488937
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.006514363288879,
            "bert": 0.8404362893104553
        },
        "Whole dataset (Slake)": {
            "bart": -4.5395334017276765,
            "bert": 0.9326555782556534
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.331778316497803,
            "bert": 0.7979413866996765
        },
        "Whole dataset (VCR)": {
            "bart": -4.073927264213562,
            "bert": 0.8563197916746139
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.1399896478652956,
            "bert": 0.8976585394144059
        },
        "Whole dataset (VQA)": {
            "bart": -5.453605756759644,
            "bert": 0.9228336781263351
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.767652740478516,
            "bert": 0.9127119350433349
        },
        "Whole dataset (Winoground)": {
            "bart": -6.007180404663086,
            "bert": 0.8226325684785842
        },
        "random (POPE)": {
            "acc": 0.511400651465798,
            "prec": 0.4966216216216216,
            "rec": 0.9932432432432432,
            "f1": 0.6621621621621621
        },
        "popular (POPE)": {
            "acc": 0.5179153094462541,
            "prec": 0.5151515151515151,
            "rec": 0.9745222929936306,
            "f1": 0.6740088105726872
        },
        "adversarial (POPE)": {
            "acc": 0.46503496503496505,
            "prec": 0.4631578947368421,
            "rec": 1.0,
            "f1": 0.6330935251798561
        }
    },
    "MiniGPT4-Vicuna0-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.19337491933749193
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.17565872020075282
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.222
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.21495709031467103
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2179591836734694
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2206896551724138
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23412698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2345924453280318
        },
        "Instance Location (SEED_2)": {
            "acc": 0.24233128834355827
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.25773195876288657
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.2422419252691577
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2348443473511742
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.14393939393939395
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2874251497005988
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2328767123287671
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.30324909747292417
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.1466275659824047
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.23564954682779457
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25833333333333336
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.20606060606060606
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.20100502512562815
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2857142857142857
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.5176470588235295,
            "prec": 0.6666666666666666,
            "rec": 0.07058823529411765,
            "f1": 0.12765957446808512
        },
        "posters (MME)": {
            "acc": 0.3197278911564626,
            "prec": 0.1267605633802817,
            "rec": 0.061224489795918366,
            "f1": 0.08256880733944953
        },
        "position (MME)": {
            "acc": 0.4,
            "prec": 0.40625,
            "rec": 0.43333333333333335,
            "f1": 0.41935483870967744
        },
        "scene (MME)": {
            "acc": 0.435,
            "prec": 0.3488372093023256,
            "rec": 0.15,
            "f1": 0.20979020979020982
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.19285714285714287,
            "prec": 0.16923076923076924,
            "rec": 0.15714285714285714,
            "f1": 0.16296296296296298
        },
        "artwork (MME)": {
            "acc": 0.0125,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "landmark (MME)": {
            "acc": 0.08,
            "prec": 0.0058823529411764705,
            "rec": 0.005,
            "f1": 0.005405405405405405
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5625,
            "rec": 0.3,
            "f1": 0.3913043478260869
        },
        "numerical_calculation (MME)": {
            "acc": 0.15,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.38333333333333336,
            "prec": 0.4,
            "rec": 0.4666666666666667,
            "f1": 0.4307692307692308
        },
        "color (MME)": {
            "acc": 0.43333333333333335,
            "prec": 0.43333333333333335,
            "rec": 0.43333333333333335,
            "f1": 0.43333333333333335
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.65,
            "f1": 0.5652173913043479
        },
        "code_reasoning (MME)": {
            "acc": 0.25,
            "prec": 0.32142857142857145,
            "rec": 0.45,
            "f1": 0.375
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.26744186046511625
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2634920634920635
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2785388127853881
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2198581560283688
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.26256983240223464
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.3116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.4226044226044226
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2828282828282828
        },
        "ocr (MMBench_CN)": {
            "acc": 0.32051282051282054
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2768361581920904
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2198581560283688
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.44
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.3223684210526316
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.5454545454545454
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2553191489361702
        },
        "image_style (MMBench_CN)": {
            "acc": 0.39622641509433965
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.38257575757575757
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.34
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.42857142857142855
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.27906976744186046
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2507936507936508
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2420091324200913
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2695035460992908
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2122905027932961
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.2800982800982801
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2803030303030303
        },
        "ocr (MMBench_EN)": {
            "acc": 0.19230769230769232
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.295
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.3092105263157895
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.3465909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2971698113207547
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.3143939393939394
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.24
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2642857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.0
        },
        "Math (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.0
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.0
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.0
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1111111111111111
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.18181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.5342465753424658
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "State capitals (ScienceQA)": {
            "acc": 0.23397435897435898
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.5616438356164384
        },
        "States of matter (ScienceQA)": {
            "acc": 0.5357142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.35664335664335667
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.46938775510204084
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.38095238095238093
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.08888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.4375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4074074074074074
        },
        "Maps (ScienceQA)": {
            "acc": 0.17391304347826086
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.35294117647058826
        },
        "Classification (ScienceQA)": {
            "acc": 0.43037974683544306
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.2535211267605634
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.1548223350253807
        },
        "3D Distance (CVBench)": {
            "acc": 0.43833333333333335
        },
        "2D Relation (CVBench)": {
            "acc": 0.5015384615384615
        },
        "3D Depth (CVBench)": {
            "acc": 0.495
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.951579463481903,
            "bert": 0.7732093608379365
        },
        "Whole dataset (Enrico)": {
            "bart": -7.170467548370361,
            "bert": 0.8897698503732682
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.5116455078125,
            "bert": 0.8132851165533066
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.497836759090424,
            "bert": 0.887602646946907
        },
        "Whole dataset (GQA)": {
            "bart": -6.197926024198532,
            "bert": 0.9204564172029496
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.152660570144653,
            "bert": 0.8200606471300125
        },
        "Whole dataset (INAT)": {
            "bart": -6.606054105758667,
            "bert": 0.7730320411920547
        },
        "Whole dataset (IRFL)": {
            "bart": -5.538346436023712,
            "bert": 0.8817986565828323
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.119238545894623,
            "bert": 0.8497397673130035
        },
        "Whole dataset (Memotion)": {
            "bart": -5.1960964012146,
            "bert": 0.8143949341773987
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.6843892145156865,
            "bert": 0.78666450381279
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.47103859424591,
            "bert": 0.5391433572769165
        },
        "Whole dataset (NLVR)": {
            "bart": -6.297097706794739,
            "bert": 0.922824792265892
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.693289313316345,
            "bert": 0.878253487944603
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.637996505498886,
            "bert": 0.9136415266990662
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.289817541837692,
            "bert": 0.8059235203266144
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.603776437044144,
            "bert": 0.8245888370275497
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.5139604616165165,
            "bert": 0.5879745638370514
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.874355192184448,
            "bert": 0.8600733864307404
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.074284343719483,
            "bert": 0.8476643997430802
        },
        "Whole dataset (Slake)": {
            "bart": -5.753858730793,
            "bert": 0.9015647804737091
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.9849400639534,
            "bert": 0.7894957554340363
        },
        "Whole dataset (VCR)": {
            "bart": -4.144409771561623,
            "bert": 0.9009757417440415
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.439992296695709,
            "bert": 0.9005272156000137
        },
        "Whole dataset (VQA)": {
            "bart": -6.0878134846687315,
            "bert": 0.8956948202848435
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.597200133800507,
            "bert": 0.5405997824668884
        },
        "Whole dataset (Winoground)": {
            "bart": -6.769330258369446,
            "bert": 0.8108768087625503
        },
        "random (POPE)": {
            "acc": 0.5798045602605864,
            "prec": 0.7209302325581395,
            "rec": 0.20945945945945946,
            "f1": 0.3246073298429319
        },
        "popular (POPE)": {
            "acc": 0.5602605863192183,
            "prec": 0.7291666666666666,
            "rec": 0.2229299363057325,
            "f1": 0.34146341463414637
        },
        "adversarial (POPE)": {
            "acc": 0.6013986013986014,
            "prec": 0.6730769230769231,
            "rec": 0.26515151515151514,
            "f1": 0.3804347826086957
        }
    },
    "MiniGPT4-Vicuna0-13B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.28565282856528285
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.4251497005988024
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.3174404015056462
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2834331337325349
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.3
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.30036779730281976
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2530612244897959
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.42758620689655175
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23412698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.31809145129224653
        },
        "Instance Location (SEED_2)": {
            "acc": 0.2832310838445808
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.4020618556701031
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.3157061431285624
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2960131075914801
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.7088607594936709
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.21212121212121213
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.3028919330289193
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.33935018050541516
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.14173998044965788
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.3474320241691843
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.29559748427672955
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3939393939393939
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.27638190954773867
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.24489795918367346
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.18518518518518517
        },
        "celebrity (MME)": {
            "acc": 0.4970588235294118,
            "prec": 0.3333333333333333,
            "rec": 0.0058823529411764705,
            "f1": 0.011560693641618498
        },
        "posters (MME)": {
            "acc": 0.3741496598639456,
            "prec": 0.352,
            "rec": 0.29931972789115646,
            "f1": 0.32352941176470584
        },
        "position (MME)": {
            "acc": 0.23333333333333334,
            "prec": 0.2777777777777778,
            "rec": 0.3333333333333333,
            "f1": 0.303030303030303
        },
        "scene (MME)": {
            "acc": 0.71,
            "prec": 0.7258064516129032,
            "rec": 0.675,
            "f1": 0.6994818652849741
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.32857142857142857,
            "prec": 0.3064516129032258,
            "rec": 0.2714285714285714,
            "f1": 0.28787878787878785
        },
        "artwork (MME)": {
            "acc": 0.6,
            "prec": 0.7222222222222222,
            "rec": 0.325,
            "f1": 0.44827586206896547
        },
        "landmark (MME)": {
            "acc": 0.2525,
            "prec": 0.18064516129032257,
            "rec": 0.14,
            "f1": 0.15774647887323942
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.6333333333333333,
            "rec": 0.6333333333333333,
            "f1": 0.6333333333333333
        },
        "numerical_calculation (MME)": {
            "acc": 0.125,
            "prec": 0.058823529411764705,
            "rec": 0.05,
            "f1": 0.05405405405405405
        },
        "count (MME)": {
            "acc": 0.2833333333333333,
            "prec": 0.35555555555555557,
            "rec": 0.5333333333333333,
            "f1": 0.42666666666666675
        },
        "color (MME)": {
            "acc": 0.3333333333333333,
            "prec": 0.3684210526315789,
            "rec": 0.4666666666666667,
            "f1": 0.4117647058823529
        },
        "OCR (MME)": {
            "acc": 0.225,
            "prec": 0.2962962962962963,
            "rec": 0.4,
            "f1": 0.3404255319148936
        },
        "code_reasoning (MME)": {
            "acc": 0.05,
            "prec": 0.05,
            "rec": 0.05,
            "f1": 0.05000000000000001
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.27325581395348836
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.28888888888888886
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.27692307692307694
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2420091324200913
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2346368715083799
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.3209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.3759213759213759
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.28535353535353536
        },
        "ocr (MMBench_CN)": {
            "acc": 0.3141025641025641
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2824858757062147
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.23049645390070922
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.41
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.3256578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.3352272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_CN)": {
            "acc": 0.42452830188679247
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.3560606060606061
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.26
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.39285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.26744186046511625
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.326984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.34615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3105022831050228
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2978723404255319
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.35195530726256985
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.40930232558139534
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.3857493857493858
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.45454545454545453
        },
        "ocr (MMBench_EN)": {
            "acc": 0.3076923076923077
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2695035460992908
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.49
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.5394736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8465909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2872340425531915
        },
        "image_style (MMBench_EN)": {
            "acc": 0.49056603773584906
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.5757575757575758
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.3142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.0
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.0
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.0
        },
        "Marketing (MMMU)": {
            "acc": 0.0
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.18253968253968253
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1715686274509804
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6301369863013698
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.5096153846153846
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9041095890410958
        },
        "States of matter (ScienceQA)": {
            "acc": 0.5357142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.6013986013986014
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.6938775510204082
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1935483870967742
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.14634146341463414
        },
        "Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.7818181818181819
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.65625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Classification (ScienceQA)": {
            "acc": 0.6329113924050633
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.24492385786802032
        },
        "3D Distance (CVBench)": {
            "acc": 0.41833333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.49846153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.3566666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.032300910949707,
            "bert": 0.7913258516788483
        },
        "Whole dataset (Enrico)": {
            "bart": -7.074942421913147,
            "bert": 0.7894330167770386
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.137508602142334,
            "bert": 0.8842587476968765
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.020994575023651,
            "bert": 0.8603472220897674
        },
        "Whole dataset (GQA)": {
            "bart": -6.5950591599941255,
            "bert": 0.8169126671552658
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.4302889919281006,
            "bert": 0.7940821725130082
        },
        "Whole dataset (INAT)": {
            "bart": -6.469945876598358,
            "bert": 0.785131134390831
        },
        "Whole dataset (IRFL)": {
            "bart": -6.2701688933372495,
            "bert": 0.9064034551382065
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.7638288176059724,
            "bert": 0.8526149153709411
        },
        "Whole dataset (Memotion)": {
            "bart": -5.391027843952179,
            "bert": 0.8588308769464493
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.499166402816773,
            "bert": 0.853313775062561
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.339654965400696,
            "bert": 0.7929160112142563
        },
        "Whole dataset (NLVR)": {
            "bart": -3.8024346470832824,
            "bert": 0.9674602162837982
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7914672183990477,
            "bert": 0.9599053448438645
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.1746457481384276,
            "bert": 0.8882518655061722
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.476030011177063,
            "bert": 0.8449946308135986
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.541651955842972,
            "bert": 0.7897033357620239
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.4364138007164,
            "bert": 0.8139769107103347
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.180639221668243,
            "bert": 0.8067779183387757
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.090678102970124,
            "bert": 0.8491875612735749
        },
        "Whole dataset (Slake)": {
            "bart": -6.413522001504898,
            "bert": 0.8571744459867477
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.123822083473206,
            "bert": 0.7685105448961258
        },
        "Whole dataset (VCR)": {
            "bart": -4.031073977947235,
            "bert": 0.8962900334596634
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.607397725582123,
            "bert": 0.8329849755764007
        },
        "Whole dataset (VQA)": {
            "bart": -6.4090783143043515,
            "bert": 0.8270801830291749
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.467119634151459,
            "bert": 0.8404996812343597
        },
        "Whole dataset (Winoground)": {
            "bart": -7.259985785484314,
            "bert": 0.8353707218170165
        },
        "random (POPE)": {
            "acc": 0.15960912052117263,
            "prec": 0.008928571428571428,
            "rec": 0.006756756756756757,
            "f1": 0.007692307692307693
        },
        "popular (POPE)": {
            "acc": 0.14332247557003258,
            "prec": 0.009259259259259259,
            "rec": 0.006369426751592357,
            "f1": 0.007547169811320756
        },
        "adversarial (POPE)": {
            "acc": 0.11888111888111888,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        }
    },
    "mPLUG-Owl2-LLaMA2-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6360507636050764
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5988023952095808
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5219573400250941
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2315369261477046
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.846
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5925623212096445
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.39510204081632655
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5494252873563218
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.42956349206349204
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4300861497680583
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5439672801635992
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7298923369221026
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6728563626433642
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.3872255489021956
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5068493150684932
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4981949458483754
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3255131964809384
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7764350453172205
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.21666666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.42138364779874216
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8484848484848485
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5778894472361809
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.32653061224489793
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.1728395061728395
        },
        "celebrity (MME)": {
            "acc": 0.8647058823529412,
            "prec": 0.9920634920634921,
            "rec": 0.7352941176470589,
            "f1": 0.8445945945945946
        },
        "posters (MME)": {
            "acc": 0.8129251700680272,
            "prec": 0.9693877551020408,
            "rec": 0.6462585034013606,
            "f1": 0.7755102040816327
        },
        "position (MME)": {
            "acc": 0.55,
            "prec": 0.5283018867924528,
            "rec": 0.9333333333333333,
            "f1": 0.6746987951807228
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.949685534591195,
            "rec": 0.755,
            "f1": 0.8412256267409471
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.6704545454545454,
            "rec": 0.8428571428571429,
            "f1": 0.7468354430379747
        },
        "artwork (MME)": {
            "acc": 0.785,
            "prec": 0.7794117647058824,
            "rec": 0.795,
            "f1": 0.787128712871287
        },
        "landmark (MME)": {
            "acc": 0.77,
            "prec": 0.9821428571428571,
            "rec": 0.55,
            "f1": 0.7051282051282052
        },
        "text_translation (MME)": {
            "acc": 0.7,
            "prec": 0.7222222222222222,
            "rec": 0.65,
            "f1": 0.6842105263157895
        },
        "existence (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.9629629629629629,
            "rec": 0.8666666666666667,
            "f1": 0.912280701754386
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.3,
            "rec": 0.15,
            "f1": 0.2
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8055555555555556,
            "rec": 0.9666666666666667,
            "f1": 0.8787878787878789
        },
        "color (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7317073170731707,
            "rec": 1.0,
            "f1": 0.8450704225352113
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6060606060606061,
            "rec": 1.0,
            "f1": 0.7547169811320755
        },
        "code_reasoning (MME)": {
            "acc": 0.375,
            "prec": 0.3333333333333333,
            "rec": 0.25,
            "f1": 0.28571428571428575
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.872093023255814
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.4888888888888889
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4977168949771689
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5602836879432624
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5083798882681564
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8790697674418605
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8106060606060606
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3615819209039548
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.3723404255319149
        },
        "image_style (MMBench_CN)": {
            "acc": 0.839622641509434
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7689393939393939
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8857142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5428571428571428
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6210045662100456
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5810055865921788
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8837209302325582
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9065656565656566
        },
        "ocr (MMBench_EN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5141843971631206
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8618421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9292452830188679
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8068181818181818
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.35333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.1
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.25
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18137254901960784
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.8531468531468531
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9090909090909091
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.45098039215686275
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5211267605633803
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.565989847715736
        },
        "3D Distance (CVBench)": {
            "acc": 0.5616666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.5692307692307692
        },
        "3D Depth (CVBench)": {
            "acc": 0.74
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.746612434387207,
            "bert": 0.7883062386512756
        },
        "Whole dataset (Enrico)": {
            "bart": -5.5638652038574214,
            "bert": 0.9890404915809632
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.09660360455513,
            "bert": 0.9992465847730636
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2786208820343017,
            "bert": 0.8650129640102386
        },
        "Whole dataset (GQA)": {
            "bart": -3.9608646488189696,
            "bert": 0.9936220556497574
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.507883987426758,
            "bert": 0.8141994112730027
        },
        "Whole dataset (INAT)": {
            "bart": -6.275110049247742,
            "bert": 0.7967429912090301
        },
        "Whole dataset (IRFL)": {
            "bart": -4.206475217342376,
            "bert": 0.9986704939603805
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.602734251022339,
            "bert": 0.8662764763832093
        },
        "Whole dataset (Memotion)": {
            "bart": -4.709148273468018,
            "bert": 0.9077485060691833
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.4457275807857513,
            "bert": 0.8910039502382279
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.5434328305721285,
            "bert": 0.9983396291732788
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9995363056659698
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.524619126319885,
            "bert": 0.9995826703310012
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.422149146795273,
            "bert": 0.8837297993898392
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.197481468319893,
            "bert": 0.9363548916578293
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.065745344758033,
            "bert": 0.8385012012720108
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.204160150289535,
            "bert": 0.908220664858818
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.188109055757523,
            "bert": 0.9407487523555755
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.159143037796021,
            "bert": 0.8618515986204147
        },
        "Whole dataset (Slake)": {
            "bart": -3.701626191139221,
            "bert": 0.9909361535310746
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.21443354845047,
            "bert": 0.918247047662735
        },
        "Whole dataset (VCR)": {
            "bart": -3.354721710681915,
            "bert": 0.9214894485473633
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.542208334207535,
            "bert": 0.9079737013578415
        },
        "Whole dataset (VQA)": {
            "bart": -4.349558652639389,
            "bert": 0.9767402672767639
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.531935737133026,
            "bert": 0.940104924440384
        },
        "Whole dataset (Winoground)": {
            "bart": -4.382363708019256,
            "bert": 0.9979371041059494
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.916030534351145,
            "rec": 0.8108108108108109,
            "f1": 0.860215053763441
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.8657718120805369,
            "rec": 0.821656050955414,
            "f1": 0.8431372549019608
        },
        "adversarial (POPE)": {
            "acc": 0.8391608391608392,
            "prec": 0.8359375,
            "rec": 0.8106060606060606,
            "f1": 0.8230769230769232
        }
    },
    "mPLUG-Owl2_1": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6840180684018068
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5868263473053892
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.501254705144291
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27944111776447106
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.884
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6191254597466285
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3836734693877551
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.593103448275862
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.310515873015873
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.441351888667992
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6513292433537833
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7628865979381443
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.740658644711843
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7007099945385036
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.20454545454545456
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.3253493013972056
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5235920852359208
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.48375451263537905
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.27956989247311825
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.743202416918429
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.36666666666666664
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.33962264150943394
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.896969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.19753086419753085
        },
        "celebrity (MME)": {
            "acc": 0.888235294117647,
            "prec": 0.9177215189873418,
            "rec": 0.8529411764705882,
            "f1": 0.8841463414634145
        },
        "posters (MME)": {
            "acc": 0.8741496598639455,
            "prec": 0.9508196721311475,
            "rec": 0.7891156462585034,
            "f1": 0.862453531598513
        },
        "position (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8484848484848485,
            "rec": 0.9333333333333333,
            "f1": 0.888888888888889
        },
        "scene (MME)": {
            "acc": 0.865,
            "prec": 0.8762886597938144,
            "rec": 0.85,
            "f1": 0.8629441624365483
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.6753246753246753,
            "rec": 0.7428571428571429,
            "f1": 0.7074829931972789
        },
        "artwork (MME)": {
            "acc": 0.7625,
            "prec": 0.7091633466135459,
            "rec": 0.89,
            "f1": 0.7893569844789359
        },
        "landmark (MME)": {
            "acc": 0.9275,
            "prec": 0.934010152284264,
            "rec": 0.92,
            "f1": 0.9269521410579346
        },
        "text_translation (MME)": {
            "acc": 0.825,
            "prec": 0.8095238095238095,
            "rec": 0.85,
            "f1": 0.8292682926829269
        },
        "existence (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7317073170731707,
            "rec": 1.0,
            "f1": 0.8450704225352113
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.35294117647058826,
            "rec": 0.3,
            "f1": 0.3243243243243243
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.6976744186046512,
            "rec": 1.0,
            "f1": 0.8219178082191781
        },
        "color (MME)": {
            "acc": 0.8,
            "prec": 0.7368421052631579,
            "rec": 0.9333333333333333,
            "f1": 0.8235294117647058
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "code_reasoning (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5873015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5205479452054794
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5460992907801419
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5810055865921788
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.9393939393939394
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7948717948717948
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5283687943262412
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8322368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8962264150943396
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7840909090909091
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6952380952380952
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.38461538461538464
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.680365296803653
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6737588652482269
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7318435754189944
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9494949494949495
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8269230769230769
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.423728813559322
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8914473684210527
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9245283018867925
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4666666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8642857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.1
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27459016393442626
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.27058823529411763
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.38636363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7945205479452054
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9198717948717948
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.2903225806451613
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.5181818181818182
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4666666666666667
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9130434782608695
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8909090909090909
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5740740740740741
        },
        "Maps (ScienceQA)": {
            "acc": 0.4782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Classification (ScienceQA)": {
            "acc": 0.7341772151898734
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6551724137931034
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.5380710659898477
        },
        "3D Distance (CVBench)": {
            "acc": 0.64
        },
        "2D Relation (CVBench)": {
            "acc": 0.6692307692307692
        },
        "3D Depth (CVBench)": {
            "acc": 0.7066666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.822976865768433,
            "bert": 0.7981548535823823
        },
        "Whole dataset (Enrico)": {
            "bart": -5.913690524101257,
            "bert": 0.9767786222696304
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.070334078073501,
            "bert": 0.9992431277036666
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3095743215084075,
            "bert": 0.8672737407684327
        },
        "Whole dataset (GQA)": {
            "bart": -4.078364828824997,
            "bert": 0.9885280394554138
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.38796177983284,
            "bert": 0.933881110548973
        },
        "Whole dataset (INAT)": {
            "bart": -6.00897786617279,
            "bert": 0.7942555338144303
        },
        "Whole dataset (IRFL)": {
            "bart": -4.116903569698334,
            "bert": 0.9986909645795822
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.5034658908843994,
            "bert": 0.8678702974319458
        },
        "Whole dataset (Memotion)": {
            "bart": -3.7585914373397826,
            "bert": 0.9072811806201935
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.5065477871894837,
            "bert": 0.9027167809009552
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.429741665124893,
            "bert": 0.9983676421642304
        },
        "Whole dataset (NLVR)": {
            "bart": -3.309950361251831,
            "bert": 0.9995919728279113
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.650194568634033,
            "bert": 0.9997360289096833
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.3562725138664247,
            "bert": 0.8904061800241471
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.593813344836235,
            "bert": 0.9119101995229721
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.38027052462101,
            "bert": 0.8269370287656784
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.151589468717575,
            "bert": 0.9140652334690094
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.422251702547073,
            "bert": 0.9116439318656921
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.8421726274490355,
            "bert": 0.8616773903369903
        },
        "Whole dataset (Slake)": {
            "bart": -3.967611700296402,
            "bert": 0.9827029460668564
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.318761830329895,
            "bert": 0.8063769155740738
        },
        "Whole dataset (VCR)": {
            "bart": -3.500136910676956,
            "bert": 0.9187815958261489
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.565400791168213,
            "bert": 0.9073945754766464
        },
        "Whole dataset (VQA)": {
            "bart": -4.867190266847611,
            "bert": 0.9507356476783753
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.07465532541275,
            "bert": 0.9398747527599335
        },
        "Whole dataset (Winoground)": {
            "bart": -4.319780427217483,
            "bert": 0.9979503917694091
        },
        "random (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9256198347107438,
            "rec": 0.7567567567567568,
            "f1": 0.8327137546468402
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9051094890510949,
            "rec": 0.7898089171974523,
            "f1": 0.8435374149659864
        },
        "adversarial (POPE)": {
            "acc": 0.8461538461538461,
            "prec": 0.8728813559322034,
            "rec": 0.7803030303030303,
            "f1": 0.8240000000000001
        }
    },
    "LLaVA-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6618627661862766
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6586826347305389
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.513801756587202
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.848
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5811197384552513
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.40244897959183673
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4528735632183908
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23809523809523808
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4088800530152419
        },
        "Instance Location (SEED_2)": {
            "acc": 0.588957055214724
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7010309278350515
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7355921469284357
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6690333151283452
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5249500998003992
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5144596651445966
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5090252707581228
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3343108504398827
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7854984894259819
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.44654088050314467
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8303030303030303
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6331658291457286
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.7823529411764706,
            "prec": 0.9363636363636364,
            "rec": 0.6058823529411764,
            "f1": 0.7357142857142857
        },
        "posters (MME)": {
            "acc": 0.7687074829931972,
            "prec": 0.9759036144578314,
            "rec": 0.5510204081632653,
            "f1": 0.7043478260869563
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6857142857142857,
            "rec": 0.8,
            "f1": 0.7384615384615385
        },
        "scene (MME)": {
            "acc": 0.845,
            "prec": 0.9423076923076923,
            "rec": 0.735,
            "f1": 0.8258426966292134
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7285714285714285,
            "prec": 0.7222222222222222,
            "rec": 0.7428571428571429,
            "f1": 0.732394366197183
        },
        "artwork (MME)": {
            "acc": 0.73,
            "prec": 0.7371134020618557,
            "rec": 0.715,
            "f1": 0.7258883248730965
        },
        "landmark (MME)": {
            "acc": 0.8775,
            "prec": 0.8994708994708994,
            "rec": 0.85,
            "f1": 0.8740359897172236
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.25,
            "rec": 0.1,
            "f1": 0.14285714285714288
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8571428571428571,
            "rec": 0.8,
            "f1": 0.8275862068965518
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.8,
            "rec": 0.9333333333333333,
            "f1": 0.8615384615384616
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.7777777777777778,
            "rec": 0.7,
            "f1": 0.7368421052631577
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5384615384615384,
            "rec": 0.35,
            "f1": 0.4242424242424242
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8604651162790697
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5142857142857142
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4657534246575342
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5602836879432624
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5977653631284916
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7803030303030303
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.41843971631205673
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.795
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.648936170212766
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7971698113207547
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7234848484848485
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.41333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5841269841269842
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5384615384615384
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5616438356164384
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.41843971631205673
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8443396226415094
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8571428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6666666666666666
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.11904761904761904
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2222222222222222
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.19607843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.38636363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9315068493150684
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.7972027972027972
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.13725490196078433
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7647058823529411
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5469543147208121
        },
        "3D Distance (CVBench)": {
            "acc": 0.5383333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6584615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.7183333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.817549507617951,
            "bert": 0.7908032763004303
        },
        "Whole dataset (Enrico)": {
            "bart": -6.79784135222435,
            "bert": 0.9602552115917206
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.9417745196819305,
            "bert": 0.9786695492267609
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3017472159862518,
            "bert": 0.8669015127420425
        },
        "Whole dataset (GQA)": {
            "bart": -3.734028012752533,
            "bert": 0.9908311414718628
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.835832540988922,
            "bert": 0.8072480243444443
        },
        "Whole dataset (INAT)": {
            "bart": -6.230460774898529,
            "bert": 0.7915394896268845
        },
        "Whole dataset (IRFL)": {
            "bart": -4.331641778945923,
            "bert": 0.998643918633461
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.974736423492432,
            "bert": 0.8502628785371781
        },
        "Whole dataset (Memotion)": {
            "bart": -4.040883309841156,
            "bert": 0.9053909426927567
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7820967108011248,
            "bert": 0.8794166439771652
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.056637191772461,
            "bert": 0.9916466581821441
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.399775473475456,
            "bert": 0.8866179907321929
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.261044557094574,
            "bert": 0.9395604610443116
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.001306726336479,
            "bert": 0.8349455624818802
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.01955699801445,
            "bert": 0.9119332540035248
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.59524160027504,
            "bert": 0.9066331166028977
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.22669559955597,
            "bert": 0.8524310612678527
        },
        "Whole dataset (Slake)": {
            "bart": -3.819754041433334,
            "bert": 0.9910739296674729
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.423347067832947,
            "bert": 0.8900685334205627
        },
        "Whole dataset (VCR)": {
            "bart": -3.1787890040874482,
            "bert": 0.9245484209060669
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.608805062174797,
            "bert": 0.9097419291734695
        },
        "Whole dataset (VQA)": {
            "bart": -4.390084266662598,
            "bert": 0.9732273900508881
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.524716817140579,
            "bert": 0.9404468786716461
        },
        "Whole dataset (Winoground)": {
            "bart": -4.151528165340424,
            "bert": 0.9979979795217514
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9736842105263158,
            "rec": 0.75,
            "f1": 0.8473282442748091
        },
        "popular (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.9076923076923077,
            "rec": 0.7515923566878981,
            "f1": 0.8222996515679443
        },
        "adversarial (POPE)": {
            "acc": 0.8356643356643356,
            "prec": 0.8899082568807339,
            "rec": 0.7348484848484849,
            "f1": 0.8049792531120332
        }
    },
    "LLaVA-13B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6792858679285868
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6467065868263473
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5357590966122961
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2774451097804391
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.838
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6093175316714344
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4220408163265306
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5379310344827586
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25892857142857145
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4340622929092114
        },
        "Instance Location (SEED_2)": {
            "acc": 0.630879345603272
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7498416719442685
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.70398689240852
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5449101796407185
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5190258751902588
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.592057761732852
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.37927663734115347
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7673716012084593
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.43333333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5094339622641509
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8090909090909091
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6633165829145728
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2857142857142857
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2962962962962963
        },
        "celebrity (MME)": {
            "acc": 0.8705882352941177,
            "prec": 0.9256756756756757,
            "rec": 0.8058823529411765,
            "f1": 0.8616352201257862
        },
        "posters (MME)": {
            "acc": 0.8435374149659864,
            "prec": 0.9173553719008265,
            "rec": 0.7551020408163265,
            "f1": 0.8283582089552239
        },
        "position (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.627906976744186,
            "rec": 0.9,
            "f1": 0.7397260273972602
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8907103825136612,
            "rec": 0.815,
            "f1": 0.8511749347258486
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7428571428571429,
            "prec": 0.717948717948718,
            "rec": 0.8,
            "f1": 0.7567567567567569
        },
        "artwork (MME)": {
            "acc": 0.735,
            "prec": 0.6974789915966386,
            "rec": 0.83,
            "f1": 0.7579908675799087
        },
        "landmark (MME)": {
            "acc": 0.8325,
            "prec": 0.9523809523809523,
            "rec": 0.7,
            "f1": 0.8069164265129684
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.375,
            "rec": 0.3,
            "f1": 0.33333333333333326
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.725,
            "rec": 0.9666666666666667,
            "f1": 0.8285714285714285
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.8,
            "prec": 0.75,
            "rec": 0.9,
            "f1": 0.8181818181818182
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.45454545454545453,
            "rec": 0.5,
            "f1": 0.47619047619047616
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8023255813953488
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.580952380952381
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5923076923076923
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5251141552511416
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5027932960893855
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9255813953488372
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7676767676767676
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7435897435897436
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.423728813559322
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.549645390070922
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.815
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8355263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.46808510638297873
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7405660377358491
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7916666666666666
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5650793650793651
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6153846153846154
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5662100456621004
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6595744680851063
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5754189944134078
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8863636363636364
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4180790960451977
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5177304964539007
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8584905660377359
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8787878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.28104575163398693
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.24705882352941178
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20098039215686275
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.375
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.8531468531468531
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.20967741935483872
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.8095238095238095
        },
        "Magnets (ScienceQA)": {
            "acc": 0.39090909090909093
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.8461538461538461
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7647058823529411
        },
        "Classification (ScienceQA)": {
            "acc": 0.8987341772151899
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.567258883248731
        },
        "3D Distance (CVBench)": {
            "acc": 0.5816666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.6476923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.7216666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.66318128824234,
            "bert": 0.8183043706417084
        },
        "Whole dataset (Enrico)": {
            "bart": -5.581485197544098,
            "bert": 0.9872999465465546
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.928792847394943,
            "bert": 0.9813384211063385
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.20494509100914,
            "bert": 0.876506375670433
        },
        "Whole dataset (GQA)": {
            "bart": -3.5415072798728944,
            "bert": 0.9937592810392379
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.1815859937667845,
            "bert": 0.8009202462434769
        },
        "Whole dataset (INAT)": {
            "bart": -6.288656010627746,
            "bert": 0.7920597130060196
        },
        "Whole dataset (IRFL)": {
            "bart": -4.257583101987839,
            "bert": 0.9986557686328887
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.960433828830719,
            "bert": 0.8493092876672744
        },
        "Whole dataset (Memotion)": {
            "bart": -3.9246785163879396,
            "bert": 0.9053271293640137
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.3511261945962905,
            "bert": 0.886489275097847
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.493659347295761,
            "bert": 0.9964986813068389
        },
        "Whole dataset (NLVR)": {
            "bart": -3.521717209815979,
            "bert": 0.9991674458980561
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.544541015625,
            "bert": 0.9992994427680969
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.256368829011917,
            "bert": 0.8976052409410477
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1699642860889434,
            "bert": 0.9398654198646545
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.203762289881706,
            "bert": 0.8148287534713745
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.0393082356452945,
            "bert": 0.9147891819477081
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.810079734325409,
            "bert": 0.9344771176576614
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.206875495910644,
            "bert": 0.8545042914152146
        },
        "Whole dataset (Slake)": {
            "bart": -3.7017404770851137,
            "bert": 0.9927424365282058
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.03679331600666,
            "bert": 0.923856605887413
        },
        "Whole dataset (VCR)": {
            "bart": -3.087262631058693,
            "bert": 0.9242456662654877
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5502785450220107,
            "bert": 0.9090611511468887
        },
        "Whole dataset (VQA)": {
            "bart": -4.180354508161545,
            "bert": 0.9678724849224091
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.645703320503235,
            "bert": 0.9425813341140747
        },
        "Whole dataset (Winoground)": {
            "bart": -4.1727788341045375,
            "bert": 0.9979900777339935
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9743589743589743,
            "rec": 0.7702702702702703,
            "f1": 0.8603773584905661
        },
        "popular (POPE)": {
            "acc": 0.8371335504885994,
            "prec": 0.9147286821705426,
            "rec": 0.7515923566878981,
            "f1": 0.8251748251748251
        },
        "adversarial (POPE)": {
            "acc": 0.8566433566433567,
            "prec": 0.9252336448598131,
            "rec": 0.75,
            "f1": 0.8284518828451883
        }
    },
    "LLaVA-v1.6-Vicuna-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7078941707894171
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6986027944111777
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5238393977415308
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.3373253493013972
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.774
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6072742133224356
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.36979591836734693
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.43908045977011495
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26884920634920634
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.43074884029158383
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6032719836400818
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7319587628865979
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7492083597213426
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7094483888585472
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.16666666666666666
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5848303393213573
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5235920852359208
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.516245487364621
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.43304007820136853
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7613293051359517
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4083333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5157232704402516
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.796969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6582914572864321
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2653061224489796
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.8088235294117647,
            "prec": 0.8034682080924855,
            "rec": 0.8176470588235294,
            "f1": 0.8104956268221575
        },
        "posters (MME)": {
            "acc": 0.8367346938775511,
            "prec": 0.896,
            "rec": 0.7619047619047619,
            "f1": 0.823529411764706
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6666666666666666,
            "rec": 0.8,
            "f1": 0.7272727272727272
        },
        "scene (MME)": {
            "acc": 0.865,
            "prec": 0.9147727272727273,
            "rec": 0.805,
            "f1": 0.8563829787234043
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6091954022988506,
            "rec": 0.7571428571428571,
            "f1": 0.6751592356687899
        },
        "artwork (MME)": {
            "acc": 0.6675,
            "prec": 0.6030769230769231,
            "rec": 0.98,
            "f1": 0.7466666666666667
        },
        "landmark (MME)": {
            "acc": 0.815,
            "prec": 0.7669491525423728,
            "rec": 0.905,
            "f1": 0.8302752293577982
        },
        "text_translation (MME)": {
            "acc": 0.4,
            "prec": 0.43333333333333335,
            "rec": 0.65,
            "f1": 0.5199999999999999
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 1.0,
            "rec": 0.9,
            "f1": 0.9473684210526316
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.16666666666666666,
            "rec": 0.05,
            "f1": 0.07692307692307691
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.78125,
            "rec": 0.8333333333333334,
            "f1": 0.8064516129032259
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.8387096774193549,
            "rec": 0.8666666666666667,
            "f1": 0.8524590163934426
        },
        "OCR (MME)": {
            "acc": 0.825,
            "prec": 0.8095238095238095,
            "rec": 0.85,
            "f1": 0.8292682926829269
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.55,
            "f1": 0.5238095238095238
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5079365079365079
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5153846153846153
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.502283105022831
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5642458100558659
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7424242424242424
        },
        "ocr (MMBench_CN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4180790960451977
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4078014184397163
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8820754716981132
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7878787878787878
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4866666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5682539682539682
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6230769230769231
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5662100456621004
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5865921787709497
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5035460992907801
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9245283018867925
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8863636363636364
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8857142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.5
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.3
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2827868852459016
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3464052287581699
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.25882352941176473
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15196078431372548
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.26136363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8082191780821918
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.20967741935483872
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.35555555555555557
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.4782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.43661971830985913
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5786802030456852
        },
        "3D Distance (CVBench)": {
            "acc": 0.555
        },
        "2D Relation (CVBench)": {
            "acc": 0.6123076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.695
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.85683925151825,
            "bert": 0.800710163116455
        },
        "Whole dataset (Enrico)": {
            "bart": -5.939996981620789,
            "bert": 0.9701500350236892
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.994857330322265,
            "bert": 0.8145452100038528
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2741728949546816,
            "bert": 0.8626384031772614
        },
        "Whole dataset (GQA)": {
            "bart": -3.847278664112091,
            "bert": 0.9922405207157134
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -8.003609199523925,
            "bert": 0.7935239839553833
        },
        "Whole dataset (INAT)": {
            "bart": -6.506973333358765,
            "bert": 0.7665351003408432
        },
        "Whole dataset (IRFL)": {
            "bart": -4.307522261142731,
            "bert": 0.9986514610052108
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.793482667207718,
            "bert": 0.845162159204483
        },
        "Whole dataset (Memotion)": {
            "bart": -5.154691853523254,
            "bert": 0.8463616865873337
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.722810268998146,
            "bert": 0.8676389813423157
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.529197320938111,
            "bert": 0.8062470763921737
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5351433861255646,
            "bert": 0.8734173232316971
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.133608455657959,
            "bert": 0.9405216419696808
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.446188896894455,
            "bert": 0.7832258820533753
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.914812515974045,
            "bert": 0.9049169105291367
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.301944885253906,
            "bert": 0.8228701686859131
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.1306052494049075,
            "bert": 0.8487919920682907
        },
        "Whole dataset (Slake)": {
            "bart": -3.6406853830814363,
            "bert": 0.9921116656064988
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.476346778869629,
            "bert": 0.7850649070739746
        },
        "Whole dataset (VCR)": {
            "bart": -3.8393717467784882,
            "bert": 0.8713848686218262
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6508263784646986,
            "bert": 0.9040324127674103
        },
        "Whole dataset (VQA)": {
            "bart": -4.221212694644928,
            "bert": 0.9702708846330643
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.434775424003601,
            "bert": 0.9419688564538956
        },
        "Whole dataset (Winoground)": {
            "bart": -4.4064832258224484,
            "bert": 0.9979295617341996
        },
        "random (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.9896907216494846,
            "rec": 0.6486486486486487,
            "f1": 0.7836734693877551
        },
        "popular (POPE)": {
            "acc": 0.8143322475570033,
            "prec": 0.9716981132075472,
            "rec": 0.6560509554140127,
            "f1": 0.7832699619771862
        },
        "adversarial (POPE)": {
            "acc": 0.8321678321678322,
            "prec": 0.9772727272727273,
            "rec": 0.6515151515151515,
            "f1": 0.7818181818181817
        }
    },
    "LLaVA-v1.6-Vicuna-13B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7377930737793074
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6966067864271457
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5131744040150564
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.29141716566866266
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.808
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6142214957090315
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.39755102040816326
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3931034482758621
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24503968253968253
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4572564612326044
        },
        "Instance Location (SEED_2)": {
            "acc": 0.630879345603272
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7319587628865979
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7637745408486384
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.724194429273621
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2878787878787879
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5489021956087824
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5296803652968036
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.6064981949458483
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3313782991202346
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7824773413897281
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.39166666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5660377358490566
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8363636363636363
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6683417085427136
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.8176470588235294,
            "prec": 0.8698630136986302,
            "rec": 0.7470588235294118,
            "f1": 0.8037974683544304
        },
        "posters (MME)": {
            "acc": 0.8367346938775511,
            "prec": 0.9024390243902439,
            "rec": 0.7551020408163265,
            "f1": 0.8222222222222222
        },
        "position (MME)": {
            "acc": 0.6166666666666667,
            "prec": 0.5777777777777777,
            "rec": 0.8666666666666667,
            "f1": 0.6933333333333332
        },
        "scene (MME)": {
            "acc": 0.88,
            "prec": 0.9367816091954023,
            "rec": 0.815,
            "f1": 0.8716577540106951
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7071428571428572,
            "prec": 0.6705882352941176,
            "rec": 0.8142857142857143,
            "f1": 0.7354838709677418
        },
        "artwork (MME)": {
            "acc": 0.7675,
            "prec": 0.7377777777777778,
            "rec": 0.83,
            "f1": 0.7811764705882354
        },
        "landmark (MME)": {
            "acc": 0.67,
            "prec": 0.9594594594594594,
            "rec": 0.355,
            "f1": 0.5182481751824818
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.5555555555555556,
            "rec": 0.25,
            "f1": 0.3448275862068966
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5714285714285714,
            "rec": 0.4,
            "f1": 0.47058823529411764
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7073170731707317,
            "rec": 0.9666666666666667,
            "f1": 0.8169014084507042
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.85,
            "prec": 0.7916666666666666,
            "rec": 0.95,
            "f1": 0.8636363636363635
        },
        "code_reasoning (MME)": {
            "acc": 0.425,
            "prec": 0.4482758620689655,
            "rec": 0.65,
            "f1": 0.5306122448979592
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7906976744186046
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5650793650793651
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6461538461538462
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6089385474860335
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7474747474747475
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7756410256410257
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4350282485875706
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4787234042553192
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8632075471698113
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8409090909090909
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9285714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9011627906976745
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6063492063492063
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5525114155251142
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6666666666666666
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7318435754189944
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8914141414141414
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8012820512820513
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.423728813559322
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5354609929078015
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8322368421052632
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 1.0
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8915094339622641
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8825757575757576
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.6
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.5
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.4
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.3114754098360656
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18137254901960784
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.4431818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9807692307692307
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8775510204081632
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.27419354838709675
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5853658536585366
        },
        "Geography (ScienceQA)": {
            "acc": 0.7619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6724137931034483
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.35555555555555557
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7692307692307693
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Maps (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.9493670886075949
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.5596446700507615
        },
        "3D Distance (CVBench)": {
            "acc": 0.5516666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.6430769230769231
        },
        "3D Depth (CVBench)": {
            "acc": 0.78
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.076008365154267,
            "bert": 0.7686288315057754
        },
        "Whole dataset (Enrico)": {
            "bart": -5.179761867523194,
            "bert": 0.9765313464403153
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.052977118492127,
            "bert": 0.8059387826919555
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2681564354896544,
            "bert": 0.8607540571689606
        },
        "Whole dataset (GQA)": {
            "bart": -3.7954930901527404,
            "bert": 0.9921679621934891
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.726569805145264,
            "bert": 0.7950005400180816
        },
        "Whole dataset (INAT)": {
            "bart": -6.437267301082611,
            "bert": 0.7711612856388093
        },
        "Whole dataset (IRFL)": {
            "bart": -4.323619594573975,
            "bert": 0.9986376315355301
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.800951964855194,
            "bert": 0.8462481558322906
        },
        "Whole dataset (Memotion)": {
            "bart": -5.298174805641175,
            "bert": 0.8200833213329315
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.296677035689354,
            "bert": 0.8974543553590775
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.092910294532776,
            "bert": 0.816019252538681
        },
        "Whole dataset (NLVR)": {
            "bart": -2.7778848886489866,
            "bert": 0.9989849507808686
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.9416489744186403,
            "bert": 0.9991280114650727
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.494532024860382,
            "bert": 0.8726312148571015
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.0923974549770357,
            "bert": 0.9378445553779602
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.27566123008728,
            "bert": 0.8044456940889358
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.064248669147491,
            "bert": 0.9105465561151505
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.599291801452637,
            "bert": 0.7968005973100662
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.9102702236175535,
            "bert": 0.8431073588132858
        },
        "Whole dataset (Slake)": {
            "bart": -3.5808946406841278,
            "bert": 0.9961955630779267
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.485254130363464,
            "bert": 0.7891557520627975
        },
        "Whole dataset (VCR)": {
            "bart": -4.128804595470428,
            "bert": 0.8494504821300507
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6535405069589615,
            "bert": 0.9036496728658676
        },
        "Whole dataset (VQA)": {
            "bart": -4.077098388671875,
            "bert": 0.9700626647472381
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.506056228876114,
            "bert": 0.9392359429597854
        },
        "Whole dataset (Winoground)": {
            "bart": -4.58621088385582,
            "bert": 0.9978834116458892
        },
        "random (POPE)": {
            "acc": 0.8241042345276873,
            "prec": 0.9895833333333334,
            "rec": 0.6418918918918919,
            "f1": 0.778688524590164
        },
        "popular (POPE)": {
            "acc": 0.8078175895765473,
            "prec": 0.9711538461538461,
            "rec": 0.643312101910828,
            "f1": 0.7739463601532568
        },
        "adversarial (POPE)": {
            "acc": 0.8426573426573427,
            "prec": 1.0,
            "rec": 0.6590909090909091,
            "f1": 0.7945205479452054
        }
    },
    "LLaVA-v1.6-Mistral-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7268229726822972
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.7065868263473054
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5357590966122961
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.3313373253493014
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.77
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5970576215774418
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.41959183673469386
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.43448275862068964
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2400793650793651
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.415506958250497
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6390593047034765
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7422680412371134
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7780240658644711
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7236482796286182
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.29545454545454547
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5968063872255489
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5509893455098934
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5631768953068592
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34701857282502446
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7613293051359517
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.35833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5723270440251572
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7303030303030303
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6482412060301508
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2857142857142857
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.7941176470588235,
            "prec": 0.9098360655737705,
            "rec": 0.6529411764705882,
            "f1": 0.7602739726027397
        },
        "posters (MME)": {
            "acc": 0.7857142857142857,
            "prec": 0.868421052631579,
            "rec": 0.673469387755102,
            "f1": 0.7586206896551724
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6666666666666666,
            "rec": 0.8,
            "f1": 0.7272727272727272
        },
        "scene (MME)": {
            "acc": 0.865,
            "prec": 0.9244186046511628,
            "rec": 0.795,
            "f1": 0.8548387096774194
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.7205882352941176,
            "rec": 0.7,
            "f1": 0.7101449275362319
        },
        "artwork (MME)": {
            "acc": 0.7075,
            "prec": 0.7064676616915423,
            "rec": 0.71,
            "f1": 0.7082294264339151
        },
        "landmark (MME)": {
            "acc": 0.8125,
            "prec": 0.9370629370629371,
            "rec": 0.67,
            "f1": 0.7813411078717202
        },
        "text_translation (MME)": {
            "acc": 0.475,
            "prec": 0.4,
            "rec": 0.1,
            "f1": 0.16000000000000003
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 1.0,
            "rec": 0.9,
            "f1": 0.9473684210526316
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.75,
            "rec": 0.9,
            "f1": 0.8181818181818182
        },
        "color (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.90625,
            "rec": 0.9666666666666667,
            "f1": 0.9354838709677419
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.8125,
            "rec": 0.65,
            "f1": 0.7222222222222223
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.46153846153846156,
            "rec": 0.3,
            "f1": 0.3636363636363637
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7790697674418605
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6095238095238096
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5846153846153846
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5388127853881278
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6666666666666666
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6927374301675978
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8790697674418605
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6843434343434344
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7628205128205128
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4124293785310734
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.43617021276595747
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8143939393939394
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4866666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8785714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6158730158730159
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.676923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6312849162011173
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8383838383838383
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8012820512820513
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4858156028368794
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8552631578947368
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_EN)": {
            "acc": 0.910377358490566
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8977272727272727
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.49333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.5
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.6
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2827868852459016
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3006535947712418
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.8461538461538461
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9387755102040817
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.509090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5294117647058824
        },
        "Classification (ScienceQA)": {
            "acc": 0.9113924050632911
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "2D Count (CVBench)": {
            "acc": 0.5710659898477157
        },
        "3D Distance (CVBench)": {
            "acc": 0.5766666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.6507692307692308
        },
        "3D Depth (CVBench)": {
            "acc": 0.7366666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.948048801422119,
            "bert": 0.7664463067054749
        },
        "Whole dataset (Enrico)": {
            "bart": -5.9670757234096525,
            "bert": 0.8943231707811355
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.972132592201233,
            "bert": 0.8172242718935013
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2876870799064637,
            "bert": 0.8617935883998871
        },
        "Whole dataset (GQA)": {
            "bart": -3.6302272152900694,
            "bert": 0.9922994846105575
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.807211947441101,
            "bert": 0.790636573433876
        },
        "Whole dataset (INAT)": {
            "bart": -6.448303914070129,
            "bert": 0.7665538400411606
        },
        "Whole dataset (IRFL)": {
            "bart": -4.218534976243973,
            "bert": 0.9986667227745056
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.797428492307663,
            "bert": 0.8480218809843063
        },
        "Whole dataset (Memotion)": {
            "bart": -4.394420654773712,
            "bert": 0.8796324414014817
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.4566350853443146,
            "bert": 0.8854388898611069
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.554196853637695,
            "bert": 0.8024638259410858
        },
        "Whole dataset (NLVR)": {
            "bart": -3.0716005468368532,
            "bert": 0.9991704380512237
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.069328441619873,
            "bert": 0.9992603135108947
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5773130536079405,
            "bert": 0.8744274413585663
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1295090210437775,
            "bert": 0.9376635777950287
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.358481335639953,
            "bert": 0.8032550495862961
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.311086336374283,
            "bert": 0.9130070388317109
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.794408555030823,
            "bert": 0.8040022271871566
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.916344699859619,
            "bert": 0.8450241589546204
        },
        "Whole dataset (Slake)": {
            "bart": -3.985537656545639,
            "bert": 0.9946936976909637
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.644849319458007,
            "bert": 0.7996649533510208
        },
        "Whole dataset (VCR)": {
            "bart": -4.152511657476425,
            "bert": 0.8635106980800629
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6051603722572327,
            "bert": 0.9052506041526794
        },
        "Whole dataset (VQA)": {
            "bart": -4.273738694190979,
            "bert": 0.9719659203290939
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.63769184589386,
            "bert": 0.9362136578559875
        },
        "Whole dataset (Winoground)": {
            "bart": -4.553686323165894,
            "bert": 0.9974455237388611
        },
        "random (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.9801980198019802,
            "rec": 0.668918918918919,
            "f1": 0.7951807228915663
        },
        "popular (POPE)": {
            "acc": 0.8143322475570033,
            "prec": 0.9310344827586207,
            "rec": 0.6878980891719745,
            "f1": 0.7912087912087912
        },
        "adversarial (POPE)": {
            "acc": 0.8391608391608392,
            "prec": 0.9574468085106383,
            "rec": 0.6818181818181818,
            "f1": 0.7964601769911505
        }
    },
    "LLaVA-v1.6-34B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7526349752634975
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.716566866267465
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5520702634880803
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.3652694610778443
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.864
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6563138536984062
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4122448979591837
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.48735632183908045
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26587301587301587
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4181577203445991
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6676891615541922
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7525773195876289
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7773907536415453
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7542326597487712
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.42424242424242425
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.688622754491018
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.60882800608828
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5523465703971119
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.4506353861192571
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.8217522658610272
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.38333333333333336
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.7169811320754716
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8909090909090909
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6331658291457286
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2716049382716049
        },
        "celebrity (MME)": {
            "acc": 0.8823529411764706,
            "prec": 0.8611111111111112,
            "rec": 0.9117647058823529,
            "f1": 0.8857142857142858
        },
        "posters (MME)": {
            "acc": 0.9081632653061225,
            "prec": 0.9,
            "rec": 0.9183673469387755,
            "f1": 0.9090909090909091
        },
        "position (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5510204081632653,
            "rec": 0.9,
            "f1": 0.6835443037974683
        },
        "scene (MME)": {
            "acc": 0.88,
            "prec": 0.8838383838383839,
            "rec": 0.875,
            "f1": 0.8793969849246231
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.75,
            "prec": 0.6881720430107527,
            "rec": 0.9142857142857143,
            "f1": 0.7852760736196318
        },
        "artwork (MME)": {
            "acc": 0.67,
            "prec": 0.6148648648648649,
            "rec": 0.91,
            "f1": 0.7338709677419354
        },
        "landmark (MME)": {
            "acc": 0.875,
            "prec": 0.831858407079646,
            "rec": 0.94,
            "f1": 0.8826291079812206
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 0.5428571428571428,
            "rec": 0.95,
            "f1": 0.6909090909090908
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9655172413793104,
            "rec": 0.9333333333333333,
            "f1": 0.9491525423728815
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5277777777777778,
            "rec": 0.95,
            "f1": 0.6785714285714285
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.8823529411764706,
            "rec": 1.0,
            "f1": 0.9375
        },
        "OCR (MME)": {
            "acc": 0.925,
            "prec": 0.8695652173913043,
            "rec": 1.0,
            "f1": 0.9302325581395349
        },
        "code_reasoning (MME)": {
            "acc": 0.4,
            "prec": 0.4375,
            "rec": 0.7,
            "f1": 0.5384615384615384
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6571428571428571
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6307692307692307
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.639269406392694
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6808510638297872
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.8324022346368715
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8661616161616161
        },
        "ocr (MMBench_CN)": {
            "acc": 0.8141025641025641
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.6779661016949152
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.7446808510638298
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.88
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8881578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.9481132075471698
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8977272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.95
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.653968253968254
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6153846153846154
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.680365296803653
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.7801418439716312
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.8379888268156425
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9441860465116279
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8333333333333334
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.7411347517730497
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.89
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9013157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9622641509433962
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.54
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.1
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.36065573770491804
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.23809523809523808
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.477124183006536
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.5058823529411764
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.35294117647058826
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6590909090909091
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.821917808219178
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 1.0
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.9300699300699301
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9693877551020408
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.43548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.7804878048780488
        },
        "Geography (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6388888888888888
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9743589743589743
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.803921568627451
        },
        "Classification (ScienceQA)": {
            "acc": 0.9620253164556962
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.36619718309859156
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.6052631578947368
        },
        "2D Count (CVBench)": {
            "acc": 0.6027918781725888
        },
        "3D Distance (CVBench)": {
            "acc": 0.7333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.7692307692307693
        },
        "3D Depth (CVBench)": {
            "acc": 0.7833333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.6561001515388485,
            "bert": 0.7881196236610413
        },
        "Whole dataset (Enrico)": {
            "bart": -5.263171820640564,
            "bert": 0.9703613567352295
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.050511422157288,
            "bert": 0.8159962910413742
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.27942441701889,
            "bert": 0.8610266000032425
        },
        "Whole dataset (GQA)": {
            "bart": -3.4977266073226927,
            "bert": 0.9936768364906311
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.736569166183472,
            "bert": 0.7929896676540374
        },
        "Whole dataset (INAT)": {
            "bart": -6.480156722068787,
            "bert": 0.765446270108223
        },
        "Whole dataset (IRFL)": {
            "bart": -4.203606368303299,
            "bert": 0.9986701345443726
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8544756817817687,
            "bert": 0.8457171553373337
        },
        "Whole dataset (Memotion)": {
            "bart": -4.723159909248352,
            "bert": 0.9026071858406067
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.3106663861870764,
            "bert": 0.8947624003887177
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.459598881006241,
            "bert": 0.9983608186244964
        },
        "Whole dataset (NLVR)": {
            "bart": -2.990425934791565,
            "bert": 0.999136689901352
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.9734059619903563,
            "bert": 0.9992838096618653
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5270832550525664,
            "bert": 0.8740696275234222
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.025937283039093,
            "bert": 0.9395259612798691
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.067901351451874,
            "bert": 0.7965623873472214
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.975410189628601,
            "bert": 0.9098611509799958
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.860156192779541,
            "bert": 0.9486970382928849
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.839511761665344,
            "bert": 0.8429951667785645
        },
        "Whole dataset (Slake)": {
            "bart": -3.681378530263901,
            "bert": 0.9919924026727677
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.2641254639625545,
            "bert": 0.8559367883205414
        },
        "Whole dataset (VCR)": {
            "bart": -3.154912261068821,
            "bert": 0.9222040194272995
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7717908924818038,
            "bert": 0.9033993428945541
        },
        "Whole dataset (VQA)": {
            "bart": -3.930473643541336,
            "bert": 0.9762656366825104
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.828663442134857,
            "bert": 0.9435493272542953
        },
        "Whole dataset (Winoground)": {
            "bart": -4.05048112154007,
            "bert": 0.9980170124769211
        },
        "random (POPE)": {
            "acc": 0.8241042345276873,
            "prec": 1.0,
            "rec": 0.6351351351351351,
            "f1": 0.7768595041322314
        },
        "popular (POPE)": {
            "acc": 0.8208469055374593,
            "prec": 0.9722222222222222,
            "rec": 0.6687898089171974,
            "f1": 0.7924528301886792
        },
        "adversarial (POPE)": {
            "acc": 0.8531468531468531,
            "prec": 0.9787234042553191,
            "rec": 0.696969696969697,
            "f1": 0.8141592920353982
        }
    },
    "Cambrian-Phi3-3B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7352118735211873
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.688622754491018
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.6166875784190715
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.47305389221556887
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.678
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6763383735185942
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.45714285714285713
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5862068965517241
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.30456349206349204
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4625579854208085
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6390593047034765
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7701076630778974
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7547788093937738
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3409090909090909
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.7005988023952096
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5570776255707762
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.6931407942238267
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.47996089931573804
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7794561933534743
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25833333333333336
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.6415094339622641
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7909090909090909
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6633165829145728
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.5102040816326531
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6588235294117647,
            "prec": 0.9821428571428571,
            "rec": 0.3235294117647059,
            "f1": 0.48672566371681414
        },
        "posters (MME)": {
            "acc": 0.7959183673469388,
            "prec": 0.9065420560747663,
            "rec": 0.6598639455782312,
            "f1": 0.7637795275590551
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7222222222222222,
            "rec": 0.8666666666666667,
            "f1": 0.7878787878787877
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.9559748427672956,
            "rec": 0.76,
            "f1": 0.8467966573816156
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6144578313253012,
            "rec": 0.7285714285714285,
            "f1": 0.6666666666666666
        },
        "artwork (MME)": {
            "acc": 0.7275,
            "prec": 0.837037037037037,
            "rec": 0.565,
            "f1": 0.6746268656716418
        },
        "landmark (MME)": {
            "acc": 0.83,
            "prec": 0.8837209302325582,
            "rec": 0.76,
            "f1": 0.8172043010752689
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.25,
            "f1": 0.3333333333333333
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7631578947368421,
            "rec": 0.9666666666666667,
            "f1": 0.8529411764705883
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.6333333333333333,
            "rec": 0.95,
            "f1": 0.7599999999999999
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5333333333333333,
            "rec": 0.4,
            "f1": 0.4571428571428572
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9011627906976745
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5968253968253968
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5307692307692308
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5662100456621004
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6927374301675978
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7045454545454546
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6153846153846154
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8552631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9147727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8726415094339622
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7992424242424242
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.85
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6349206349206349
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6076923076923076
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.680365296803653
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.7588652482269503
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6815642458100558
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8409090909090909
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.7127659574468085
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8980263157894737
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9292452830188679
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9356060606060606
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5733333333333334
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.95
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.3073770491803279
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.28104575163398693
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20588235294117646
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8767123287671232
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.916083916083916
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.9512195121951219
        },
        "Geography (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Magnets (ScienceQA)": {
            "acc": 0.509090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.35294117647058826
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9743589743589743
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.46296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.8695652173913043
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.9746835443037974
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5211267605633803
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.6307106598984772
        },
        "3D Distance (CVBench)": {
            "acc": 0.7033333333333334
        },
        "2D Relation (CVBench)": {
            "acc": 0.6784615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.7066666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.5961434626579285,
            "bert": 0.8166986668109893
        },
        "Whole dataset (Enrico)": {
            "bart": -5.821685035228729,
            "bert": 0.9509234511852265
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.933626019954682,
            "bert": 0.7887052065134048
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.315742498636246,
            "bert": 0.8540850722789765
        },
        "Whole dataset (GQA)": {
            "bart": -3.571299345493317,
            "bert": 0.9937012469768525
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -8.233414053916931,
            "bert": 0.7872293746471405
        },
        "Whole dataset (INAT)": {
            "bart": -6.519991917610168,
            "bert": 0.7646913975477219
        },
        "Whole dataset (IRFL)": {
            "bart": -4.191546609401703,
            "bert": 0.9986739057302475
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8558854603767396,
            "bert": 0.8450341176986694
        },
        "Whole dataset (Memotion)": {
            "bart": -5.444639849662781,
            "bert": 0.79773293197155
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.5356709229946137,
            "bert": 0.8937920171022415
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.671424831151962,
            "bert": 0.9726605075597763
        },
        "Whole dataset (NLVR)": {
            "bart": -2.7978067779541016,
            "bert": 0.9995735108852386
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.670116457939148,
            "bert": 0.9993941313028336
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.6628106701374055,
            "bert": 0.8586425477266312
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.030708194375038,
            "bert": 0.9383745270967484
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.788639959096908,
            "bert": 0.7959051138162613
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.5179409116506575,
            "bert": 0.9118063122034072
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.22016480088234,
            "bert": 0.7986764222383499
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.744818696975708,
            "bert": 0.8445630860328674
        },
        "Whole dataset (Slake)": {
            "bart": -3.6895922911167145,
            "bert": 0.9959226530790329
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.5410640954971315,
            "bert": 0.7832257062196731
        },
        "Whole dataset (VCR)": {
            "bart": -3.8732952451705933,
            "bert": 0.8521805781126023
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.632182692885399,
            "bert": 0.9061125922203064
        },
        "Whole dataset (VQA)": {
            "bart": -4.054292652606964,
            "bert": 0.9776974391937255
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.612123793363571,
            "bert": 0.9405735051631927
        },
        "Whole dataset (Winoground)": {
            "bart": -4.211632384061813,
            "bert": 0.9897874641418457
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9824561403508771,
            "rec": 0.7567567567567568,
            "f1": 0.8549618320610687
        },
        "popular (POPE)": {
            "acc": 0.8599348534201955,
            "prec": 0.9523809523809523,
            "rec": 0.7643312101910829,
            "f1": 0.8480565371024734
        },
        "adversarial (POPE)": {
            "acc": 0.9090909090909091,
            "prec": 0.9818181818181818,
            "rec": 0.8181818181818182,
            "f1": 0.8925619834710744
        }
    },
    "Cambrian-8B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7627446762744676
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.7085828343313373
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5878293601003765
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.4471057884231537
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.796
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6939109113199836
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4473469387755102
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5747126436781609
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24702380952380953
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.47647448641484424
        },
        "Instance Location (SEED_2)": {
            "acc": 0.7004089979550102
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7827739075364154
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.770617149098853
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.32575757575757575
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.6866267465069861
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5601217656012176
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.7184115523465704
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.4389051808406647
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.770392749244713
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.6289308176100629
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7878787878787878
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6532663316582915
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.5510204081632653
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.8705882352941177,
            "prec": 0.8579545454545454,
            "rec": 0.888235294117647,
            "f1": 0.8728323699421965
        },
        "posters (MME)": {
            "acc": 0.8197278911564626,
            "prec": 0.9196428571428571,
            "rec": 0.7006802721088435,
            "f1": 0.7953667953667953
        },
        "position (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7878787878787878,
            "rec": 0.8666666666666667,
            "f1": 0.8253968253968254
        },
        "scene (MME)": {
            "acc": 0.7525,
            "prec": 0.7988165680473372,
            "rec": 0.675,
            "f1": 0.7317073170731707
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5214285714285715,
            "prec": 0.5164835164835165,
            "rec": 0.6714285714285714,
            "f1": 0.5838509316770187
        },
        "artwork (MME)": {
            "acc": 0.6425,
            "prec": 0.6446700507614214,
            "rec": 0.635,
            "f1": 0.6397984886649875
        },
        "landmark (MME)": {
            "acc": 0.7825,
            "prec": 0.8265895953757225,
            "rec": 0.715,
            "f1": 0.7667560321715817
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.5555555555555556,
            "rec": 1.0,
            "f1": 0.7142857142857143
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5263157894736842,
            "rec": 1.0,
            "f1": 0.6896551724137931
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.75,
            "rec": 1.0,
            "f1": 0.8571428571428571
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8529411764705882,
            "rec": 0.9666666666666667,
            "f1": 0.90625
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5714285714285714,
            "rec": 1.0,
            "f1": 0.7272727272727273
        },
        "code_reasoning (MME)": {
            "acc": 0.575,
            "prec": 0.5405405405405406,
            "rec": 1.0,
            "f1": 0.7017543859649124
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6285714285714286
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5923076923076923
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5068493150684932
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5815602836879432
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6759776536312849
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8976744186046511
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8156565656565656
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6730769230769231
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.77
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8552631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8632075471698113
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8712121212121212
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5866666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9571428571428572
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.7174603174603175
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6538461538461539
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.7123287671232876
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.8156028368794326
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7039106145251397
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8888888888888888
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8012820512820513
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4519774011299435
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.7375886524822695
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8881578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6276595744680851
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9339622641509434
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9734848484848485
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.58
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9785714285714285
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.1
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3202614379084967
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8356164383561644
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9711538461538461
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.958041958041958
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1935483870967742
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.8780487804878049
        },
        "Geography (ScienceQA)": {
            "acc": 0.9761904761904762
        },
        "Magnets (ScienceQA)": {
            "acc": 0.509090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.7777777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.8793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.6296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.9130434782608695
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.8627450980392157
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.6738578680203046
        },
        "3D Distance (CVBench)": {
            "acc": 0.7
        },
        "2D Relation (CVBench)": {
            "acc": 0.8153846153846154
        },
        "3D Depth (CVBench)": {
            "acc": 0.755
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.640458962917328,
            "bert": 0.7871805477142334
        },
        "Whole dataset (Enrico)": {
            "bart": -6.908868250846862,
            "bert": 0.7984136515855789
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.86178781747818,
            "bert": 0.7887343907356262
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4342384004592894,
            "bert": 0.8458634179830551
        },
        "Whole dataset (GQA)": {
            "bart": -3.8488724100589753,
            "bert": 0.9900500917434693
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -8.216754188537598,
            "bert": 0.786236299276352
        },
        "Whole dataset (INAT)": {
            "bart": -6.428267598152161,
            "bert": 0.7667923760414124
        },
        "Whole dataset (IRFL)": {
            "bart": -4.141023087501526,
            "bert": 0.9986834222078323
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.6576324594020844,
            "bert": 0.8490686398744584
        },
        "Whole dataset (Memotion)": {
            "bart": -5.2523884153366085,
            "bert": 0.798891794681549
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.065245041847229,
            "bert": 0.8306310093402862
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -8.134941878318786,
            "bert": 0.7867518782615661
        },
        "Whole dataset (NLVR)": {
            "bart": -4.767875351905823,
            "bert": 0.891236070394516
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.0093740606307984,
            "bert": 0.9973034000396729
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5968106961250306,
            "bert": 0.8740910655260086
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.141336156129837,
            "bert": 0.9307254612445831
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.061797542572021,
            "bert": 0.7920234948396683
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.5712897151708605,
            "bert": 0.9114026659727097
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.062151622772217,
            "bert": 0.7965634584426879
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.64397527217865,
            "bert": 0.8461664009094239
        },
        "Whole dataset (Slake)": {
            "bart": -4.052730168104172,
            "bert": 0.9773402535915374
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.022197225093842,
            "bert": 0.7856330811977387
        },
        "Whole dataset (VCR)": {
            "bart": -3.9407039391994476,
            "bert": 0.8425862914323807
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.4198811012506485,
            "bert": 0.9078871148824692
        },
        "Whole dataset (VQA)": {
            "bart": -4.2220676279067995,
            "bert": 0.959230243563652
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.387323722839356,
            "bert": 0.9382249015569687
        },
        "Whole dataset (Winoground)": {
            "bart": -4.226755567789078,
            "bert": 0.9979757118225098
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9663865546218487,
            "rec": 0.777027027027027,
            "f1": 0.8614232209737828
        },
        "popular (POPE)": {
            "acc": 0.8664495114006515,
            "prec": 0.9461538461538461,
            "rec": 0.7834394904458599,
            "f1": 0.8571428571428572
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9727272727272728,
            "rec": 0.8106060606060606,
            "f1": 0.8842975206611571
        }
    },
    "Cambrian-13B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7506990750699075
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6646706586826348
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.6079046424090339
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.4471057884231537
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.824
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.7020841847159788
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5770114942528736
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.3323412698412698
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4247846255798542
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6758691206543967
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7525773195876289
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.779607346421786
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7744401966138722
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2803030303030303
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.6626746506986028
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5829528158295282
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.7003610108303249
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.4916911045943304
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7885196374622356
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.6226415094339622
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8393939393939394
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6683417085427136
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.8529411764705882,
            "prec": 0.9477611940298507,
            "rec": 0.7470588235294118,
            "f1": 0.8355263157894737
        },
        "posters (MME)": {
            "acc": 0.8367346938775511,
            "prec": 0.9090909090909091,
            "rec": 0.7482993197278912,
            "f1": 0.8208955223880596
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.7058823529411765,
            "rec": 0.8,
            "f1": 0.7500000000000001
        },
        "scene (MME)": {
            "acc": 0.685,
            "prec": 0.6907216494845361,
            "rec": 0.67,
            "f1": 0.6802030456852791
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.55,
            "prec": 0.5384615384615384,
            "rec": 0.7,
            "f1": 0.608695652173913
        },
        "artwork (MME)": {
            "acc": 0.735,
            "prec": 0.7865853658536586,
            "rec": 0.645,
            "f1": 0.7087912087912089
        },
        "landmark (MME)": {
            "acc": 0.8025,
            "prec": 0.9618320610687023,
            "rec": 0.63,
            "f1": 0.7613293051359518
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.5588235294117647,
            "rec": 0.95,
            "f1": 0.7037037037037037
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5384615384615384,
            "rec": 0.7,
            "f1": 0.608695652173913
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.8823529411764706,
            "rec": 1.0,
            "f1": 0.9375
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.6896551724137931,
            "rec": 1.0,
            "f1": 0.8163265306122449
        },
        "code_reasoning (MME)": {
            "acc": 0.55,
            "prec": 0.5333333333333333,
            "rec": 0.8,
            "f1": 0.64
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6444444444444445
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.502283105022831
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6312849162011173
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.958139534883721
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7626262626262627
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6346153846153846
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.624113475177305
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8585526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8679245283018868
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8295454545454546
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.49333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.95
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.7015873015873015
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.676923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6712328767123288
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.7588652482269503
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7486033519553073
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9217171717171717
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8141025641025641
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.423728813559322
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6808510638297872
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8881578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6063829787234043
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9056603773584906
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5533333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.1
        },
        "History (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.1
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2827868852459016
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1111111111111111
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2107843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.9090909090909091
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.8780487804878049
        },
        "Geography (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6388888888888888
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.8974358974358975
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.6296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6862745098039216
        },
        "Classification (ScienceQA)": {
            "acc": 0.9873417721518988
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7105263157894737
        },
        "2D Count (CVBench)": {
            "acc": 0.649746192893401
        },
        "3D Distance (CVBench)": {
            "acc": 0.6516666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.816923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.745
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.761325492858886,
            "bert": 0.789453557729721
        },
        "Whole dataset (Enrico)": {
            "bart": -5.892890585660934,
            "bert": 0.8986767411231995
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.8386098623275755,
            "bert": 0.7882901388406753
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3242909109592436,
            "bert": 0.8569860643148423
        },
        "Whole dataset (GQA)": {
            "bart": -3.6694012796878814,
            "bert": 0.9821611809730529
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.920159783363342,
            "bert": 0.7881982707977295
        },
        "Whole dataset (INAT)": {
            "bart": -6.461796350479126,
            "bert": 0.7679140788316726
        },
        "Whole dataset (IRFL)": {
            "bart": -4.263320800065994,
            "bert": 0.9986564874649048
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.5934386146068573,
            "bert": 0.8511972123384476
        },
        "Whole dataset (Memotion)": {
            "bart": -5.380088453292847,
            "bert": 0.7977671927213669
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.5660585880279543,
            "bert": 0.8702000576257706
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -8.121594572067261,
            "bert": 0.7872338563203811
        },
        "Whole dataset (NLVR)": {
            "bart": -3.2701065826416014,
            "bert": 0.9992371225357055
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.404503526687622,
            "bert": 0.972989616394043
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5626320135593414,
            "bert": 0.8908579748868942
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.4651402747631073,
            "bert": 0.9012543034553527
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.853903402686119,
            "bert": 0.8324719983339309
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.446825906038284,
            "bert": 0.9075843268632888
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.507996379137039,
            "bert": 0.8312963926792145
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.573957500457763,
            "bert": 0.8450243800878525
        },
        "Whole dataset (Slake)": {
            "bart": -3.864030499458313,
            "bert": 0.9841922158002854
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.139724886417389,
            "bert": 0.7847239983081817
        },
        "Whole dataset (VCR)": {
            "bart": -3.724598476886749,
            "bert": 0.850076031088829
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.9428275513648985,
            "bert": 0.8941395235061645
        },
        "Whole dataset (VQA)": {
            "bart": -4.453968303203583,
            "bert": 0.9383012819290161
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.061559929847717,
            "bert": 0.9431931912899018
        },
        "Whole dataset (Winoground)": {
            "bart": -4.319780427217483,
            "bert": 0.9979503917694091
        },
        "random (POPE)": {
            "acc": 0.8925081433224755,
            "prec": 0.9831932773109243,
            "rec": 0.7905405405405406,
            "f1": 0.8764044943820225
        },
        "popular (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9534883720930233,
            "rec": 0.7834394904458599,
            "f1": 0.8601398601398603
        },
        "adversarial (POPE)": {
            "acc": 0.916083916083916,
            "prec": 0.9655172413793104,
            "rec": 0.8484848484848485,
            "f1": 0.9032258064516129
        }
    },
    "Cambrian-34B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7627446762744676
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6746506986027944
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.6260978670012547
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.4351297405189621
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.874
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.7029015120555783
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4636734693877551
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.7034482758620689
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24107142857142858
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.44731610337972166
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6779141104294478
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7628865979381443
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7761241291956935
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7842708902239214
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.45454545454545453
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.7784431137724551
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.6164383561643836
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.8447653429602888
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.49071358748778104
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.325
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.8238993710691824
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8878787878787879
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4897959183673469
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.8,
            "prec": 0.8227848101265823,
            "rec": 0.7647058823529411,
            "f1": 0.7926829268292683
        },
        "posters (MME)": {
            "acc": 0.9523809523809523,
            "prec": 0.9854014598540146,
            "rec": 0.9183673469387755,
            "f1": 0.9507042253521126
        },
        "position (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8125,
            "rec": 0.8666666666666667,
            "f1": 0.8387096774193549
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.9801324503311258,
            "rec": 0.74,
            "f1": 0.8433048433048433
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7714285714285715,
            "prec": 0.7317073170731707,
            "rec": 0.8571428571428571,
            "f1": 0.7894736842105263
        },
        "artwork (MME)": {
            "acc": 0.7675,
            "prec": 0.7743589743589744,
            "rec": 0.755,
            "f1": 0.7645569620253165
        },
        "landmark (MME)": {
            "acc": 0.905,
            "prec": 0.95,
            "rec": 0.855,
            "f1": 0.8999999999999999
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.5714285714285714,
            "rec": 1.0,
            "f1": 0.7272727272727273
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.6,
            "prec": 0.5909090909090909,
            "rec": 0.65,
            "f1": 0.6190476190476191
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "OCR (MME)": {
            "acc": 0.825,
            "prec": 0.7407407407407407,
            "rec": 1.0,
            "f1": 0.851063829787234
        },
        "code_reasoning (MME)": {
            "acc": 0.65,
            "prec": 0.625,
            "rec": 0.75,
            "f1": 0.6818181818181818
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.7587301587301587
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6538461538461539
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.6712328767123288
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.8652482269503546
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.888268156424581
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9488372093023256
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.898989898989899
        },
        "ocr (MMBench_CN)": {
            "acc": 0.8141025641025641
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.6779661016949152
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.8297872340425532
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.9
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.9046052631578947
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6808510638297872
        },
        "image_style (MMBench_CN)": {
            "acc": 0.910377358490566
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.928030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5133333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9785714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.7492063492063492
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6230769230769231
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.7031963470319634
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.8794326241134752
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.8324022346368715
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9065656565656566
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8717948717948718
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.7570621468926554
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.851063829787234
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.91
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9276315789473685
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6702127659574468
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9433962264150944
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9583333333333334
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9857142857142858
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.1
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.319672131147541
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.20634920634920634
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.38562091503267976
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.43529411764705883
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3480392156862745
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6477272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.863013698630137
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "State capitals (ScienceQA)": {
            "acc": 1.0
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.993006993006993
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9591836734693877
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 1.0
        },
        "Geography (ScienceQA)": {
            "acc": 0.9523809523809523
        },
        "Magnets (ScienceQA)": {
            "acc": 0.509090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.7222222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.5111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 1.0
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.6296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.9215686274509803
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.9473684210526315
        },
        "2D Count (CVBench)": {
            "acc": 0.6763959390862944
        },
        "3D Distance (CVBench)": {
            "acc": 0.7383333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.88
        },
        "3D Depth (CVBench)": {
            "acc": 0.83
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.764820241928101,
            "bert": 0.7716048574447631
        },
        "Whole dataset (Enrico)": {
            "bart": -6.8320813703536984,
            "bert": 0.7950037086009979
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.833203692436218,
            "bert": 0.7890435671806335
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.28538992702961,
            "bert": 0.865683776140213
        },
        "Whole dataset (GQA)": {
            "bart": -3.491867432594299,
            "bert": 0.9915813219547271
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -8.150972480773925,
            "bert": 0.7862814390659332
        },
        "Whole dataset (INAT)": {
            "bart": -6.359541139602661,
            "bert": 0.7627904742956162
        },
        "Whole dataset (IRFL)": {
            "bart": -4.1886777603626255,
            "bert": 0.9986735463142395
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.5513866662979128,
            "bert": 0.8526192110776901
        },
        "Whole dataset (Memotion)": {
            "bart": -5.2551216840744015,
            "bert": 0.7977627217769623
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.295024194717407,
            "bert": 0.8113779360055924
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -8.025677394866943,
            "bert": 0.7883010226488113
        },
        "Whole dataset (NLVR)": {
            "bart": -3.4842583608627318,
            "bert": 0.9971534276008606
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.226501154899597,
            "bert": 0.991204908490181
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.544121460914612,
            "bert": 0.8829682928323745
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.079429296851158,
            "bert": 0.9283486843109131
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.678246725797653,
            "bert": 0.7949899637699127
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.410778771042824,
            "bert": 0.9099041360616684
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.755347363948822,
            "bert": 0.8006922882795334
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.637031981945038,
            "bert": 0.8451026690006256
        },
        "Whole dataset (Slake)": {
            "bart": -3.5948751378059387,
            "bert": 0.9923953729867935
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.133391680717469,
            "bert": 0.7831891512870789
        },
        "Whole dataset (VCR)": {
            "bart": -3.5013338577747346,
            "bert": 0.8448872345685959
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5366200864315034,
            "bert": 0.9063308352231979
        },
        "Whole dataset (VQA)": {
            "bart": -3.7030592250823973,
            "bert": 0.9747851324081421
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.2247973155975345,
            "bert": 0.9458459293842316
        },
        "Whole dataset (Winoground)": {
            "bart": -3.7836794209480287,
            "bert": 0.9960022842884064
        },
        "random (POPE)": {
            "acc": 0.9055374592833876,
            "prec": 0.983739837398374,
            "rec": 0.8175675675675675,
            "f1": 0.8929889298892988
        },
        "popular (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.946969696969697,
            "rec": 0.7961783439490446,
            "f1": 0.8650519031141869
        },
        "adversarial (POPE)": {
            "acc": 0.9090909090909091,
            "prec": 0.9649122807017544,
            "rec": 0.8333333333333334,
            "f1": 0.8943089430894309
        }
    },
    "Fuyu-8B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.28608302860830287
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.22554890219560877
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.21706398996235884
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.20359281437125748
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.23
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2627707396812423
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.28489795918367344
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2482758620689655
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2251984126984127
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2591119946984758
        },
        "Instance Location (SEED_2)": {
            "acc": 0.3159509202453988
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.31958762886597936
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.2713742875237492
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.269797924631349
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.5316455696202531
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.21212121212121213
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2770167427701674
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20938628158844766
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2209188660801564
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.311178247734139
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.29559748427672955
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.19696969696969696
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.25125628140703515
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.20408163265306123
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.18518518518518517
        },
        "celebrity (MME)": {
            "acc": 0.5029411764705882,
            "prec": 0.5019455252918288,
            "rec": 0.7588235294117647,
            "f1": 0.6042154566744731
        },
        "posters (MME)": {
            "acc": 0.5306122448979592,
            "prec": 0.5314685314685315,
            "rec": 0.5170068027210885,
            "f1": 0.5241379310344828
        },
        "position (MME)": {
            "acc": 0.6,
            "prec": 0.5833333333333334,
            "rec": 0.7,
            "f1": 0.6363636363636365
        },
        "scene (MME)": {
            "acc": 0.4525,
            "prec": 0.46441947565543074,
            "rec": 0.62,
            "f1": 0.5310492505353319
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.40714285714285714,
            "prec": 0.410958904109589,
            "rec": 0.42857142857142855,
            "f1": 0.4195804195804196
        },
        "artwork (MME)": {
            "acc": 0.255,
            "prec": 0.20833333333333334,
            "rec": 0.175,
            "f1": 0.19021739130434787
        },
        "landmark (MME)": {
            "acc": 0.2725,
            "prec": 0.21739130434782608,
            "rec": 0.175,
            "f1": 0.19390581717451522
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "existence (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.5769230769230769,
            "rec": 1.0,
            "f1": 0.7317073170731707
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5087719298245614,
            "rec": 0.9666666666666667,
            "f1": 0.6666666666666667
        },
        "color (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5087719298245614,
            "rec": 0.9666666666666667,
            "f1": 0.6666666666666667
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5714285714285714,
            "rec": 0.6,
            "f1": 0.5853658536585366
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5384615384615384,
            "rec": 0.35,
            "f1": 0.4242424242424242
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22093023255813954
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2507936507936508
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.25384615384615383
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2374429223744292
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.27932960893854747
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2837209302325581
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.2727272727272727
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2474747474747475
        },
        "ocr (MMBench_CN)": {
            "acc": 0.3141025641025641
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2765957446808511
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.22
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2532894736842105
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.3829787234042553
        },
        "image_style (MMBench_CN)": {
            "acc": 0.22169811320754718
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.23106060606060605
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.29333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.3
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.3081395348837209
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.23492063492063492
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.34615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2968036529680365
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3829787234042553
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.24022346368715083
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2651162790697674
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.3882063882063882
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_EN)": {
            "acc": 0.2564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2768361581920904
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2730496453900709
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.285
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.3848684210526316
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.29545454545454547
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2971698113207547
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.39015151515151514
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.35714285714285715
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.1
        },
        "Art_Theory (MMMU)": {
            "acc": 0.1
        },
        "History (MMMU)": {
            "acc": 0.1
        },
        "Materials (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.1
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.1
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2540983606557377
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2222222222222222
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.5753424657534246
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "State capitals (ScienceQA)": {
            "acc": 0.2403846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.5342465753424658
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.5314685314685315
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.4489795918367347
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.46551724137931033
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.4375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Classification (ScienceQA)": {
            "acc": 0.46835443037974683
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "2D Count (CVBench)": {
            "acc": 0.21954314720812182
        },
        "3D Distance (CVBench)": {
            "acc": 0.5016666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5169230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.48
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.241156389713288,
            "bert": 0.08714722394943238
        },
        "Whole dataset (Enrico)": {
            "bart": -7.338752329349518,
            "bert": 0.8323619318008423
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.463351855278015,
            "bert": 0.9887948662042618
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.15832090139389,
            "bert": 0.6143348509073258
        },
        "Whole dataset (GQA)": {
            "bart": -5.929382697343827,
            "bert": 0.7484346610307694
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.890162134170533,
            "bert": 0.49861507177352904
        },
        "Whole dataset (INAT)": {
            "bart": -7.2327605247497555,
            "bert": 0.3246162211894989
        },
        "Whole dataset (IRFL)": {
            "bart": -5.054924017190933,
            "bert": 0.7881803619861603
        },
        "Whole dataset (MemeCaps)": {
            "bart": -5.559166412353516,
            "bert": 0.0
        },
        "Whole dataset (Memotion)": {
            "bart": -6.316015338897705,
            "bert": 0.009995511770248412
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.505060390233994,
            "bert": 0.2811475545167923
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.68750036239624,
            "bert": 0.019991267919540406
        },
        "Whole dataset (NLVR)": {
            "bart": -4.646794652938842,
            "bert": 0.639643348455429
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.773729639053345,
            "bert": 0.6195938390493393
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.587969286441803,
            "bert": 0.647694473862648
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.266976126432419,
            "bert": 0.5248681616783142
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.477245936393738,
            "bert": 0.8506638145446778
        },
        "Whole dataset (PathVQA)": {
            "bart": -7.000269420146942,
            "bert": 0.029954851865768434
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.8039270329475405,
            "bert": 0.893984437584877
        },
        "Whole dataset (Screen2Words)": {
            "bart": -7.047172751426697,
            "bert": 0.8275837790966034
        },
        "Whole dataset (Slake)": {
            "bart": -7.443435363769531,
            "bert": 0.049958218336105344
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.843385715484619,
            "bert": 0.12008862435817719
        },
        "Whole dataset (VCR)": {
            "bart": -6.279482898712158,
            "bert": 0.08251911401748657
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.8149184650182724,
            "bert": 0.7267489147186279
        },
        "Whole dataset (VQA)": {
            "bart": -5.805981789827347,
            "bert": 0.6811661696434022
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.022696231603622,
            "bert": 0.7642311549186707
        },
        "Whole dataset (Winoground)": {
            "bart": -7.143393230438233,
            "bert": 0.1897527277469635
        },
        "random (POPE)": {
            "acc": 0.742671009771987,
            "prec": 0.7169811320754716,
            "rec": 0.7702702702702703,
            "f1": 0.742671009771987
        },
        "popular (POPE)": {
            "acc": 0.7785016286644951,
            "prec": 0.779874213836478,
            "rec": 0.7898089171974523,
            "f1": 0.7848101265822786
        },
        "adversarial (POPE)": {
            "acc": 0.6993006993006993,
            "prec": 0.6455696202531646,
            "rec": 0.7727272727272727,
            "f1": 0.7034482758620689
        }
    },
    "LLaMA-Adapter-V2-BIAS-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.25166702516670253
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2559598494353827
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.21357285429141717
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.374
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2987331426236208
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2383673469387755
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2942528735632184
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24603174603174602
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.35520212060967526
        },
        "Instance Location (SEED_2)": {
            "acc": 0.25153374233128833
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.28865979381443296
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.26852438252058264
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2861824139814309
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.6329113924050633
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.1893939393939394
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2617960426179604
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.3574007220216607
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24144672531769307
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.2598187311178248
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.325
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2830188679245283
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.48787878787878786
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.3165829145728643
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.6411764705882353,
            "prec": 0.6016949152542372,
            "rec": 0.8352941176470589,
            "f1": 0.6995073891625616
        },
        "posters (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6069868995633187,
            "rec": 0.9455782312925171,
            "f1": 0.7393617021276595
        },
        "position (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "scene (MME)": {
            "acc": 0.8175,
            "prec": 0.7679324894514767,
            "rec": 0.91,
            "f1": 0.8329519450800915
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5785714285714286,
            "prec": 0.5523809523809524,
            "rec": 0.8285714285714286,
            "f1": 0.6628571428571429
        },
        "artwork (MME)": {
            "acc": 0.565,
            "prec": 0.5361111111111111,
            "rec": 0.965,
            "f1": 0.6892857142857143
        },
        "landmark (MME)": {
            "acc": 0.855,
            "prec": 0.8480392156862745,
            "rec": 0.865,
            "f1": 0.8564356435643564
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6382978723404256,
            "rec": 1.0,
            "f1": 0.7792207792207793
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.42857142857142855,
            "rec": 0.3,
            "f1": 0.3529411764705882
        },
        "count (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "color (MME)": {
            "acc": 0.55,
            "prec": 0.5294117647058824,
            "rec": 0.9,
            "f1": 0.6666666666666667
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.7272727272727273,
            "rec": 0.8,
            "f1": 0.761904761904762
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22674418604651161
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.24126984126984127
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.16153846153846155
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2191780821917808
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.24113475177304963
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.29608938547486036
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.24186046511627907
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.21212121212121213
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23717948717948717
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.1807909604519774
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.14184397163120568
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.24013157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.25
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.20212765957446807
        },
        "image_style (MMBench_CN)": {
            "acc": 0.2688679245283019
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.26136363636363635
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.21333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.19285714285714287
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.313953488372093
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.34285714285714286
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3150684931506849
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3049645390070922
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2905027932960894
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.3813953488372093
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.5135135135135135
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.48484848484848486
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21153846153846154
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.44
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.33881578947368424
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8693181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2978723404255319
        },
        "image_style (MMBench_EN)": {
            "acc": 0.6698113207547169
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.3522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3466666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.24285714285714285
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.0
        },
        "Pharmacy (MMMU)": {
            "acc": 0.0
        },
        "Public_Health (MMMU)": {
            "acc": 0.0
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.0
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.0
        },
        "Marketing (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1885245901639344
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.20915032679738563
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3058823529411765
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.14772727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4246575342465753
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.5944055944055944
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.22580645161290322
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.43902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4666666666666667
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.4782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.5625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.16666666666666666
        },
        "Maps (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.13157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.46835443037974683
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7241379310344828
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.19718309859154928
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.23223350253807107
        },
        "3D Distance (CVBench)": {
            "acc": 0.11166666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.5584615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.59
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.739744386672974,
            "bert": 0.7953651726245881
        },
        "Whole dataset (Enrico)": {
            "bart": -7.061825213432312,
            "bert": 0.8239531654119492
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.048857173919678,
            "bert": 0.8213923490047454
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4343957722187044,
            "bert": 0.8641426515579224
        },
        "Whole dataset (GQA)": {
            "bart": -5.640161099433899,
            "bert": 0.8305559557676315
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.227293858528137,
            "bert": 0.8016704595088959
        },
        "Whole dataset (INAT)": {
            "bart": -6.261879215240478,
            "bert": 0.7929269778728485
        },
        "Whole dataset (IRFL)": {
            "bart": -6.179401540756226,
            "bert": 0.821126382946968
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9085103845596314,
            "bert": 0.8535261261463165
        },
        "Whole dataset (Memotion)": {
            "bart": -5.169304251670837,
            "bert": 0.8399798142910003
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.683828725814819,
            "bert": 0.8257602274417877
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.048743081092835,
            "bert": 0.82799429833889
        },
        "Whole dataset (NLVR)": {
            "bart": -6.059604125022888,
            "bert": 0.8179370671510696
        },
        "Whole dataset (NLVR2)": {
            "bart": -6.1799232339859005,
            "bert": 0.8106567895412445
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.655110800266266,
            "bert": 0.8797384643554688
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.2093727803230285,
            "bert": 0.8343040174245835
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.287584873437882,
            "bert": 0.801107730269432
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.731725335121155,
            "bert": 0.8240169590711593
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.867060117721557,
            "bert": 0.8191900938749314
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.107223062515259,
            "bert": 0.8492543613910675
        },
        "Whole dataset (Slake)": {
            "bart": -5.555677585601806,
            "bert": 0.8364407217502594
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.206253962516785,
            "bert": 0.7821965092420577
        },
        "Whole dataset (VCR)": {
            "bart": -3.7926270723342896,
            "bert": 0.8793673640489579
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.22578469157219,
            "bert": 0.891424975991249
        },
        "Whole dataset (VQA)": {
            "bart": -6.085953736305237,
            "bert": 0.8352419942617416
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.662932734489441,
            "bert": 0.8338826256990433
        },
        "Whole dataset (Winoground)": {
            "bart": -5.840136389732361,
            "bert": 0.8249773299694061
        },
        "random (POPE)": {
            "acc": 0.5342019543973942,
            "prec": 0.5085910652920962,
            "rec": 1.0,
            "f1": 0.6742596810933941
        },
        "popular (POPE)": {
            "acc": 0.5146579804560261,
            "prec": 0.5130718954248366,
            "rec": 1.0,
            "f1": 0.6781857451403888
        },
        "adversarial (POPE)": {
            "acc": 0.46503496503496505,
            "prec": 0.4628975265017668,
            "rec": 0.9924242424242424,
            "f1": 0.6313253012048193
        }
    },
    "LLaMA-Adapter-V2-LORA-BIAS-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.2568294256829426
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2435129740518962
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2559598494353827
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2954091816367265
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.38
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.26808336738863914
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2636734693877551
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3195402298850575
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25892857142857145
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.365142478462558
        },
        "Instance Location (SEED_2)": {
            "acc": 0.254601226993865
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.3711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.25554148195060167
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2823593664664118
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.6835443037974683
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2803030303030303
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.11976047904191617
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2800608828006088
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.35379061371841153
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.21505376344086022
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.256797583081571
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.35
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.3018867924528302
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5272727272727272
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.27638190954773867
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.7705882352941177,
            "prec": 0.7211538461538461,
            "rec": 0.8823529411764706,
            "f1": 0.7936507936507936
        },
        "posters (MME)": {
            "acc": 0.6972789115646258,
            "prec": 0.6559139784946236,
            "rec": 0.8299319727891157,
            "f1": 0.7327327327327328
        },
        "position (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.4915254237288136,
            "rec": 0.9666666666666667,
            "f1": 0.651685393258427
        },
        "scene (MME)": {
            "acc": 0.82,
            "prec": 0.8333333333333334,
            "rec": 0.8,
            "f1": 0.816326530612245
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6067415730337079,
            "rec": 0.7714285714285715,
            "f1": 0.679245283018868
        },
        "artwork (MME)": {
            "acc": 0.54,
            "prec": 0.5211640211640212,
            "rec": 0.985,
            "f1": 0.6816608996539791
        },
        "landmark (MME)": {
            "acc": 0.8275,
            "prec": 0.901840490797546,
            "rec": 0.735,
            "f1": 0.8099173553719009
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6666666666666666,
            "rec": 0.1,
            "f1": 0.1739130434782609
        },
        "existence (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.47368421052631576,
            "rec": 0.45,
            "f1": 0.46153846153846156
        },
        "count (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5454545454545454,
            "rec": 1.0,
            "f1": 0.7058823529411764
        },
        "color (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.5263157894736842,
            "rec": 0.6666666666666666,
            "f1": 0.5882352941176471
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.6785714285714286,
            "rec": 0.95,
            "f1": 0.7916666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.25
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.25396825396825395
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.1
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.228310502283105
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.3049645390070922
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.21787709497206703
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.22325581395348837
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_CN)": {
            "acc": 0.22435897435897437
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.1864406779661017
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.19148936170212766
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.19
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.26644736842105265
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.24431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2553191489361702
        },
        "image_style (MMBench_CN)": {
            "acc": 0.2169811320754717
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2916666666666667
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.313953488372093
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.326984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.25384615384615383
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2602739726027397
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.3407821229050279
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.3302325581395349
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.4275184275184275
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.4823232323232323
        },
        "ocr (MMBench_EN)": {
            "acc": 0.32051282051282054
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2907801418439716
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.38
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.3717105263157895
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.4147727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.39361702127659576
        },
        "image_style (MMBench_EN)": {
            "acc": 0.6273584905660378
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.4393939393939394
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.30666666666666664
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2857142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.0
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.0
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.0
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.0
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.0
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.06349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20098039215686275
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3972602739726027
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "State capitals (ScienceQA)": {
            "acc": 0.5
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.5753424657534246
        },
        "States of matter (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.6122448979591837
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.4523809523809524
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.40625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.037037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Classification (ScienceQA)": {
            "acc": 0.569620253164557
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.1267605633802817
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.24492385786802032
        },
        "3D Distance (CVBench)": {
            "acc": 0.02666666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5476923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.365
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.4558424735069275,
            "bert": 0.8308967053890228
        },
        "Whole dataset (Enrico)": {
            "bart": -6.790813817977905,
            "bert": 0.8227580142021179
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.9667532348632815,
            "bert": 0.8237073075771332
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4515946674346925,
            "bert": 0.8632320338487625
        },
        "Whole dataset (GQA)": {
            "bart": -5.683895988464355,
            "bert": 0.8283507394790649
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.325220494270325,
            "bert": 0.8042623370885849
        },
        "Whole dataset (INAT)": {
            "bart": -6.265754284858704,
            "bert": 0.7955108934640884
        },
        "Whole dataset (IRFL)": {
            "bart": -5.913679666519165,
            "bert": 0.8307672649621963
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9937132453918456,
            "bert": 0.8473617231845856
        },
        "Whole dataset (Memotion)": {
            "bart": -5.053593070507049,
            "bert": 0.8341049581766129
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.547276091575623,
            "bert": 0.824303777217865
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.235124917030334,
            "bert": 0.8175614458322525
        },
        "Whole dataset (NLVR)": {
            "bart": -5.26288149356842,
            "bert": 0.9142602640390396
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.210178554058075,
            "bert": 0.8685937470197678
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.60907389998436,
            "bert": 0.8789427584409714
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.1782971858978275,
            "bert": 0.834308556318283
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.181103885173798,
            "bert": 0.7980731898546218
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.638530626296997,
            "bert": 0.8208940440416336
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.424683542251587,
            "bert": 0.8114081156253815
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.057856750488281,
            "bert": 0.8496653950214386
        },
        "Whole dataset (Slake)": {
            "bart": -5.329823923110962,
            "bert": 0.8308341819047927
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.1928989028930665,
            "bert": 0.8005968195199966
        },
        "Whole dataset (VCR)": {
            "bart": -3.8223586028814314,
            "bert": 0.8938149774074554
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.176244060397148,
            "bert": 0.8897617429494857
        },
        "Whole dataset (VQA)": {
            "bart": -5.975220634937286,
            "bert": 0.8357027053833008
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.696383810043335,
            "bert": 0.8311474603414536
        },
        "Whole dataset (Winoground)": {
            "bart": -5.780665054321289,
            "bert": 0.8311839908361435
        },
        "random (POPE)": {
            "acc": 0.7003257328990228,
            "prec": 0.6206896551724138,
            "rec": 0.972972972972973,
            "f1": 0.7578947368421053
        },
        "popular (POPE)": {
            "acc": 0.5635179153094463,
            "prec": 0.5415162454873647,
            "rec": 0.9554140127388535,
            "f1": 0.6912442396313364
        },
        "adversarial (POPE)": {
            "acc": 0.493006993006993,
            "prec": 0.47547169811320755,
            "rec": 0.9545454545454546,
            "f1": 0.6347607052896727
        }
    },
    "LLaMA-Adapter-V2-LORA-BIAS-7B-v21": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.2617767261776726
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25156838143036386
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.18562874251497005
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.42
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.2627707396812423
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2587755102040816
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.271264367816092
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2698412698412698
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.30483764082173626
        },
        "Instance Location (SEED_2)": {
            "acc": 0.26993865030674846
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.35051546391752575
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.2982900569981001
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.3178590933915893
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.7848101265822784
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.14770459081836326
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2663622526636225
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2996389891696751
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.0801564027370479
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.36555891238670696
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.21666666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.31446540880503143
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5787878787878787
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.3969849246231156
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2653061224489796
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2716049382716049
        },
        "celebrity (MME)": {
            "acc": 0.7794117647058824,
            "prec": 0.8740157480314961,
            "rec": 0.6529411764705882,
            "f1": 0.7474747474747474
        },
        "posters (MME)": {
            "acc": 0.7755102040816326,
            "prec": 0.7612903225806451,
            "rec": 0.8027210884353742,
            "f1": 0.7814569536423841
        },
        "position (MME)": {
            "acc": 0.4666666666666667,
            "prec": 0.4807692307692308,
            "rec": 0.8333333333333334,
            "f1": 0.6097560975609756
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8666666666666667,
            "rec": 0.845,
            "f1": 0.8556962025316455
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.4928571428571429,
            "prec": 0.49333333333333335,
            "rec": 0.5285714285714286,
            "f1": 0.5103448275862069
        },
        "artwork (MME)": {
            "acc": 0.63,
            "prec": 0.5807453416149069,
            "rec": 0.935,
            "f1": 0.7164750957854407
        },
        "landmark (MME)": {
            "acc": 0.875,
            "prec": 0.8378378378378378,
            "rec": 0.93,
            "f1": 0.8815165876777251
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.75,
            "rec": 0.15,
            "f1": 0.24999999999999997
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.14285714285714285,
            "rec": 0.05,
            "f1": 0.07407407407407408
        },
        "count (MME)": {
            "acc": 0.6,
            "prec": 0.5555555555555556,
            "rec": 1.0,
            "f1": 0.7142857142857143
        },
        "color (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5609756097560976,
            "rec": 0.7666666666666667,
            "f1": 0.6478873239436621
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4827586206896552,
            "rec": 0.7,
            "f1": 0.5714285714285714
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.19186046511627908
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.20952380952380953
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2153846153846154
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.1872146118721461
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.3120567375886525
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.08379888268156424
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.43243243243243246
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.26262626262626265
        },
        "ocr (MMBench_CN)": {
            "acc": 0.2564102564102564
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.1524822695035461
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.585
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.16776315789473684
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.4772727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.14893617021276595
        },
        "image_style (MMBench_CN)": {
            "acc": 0.32547169811320753
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.24242424242424243
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.18666666666666668
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.32142857142857145
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.3430232558139535
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.28888888888888886
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.36153846153846153
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.182648401826484
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22905027932960895
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.39069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.4643734643734644
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.5833333333333334
        },
        "ocr (MMBench_EN)": {
            "acc": 0.2692307692307692
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2768361581920904
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.25886524822695034
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.54
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.40131578947368424
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.6647727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_EN)": {
            "acc": 0.5754716981132075
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.4053030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.32
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.32142857142857145
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.1
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.0
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29098360655737704
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.07936507936507936
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.24705882352941178
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2107843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6027397260273972
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.46794871794871795
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.6301369863013698
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.5734265734265734
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.21428571428571427
        },
        "Magnets (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3620689655172414
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.5272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.53125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.5316455696202531
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.1267605633802817
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.21192893401015228
        },
        "3D Distance (CVBench)": {
            "acc": 0.03666666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.5476923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.49333333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.693329596519471,
            "bert": 0.835782767534256
        },
        "Whole dataset (Enrico)": {
            "bart": -6.784118866920471,
            "bert": 0.8067203569412231
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.974554581642151,
            "bert": 0.8223761647939682
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.448187403678894,
            "bert": 0.861966249346733
        },
        "Whole dataset (GQA)": {
            "bart": -5.241364843845368,
            "bert": 0.9443695765733718
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.923797850608826,
            "bert": 0.8011021077632904
        },
        "Whole dataset (INAT)": {
            "bart": -6.304342498779297,
            "bert": 0.7915590023994445
        },
        "Whole dataset (IRFL)": {
            "bart": -5.349603242874146,
            "bert": 0.9474209237098694
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9822379517555238,
            "bert": 0.847867419719696
        },
        "Whole dataset (Memotion)": {
            "bart": -5.1139582967758175,
            "bert": 0.8322787272930146
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.2223848581314085,
            "bert": 0.8286578452587128
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.036656360626221,
            "bert": 0.8231722915172577
        },
        "Whole dataset (NLVR)": {
            "bart": -4.96525764465332,
            "bert": 0.9488839209079742
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.696540157794953,
            "bert": 0.9499547529220581
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.6484513485431673,
            "bert": 0.877128871679306
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.408466861248017,
            "bert": 0.9449331718683243
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.28679431438446,
            "bert": 0.8034623628854751
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.3847429251670835,
            "bert": 0.9043102151155472
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.527137899398804,
            "bert": 0.8153417098522187
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.997528200149536,
            "bert": 0.8459425640106201
        },
        "Whole dataset (Slake)": {
            "bart": -4.8531571078300475,
            "bert": 0.9372704684734344
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.903242015838623,
            "bert": 0.7982258254289627
        },
        "Whole dataset (VCR)": {
            "bart": -3.79979320704937,
            "bert": 0.897593794465065
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7344815742969515,
            "bert": 0.9496668809652329
        },
        "Whole dataset (VQA)": {
            "bart": -5.462587070465088,
            "bert": 0.9346167796850204
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.500332505702972,
            "bert": 0.9141748160123825
        },
        "Whole dataset (Winoground)": {
            "bart": -5.552553477287293,
            "bert": 0.9376625925302505
        },
        "random (POPE)": {
            "acc": 0.8403908794788274,
            "prec": 0.9380530973451328,
            "rec": 0.7162162162162162,
            "f1": 0.8122605363984673
        },
        "popular (POPE)": {
            "acc": 0.7361563517915309,
            "prec": 0.7878787878787878,
            "rec": 0.6624203821656051,
            "f1": 0.7197231833910034
        },
        "adversarial (POPE)": {
            "acc": 0.7692307692307693,
            "prec": 0.7704918032786885,
            "rec": 0.7121212121212122,
            "f1": 0.7401574803149605
        }
    },
    "OpenFlamingo-3B-vitl-mpt1b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.23381372338137235
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.21357285429141717
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.23212045169385195
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2634730538922156
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.26
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.15488353085410708
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23755102040816325
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2206896551724138
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.21825396825396826
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2001325381047051
        },
        "Instance Location (SEED_2)": {
            "acc": 0.2280163599182004
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.21649484536082475
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.23210892970234326
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.23320589841616604
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.5949367088607594
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.23484848484848486
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.27944111776447106
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2511415525114155
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23465703971119134
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.25219941348973607
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.22356495468277945
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3090909090909091
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.22613065326633167
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.20408163265306123
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.19753086419753085
        },
        "celebrity (MME)": {
            "acc": 0.21176470588235294,
            "prec": 0.12878787878787878,
            "rec": 0.1,
            "f1": 0.11258278145695363
        },
        "posters (MME)": {
            "acc": 0.24149659863945577,
            "prec": 0.30412371134020616,
            "rec": 0.4013605442176871,
            "f1": 0.34604105571847504
        },
        "position (MME)": {
            "acc": 0.4,
            "prec": 0.425,
            "rec": 0.5666666666666667,
            "f1": 0.48571428571428565
        },
        "scene (MME)": {
            "acc": 0.4725,
            "prec": 0.48450704225352115,
            "rec": 0.86,
            "f1": 0.6198198198198198
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.34285714285714286,
            "prec": 0.3877551020408163,
            "rec": 0.5428571428571428,
            "f1": 0.4523809523809524
        },
        "artwork (MME)": {
            "acc": 0.345,
            "prec": 0.4054878048780488,
            "rec": 0.665,
            "f1": 0.5037878787878789
        },
        "landmark (MME)": {
            "acc": 0.43,
            "prec": 0.4590643274853801,
            "rec": 0.785,
            "f1": 0.5793357933579335
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.4166666666666667,
            "prec": 0.4358974358974359,
            "rec": 0.5666666666666667,
            "f1": 0.4927536231884058
        },
        "numerical_calculation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.4888888888888889,
            "rec": 0.7333333333333333,
            "f1": 0.5866666666666667
        },
        "color (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.4864864864864865,
            "rec": 0.6,
            "f1": 0.5373134328358209
        },
        "OCR (MME)": {
            "acc": 0.275,
            "prec": 0.2631578947368421,
            "rec": 0.25,
            "f1": 0.25641025641025644
        },
        "code_reasoning (MME)": {
            "acc": 0.275,
            "prec": 0.3548387096774194,
            "rec": 0.55,
            "f1": 0.43137254901960786
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.12790697674418605
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2507936507936508
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2602739726027397
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.07801418439716312
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.17318435754189945
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.24186046511627907
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.25061425061425063
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2878787878787879
        },
        "ocr (MMBench_CN)": {
            "acc": 0.2692307692307692
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.20212765957446807
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.305
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.22697368421052633
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.3806818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_CN)": {
            "acc": 0.330188679245283
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.30303030303030304
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.24
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.12857142857142856
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.1511627906976744
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.24444444444444444
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.27692307692307694
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2009132420091324
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.1564245810055866
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2558139534883721
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.37626262626262624
        },
        "ocr (MMBench_EN)": {
            "acc": 0.1858974358974359
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2711864406779661
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.14893617021276595
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.19407894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.25
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_EN)": {
            "acc": 0.22169811320754718
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.26515151515151514
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3466666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.20714285714285716
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.0873015873015873
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1895424836601307
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12254901960784313
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.18181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4931506849315068
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.2564102564102564
        },
        "State capitals (ScienceQA)": {
            "acc": 0.21153846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.4965034965034965
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5408163265306123
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.08064516129032258
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.17073170731707318
        },
        "Geography (ScienceQA)": {
            "acc": 0.23809523809523808
        },
        "Magnets (ScienceQA)": {
            "acc": 0.05454545454545454
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.08888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.21875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2037037037037037
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Classification (ScienceQA)": {
            "acc": 0.46835443037974683
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.18309859154929578
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.1916243654822335
        },
        "3D Distance (CVBench)": {
            "acc": 0.42
        },
        "2D Relation (CVBench)": {
            "acc": 0.49846153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.5016666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.145001685619354,
            "bert": 0.7816054153442383
        },
        "Whole dataset (Enrico)": {
            "bart": -7.363239974975586,
            "bert": 0.7977467882633209
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -7.070642395019531,
            "bert": 0.9820776498317718
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.8220277047157287,
            "bert": 0.840431849360466
        },
        "Whole dataset (GQA)": {
            "bart": -7.283329997062683,
            "bert": 0.9034615665674209
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.856485865116119,
            "bert": 0.9318488770723343
        },
        "Whole dataset (INAT)": {
            "bart": -6.441867361068725,
            "bert": 0.8009251570701599
        },
        "Whole dataset (IRFL)": {
            "bart": -7.055507855415344,
            "bert": 0.8307172679901123
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.8574152851104735,
            "bert": 0.8488249802589416
        },
        "Whole dataset (Memotion)": {
            "bart": -6.1593441390991215,
            "bert": 0.8194164019823075
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.444689269065857,
            "bert": 0.7939001524448395
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.062977628707886,
            "bert": 0.8875960689783097
        },
        "Whole dataset (NLVR)": {
            "bart": -8.056307921409607,
            "bert": 0.9344930791854859
        },
        "Whole dataset (NLVR2)": {
            "bart": -7.082762055397033,
            "bert": 0.8829058957099915
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.2594077157974244,
            "bert": 0.8608177560567856
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.678307065963745,
            "bert": 0.8648897629976272
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.580177707672119,
            "bert": 0.7912153053283691
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.750310757160187,
            "bert": 0.865486421585083
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.710202622413635,
            "bert": 0.8181587493419648
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.708980598449707,
            "bert": 0.8393256211280823
        },
        "Whole dataset (Slake)": {
            "bart": -7.050908066034317,
            "bert": 0.9255456072092056
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.729253015518188,
            "bert": 0.7301832002401352
        },
        "Whole dataset (VCR)": {
            "bart": -3.8263650518655776,
            "bert": 0.8418706053495407
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.313208231925964,
            "bert": 0.8949938023090362
        },
        "Whole dataset (VQA)": {
            "bart": -7.047397587299347,
            "bert": 0.9121898943185807
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.816567692756653,
            "bert": 0.8498941200971604
        },
        "Whole dataset (Winoground)": {
            "bart": -7.388924384117127,
            "bert": 0.8301627171039582
        },
        "random (POPE)": {
            "acc": 0.46579804560260585,
            "prec": 0.4689922480620155,
            "rec": 0.8175675675675675,
            "f1": 0.5960591133004927
        },
        "popular (POPE)": {
            "acc": 0.48859934853420195,
            "prec": 0.5,
            "rec": 0.8280254777070064,
            "f1": 0.6235011990407674
        },
        "adversarial (POPE)": {
            "acc": 0.44755244755244755,
            "prec": 0.4449152542372881,
            "rec": 0.7954545454545454,
            "f1": 0.5706521739130435
        }
    },
    "OpenFlamingo-3B-vitl-mpt1b-langinstruct": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.17487631748763174
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.21580928481806774
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26147704590818366
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.234
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.1348590110339191
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24816326530612245
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2367816091954023
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.1765873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.08880053015241882
        },
        "Instance Location (SEED_2)": {
            "acc": 0.2167689161554192
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.25773195876288657
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.22324255858138062
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.22829055161114145
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.6708860759493671
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.14171656686626746
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.1917808219178082
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.19855595667870035
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.17888563049853373
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.2175226586102719
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.24166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18238993710691823
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.23636363636363636
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2663316582914573
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.27941176470588236,
            "prec": 0.3563218390804598,
            "rec": 0.5470588235294118,
            "f1": 0.431554524361949
        },
        "posters (MME)": {
            "acc": 0.1054421768707483,
            "prec": 0.17415730337078653,
            "rec": 0.2108843537414966,
            "f1": 0.1907692307692308
        },
        "position (MME)": {
            "acc": 0.4,
            "prec": 0.4444444444444444,
            "rec": 0.8,
            "f1": 0.5714285714285714
        },
        "scene (MME)": {
            "acc": 0.315,
            "prec": 0.38580246913580246,
            "rec": 0.625,
            "f1": 0.4770992366412214
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.35,
            "prec": 0.4117647058823529,
            "rec": 0.7,
            "f1": 0.5185185185185185
        },
        "artwork (MME)": {
            "acc": 0.135,
            "prec": 0.2125984251968504,
            "rec": 0.27,
            "f1": 0.2378854625550661
        },
        "landmark (MME)": {
            "acc": 0.105,
            "prec": 0.17355371900826447,
            "rec": 0.21,
            "f1": 0.1900452488687783
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.38333333333333336,
            "prec": 0.43137254901960786,
            "rec": 0.7333333333333333,
            "f1": 0.54320987654321
        },
        "numerical_calculation (MME)": {
            "acc": 0.2,
            "prec": 0.2857142857142857,
            "rec": 0.4,
            "f1": 0.3333333333333333
        },
        "count (MME)": {
            "acc": 0.4666666666666667,
            "prec": 0.48214285714285715,
            "rec": 0.9,
            "f1": 0.627906976744186
        },
        "color (MME)": {
            "acc": 0.36666666666666664,
            "prec": 0.4230769230769231,
            "rec": 0.7333333333333333,
            "f1": 0.5365853658536585
        },
        "OCR (MME)": {
            "acc": 0.275,
            "prec": 0.3548387096774194,
            "rec": 0.55,
            "f1": 0.43137254901960786
        },
        "code_reasoning (MME)": {
            "acc": 0.375,
            "prec": 0.42857142857142855,
            "rec": 0.75,
            "f1": 0.5454545454545454
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.1569767441860465
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2698412698412698
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2648401826484018
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.20567375886524822
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.18435754189944134
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.22325581395348837
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.2678132678132678
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.25
        },
        "ocr (MMBench_CN)": {
            "acc": 0.25
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.18439716312056736
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.33
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.24342105263157895
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2897727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_CN)": {
            "acc": 0.3018867924528302
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.26
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.11627906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2222222222222222
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.26153846153846155
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2191780821917808
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.1702127659574468
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.1452513966480447
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.25116279069767444
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.27525252525252525
        },
        "ocr (MMBench_EN)": {
            "acc": 0.14743589743589744
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.17375886524822695
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.19
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.20065789473684212
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2727272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.18085106382978725
        },
        "image_style (MMBench_EN)": {
            "acc": 0.23113207547169812
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.25757575757575757
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.34
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.25
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.0
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.1
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.20081967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.047619047619047616
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.13725490196078433
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.09803921568627451
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.20454545454545456
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.547945205479452
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 0.1891025641025641
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.4755244755244755
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.40816326530612246
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.04838709677419355
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.24390243902439024
        },
        "Geography (ScienceQA)": {
            "acc": 0.16666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.3181818181818182
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.08888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.25
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.2608695652173913
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Classification (ScienceQA)": {
            "acc": 0.4177215189873418
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.16901408450704225
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.11928934010152284
        },
        "3D Distance (CVBench)": {
            "acc": 0.36
        },
        "2D Relation (CVBench)": {
            "acc": 0.49538461538461537
        },
        "3D Depth (CVBench)": {
            "acc": 0.47333333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.094859890937805,
            "bert": 0.8043894827365875
        },
        "Whole dataset (Enrico)": {
            "bart": -7.353279047012329,
            "bert": 0.797709778547287
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.735897440910339,
            "bert": 0.9357443803548813
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.9694398403167725,
            "bert": 0.8217556238174438
        },
        "Whole dataset (GQA)": {
            "bart": -5.838836915493012,
            "bert": 0.9029173266887665
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.197940602302551,
            "bert": 0.9247949862480164
        },
        "Whole dataset (INAT)": {
            "bart": -6.334321184158325,
            "bert": 0.8024729597568512
        },
        "Whole dataset (IRFL)": {
            "bart": -6.378250403404236,
            "bert": 0.8443064665794373
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.7057149648666385,
            "bert": 0.8577798449993134
        },
        "Whole dataset (Memotion)": {
            "bart": -5.913018779754639,
            "bert": 0.8706302970647812
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.24101155757904,
            "bert": 0.7685476750135422
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.0125569295883174,
            "bert": 0.9083701926469803
        },
        "Whole dataset (NLVR)": {
            "bart": -4.353880913257599,
            "bert": 0.9446348744630814
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.765487060546875,
            "bert": 0.9043903595209122
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.4411549758911133,
            "bert": 0.8407555109262467
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.371177117824555,
            "bert": 0.8756877392530441
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.993953745365143,
            "bert": 0.8315951728820801
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.778378491401672,
            "bert": 0.8619079679250717
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.610716753005981,
            "bert": 0.8282263064384461
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.58281099319458,
            "bert": 0.8499671614170075
        },
        "Whole dataset (Slake)": {
            "bart": -5.048704074621201,
            "bert": 0.9408001536130906
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.662944099903107,
            "bert": 0.7323930490016938
        },
        "Whole dataset (VCR)": {
            "bart": -4.141010944247245,
            "bert": 0.8590565353631974
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.48014298081398,
            "bert": 0.9071682626008988
        },
        "Whole dataset (VQA)": {
            "bart": -6.757991561889648,
            "bert": 0.8911446803808212
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.9402988910675045,
            "bert": 0.8561947989463806
        },
        "Whole dataset (Winoground)": {
            "bart": -6.613386030197144,
            "bert": 0.8378998494148254
        },
        "random (POPE)": {
            "acc": 0.4527687296416938,
            "prec": 0.4645390070921986,
            "rec": 0.8851351351351351,
            "f1": 0.6093023255813953
        },
        "popular (POPE)": {
            "acc": 0.49185667752442996,
            "prec": 0.5017421602787456,
            "rec": 0.9171974522292994,
            "f1": 0.6486486486486486
        },
        "adversarial (POPE)": {
            "acc": 0.45454545454545453,
            "prec": 0.45555555555555555,
            "rec": 0.9318181818181818,
            "f1": 0.6119402985074627
        }
    },
    "OpenFlamingo-4B-vitl-rpj3b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.22026242202624222
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.18883312421580928
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.22
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.23947691050265632
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2383673469387755
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.20229885057471264
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2123015873015873
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24784625579854208
        },
        "Instance Location (SEED_2)": {
            "acc": 0.2331288343558282
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.18556701030927836
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.22039265357821405
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2206444565811032
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.5063291139240507
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.23552894211576847
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2343987823439878
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2527075812274368
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.01857282502443793
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.21450151057401812
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.25157232704402516
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3303030303030303
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.5117647058823529,
            "prec": 0.5060975609756098,
            "rec": 0.9764705882352941,
            "f1": 0.6666666666666667
        },
        "posters (MME)": {
            "acc": 0.336734693877551,
            "prec": 0.39915966386554624,
            "rec": 0.6462585034013606,
            "f1": 0.4935064935064936
        },
        "position (MME)": {
            "acc": 0.43333333333333335,
            "prec": 0.46153846153846156,
            "rec": 0.8,
            "f1": 0.5853658536585367
        },
        "scene (MME)": {
            "acc": 0.5075,
            "prec": 0.504,
            "rec": 0.945,
            "f1": 0.6573913043478261
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.36428571428571427,
            "prec": 0.4188034188034188,
            "rec": 0.7,
            "f1": 0.5240641711229946
        },
        "artwork (MME)": {
            "acc": 0.47,
            "prec": 0.4844559585492228,
            "rec": 0.935,
            "f1": 0.6382252559726963
        },
        "landmark (MME)": {
            "acc": 0.455,
            "prec": 0.4748603351955307,
            "rec": 0.85,
            "f1": 0.6093189964157706
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5510204081632653,
            "rec": 0.9,
            "f1": 0.6835443037974683
        },
        "numerical_calculation (MME)": {
            "acc": 0.3,
            "prec": 0.36666666666666664,
            "rec": 0.55,
            "f1": 0.43999999999999995
        },
        "count (MME)": {
            "acc": 0.4666666666666667,
            "prec": 0.4807692307692308,
            "rec": 0.8333333333333334,
            "f1": 0.6097560975609756
        },
        "color (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5087719298245614,
            "rec": 0.9666666666666667,
            "f1": 0.6666666666666667
        },
        "OCR (MME)": {
            "acc": 0.425,
            "prec": 0.4594594594594595,
            "rec": 0.85,
            "f1": 0.5964912280701754
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.09302325581395349
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.20952380952380953
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.13846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.1552511415525114
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.06382978723404255
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.1005586592178771
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.16279069767441862
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.20393120393120392
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.19696969696969696
        },
        "ocr (MMBench_CN)": {
            "acc": 0.1346153846153846
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.1694915254237288
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.12056737588652482
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.13815789473684212
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.3181818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.1276595744680851
        },
        "image_style (MMBench_CN)": {
            "acc": 0.22169811320754718
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2196969696969697
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.18
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.14285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.18023255813953487
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.21587301587301588
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2923076923076923
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2191780821917808
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3404255319148936
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22905027932960895
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.22325581395348837
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.16216216216216217
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.351010101010101
        },
        "ocr (MMBench_EN)": {
            "acc": 0.16025641025641027
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2824858757062147
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.1702127659574468
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.205
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.23684210526315788
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.26136363636363635
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.25
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2840909090909091
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.32
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.15
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.1
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.0
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.19262295081967212
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.07936507936507936
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1437908496732026
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15294117647058825
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.10784313725490197
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.26136363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4383561643835616
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3141025641025641
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.4246575342465753
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.4825174825174825
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.45918367346938777
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3709677419354839
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2926829268292683
        },
        "Geography (ScienceQA)": {
            "acc": 0.23809523809523808
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.11764705882352941
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.17777777777777778
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.2608695652173913
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Classification (ScienceQA)": {
            "acc": 0.4430379746835443
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.18309859154929578
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.22842639593908629
        },
        "3D Distance (CVBench)": {
            "acc": 0.5033333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.5061538461538462
        },
        "3D Depth (CVBench)": {
            "acc": 0.5033333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.1089400506019595,
            "bert": 0.7676257985830307
        },
        "Whole dataset (Enrico)": {
            "bart": -7.32293240070343,
            "bert": 0.8138357812166214
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.2662975418567655,
            "bert": 0.9990490311384201
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.8626504778862,
            "bert": 0.8464174854755402
        },
        "Whole dataset (GQA)": {
            "bart": -6.968003635406494,
            "bert": 0.9138605982065201
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.305714697837829,
            "bert": 0.851125664114952
        },
        "Whole dataset (INAT)": {
            "bart": -6.072576313018799,
            "bert": 0.8040868359804153
        },
        "Whole dataset (IRFL)": {
            "bart": -6.247133383750915,
            "bert": 0.8129358464479446
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.367335374355316,
            "bert": 0.877469755411148
        },
        "Whole dataset (Memotion)": {
            "bart": -5.177773654460907,
            "bert": 0.8641177409887314
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.951295235157013,
            "bert": 0.8090591686964035
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.984027872085571,
            "bert": 0.8597804677486419
        },
        "Whole dataset (NLVR)": {
            "bart": -4.2099028587341305,
            "bert": 0.9610188484191895
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.278219444751739,
            "bert": 0.9393997263908386
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.1955675792694094,
            "bert": 0.872496188879013
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.730981950759888,
            "bert": 0.8566689610481262
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.4818355131149294,
            "bert": 0.8113990902900696
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.632496371269226,
            "bert": 0.860514634847641
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.627449290752411,
            "bert": 0.8408871293067932
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.574789652824402,
            "bert": 0.8528382027149201
        },
        "Whole dataset (Slake)": {
            "bart": -5.241120028495788,
            "bert": 0.9384641808271408
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.445445551872253,
            "bert": 0.791181492805481
        },
        "Whole dataset (VCR)": {
            "bart": -3.922941220998764,
            "bert": 0.8479848957061767
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.790286259651184,
            "bert": 0.8880747371912002
        },
        "Whole dataset (VQA)": {
            "bart": -7.157535743713379,
            "bert": 0.9262232446670532
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.446819658279419,
            "bert": 0.831428530216217
        },
        "Whole dataset (Winoground)": {
            "bart": -6.160931425094605,
            "bert": 0.8482593476772309
        },
        "random (POPE)": {
            "acc": 0.5407166123778502,
            "prec": 0.5507246376811594,
            "rec": 0.25675675675675674,
            "f1": 0.35023041474654376
        },
        "popular (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.5576923076923077,
            "rec": 0.18471337579617833,
            "f1": 0.27751196172248804
        },
        "adversarial (POPE)": {
            "acc": 0.5559440559440559,
            "prec": 0.5396825396825397,
            "rec": 0.25757575757575757,
            "f1": 0.3487179487179487
        }
    },
    "OpenFlamingo-4B-vitl-rpj3b-langinstruct": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.1787481178748118
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.21357285429141717
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.13362609786700125
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.1656686626746507
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.208
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.22394769105026563
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.1983673469387755
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.19080459770114944
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.22718253968253968
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.13850231941683233
        },
        "Instance Location (SEED_2)": {
            "acc": 0.18711656441717792
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.1958762886597938
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.18777707409753008
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.1638448935008192
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.3670886075949367
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.21212121212121213
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.11776447105788423
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2054794520547945
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.19494584837545126
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.12023460410557185
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.23564954682779457
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.1
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.1949685534591195
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.17575757575757575
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.30864197530864196
        },
        "celebrity (MME)": {
            "acc": 0.5117647058823529,
            "prec": 0.5084033613445378,
            "rec": 0.711764705882353,
            "f1": 0.5931372549019608
        },
        "posters (MME)": {
            "acc": 0.4557823129251701,
            "prec": 0.47686832740213525,
            "rec": 0.9115646258503401,
            "f1": 0.6261682242990654
        },
        "position (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.49019607843137253,
            "rec": 0.8333333333333334,
            "f1": 0.6172839506172839
        },
        "scene (MME)": {
            "acc": 0.5475,
            "prec": 0.5258855585831063,
            "rec": 0.965,
            "f1": 0.6807760141093475
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.39285714285714285,
            "prec": 0.4336283185840708,
            "rec": 0.7,
            "f1": 0.53551912568306
        },
        "artwork (MME)": {
            "acc": 0.48,
            "prec": 0.4897959183673469,
            "rec": 0.96,
            "f1": 0.6486486486486486
        },
        "landmark (MME)": {
            "acc": 0.505,
            "prec": 0.5026881720430108,
            "rec": 0.935,
            "f1": 0.6538461538461539
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.6,
            "prec": 0.5652173913043478,
            "rec": 0.8666666666666667,
            "f1": 0.6842105263157895
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.4642857142857143,
            "rec": 0.65,
            "f1": 0.5416666666666667
        },
        "count (MME)": {
            "acc": 0.5333333333333333,
            "prec": 0.52,
            "rec": 0.8666666666666667,
            "f1": 0.65
        },
        "color (MME)": {
            "acc": 0.45,
            "prec": 0.4716981132075472,
            "rec": 0.8333333333333334,
            "f1": 0.6024096385542169
        },
        "OCR (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.08139534883720931
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.13015873015873017
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.1076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.1278538812785388
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.04964539007092199
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.1564245810055866
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.19534883720930232
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.18673218673218672
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.19696969696969696
        },
        "ocr (MMBench_CN)": {
            "acc": 0.15384615384615385
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.1694915254237288
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.18085106382978725
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.215
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.15460526315789475
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.23295454545454544
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.0851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.1792452830188679
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.20454545454545456
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.23333333333333334
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.11627906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.22857142857142856
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.228310502283105
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.1702127659574468
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2011173184357542
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.28837209302325584
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.22604422604422605
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2601010101010101
        },
        "ocr (MMBench_EN)": {
            "acc": 0.1794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.1950354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.19
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.22697368421052633
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.3125
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.19148936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.22641509433962265
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2571428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.0
        },
        "Art_Theory (MMMU)": {
            "acc": 0.0
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.0
        },
        "Art (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.0
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.0
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.20901639344262296
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1323529411764706
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.26136363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.2564102564102564
        },
        "State capitals (ScienceQA)": {
            "acc": 0.15705128205128205
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3972602739726027
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.4195804195804196
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.46938775510204084
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.21428571428571427
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.46551724137931033
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.1956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Classification (ScienceQA)": {
            "acc": 0.4177215189873418
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.1916243654822335
        },
        "3D Distance (CVBench)": {
            "acc": 0.5433333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.5184615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.5133333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.1041039371490475,
            "bert": 0.7994246780872345
        },
        "Whole dataset (Enrico)": {
            "bart": -7.143825516700745,
            "bert": 0.8177809584140777
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.205577255487442,
            "bert": 0.9963218516111374
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.9017860770225523,
            "bert": 0.8353006452322006
        },
        "Whole dataset (GQA)": {
            "bart": -6.135145823955536,
            "bert": 0.9060010409355164
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.950588963031769,
            "bert": 0.8857545310258865
        },
        "Whole dataset (INAT)": {
            "bart": -6.3261153960227965,
            "bert": 0.799994964003563
        },
        "Whole dataset (IRFL)": {
            "bart": -6.385490307807922,
            "bert": 0.8323034536838532
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8281583893299103,
            "bert": 0.886546157002449
        },
        "Whole dataset (Memotion)": {
            "bart": -5.158756504058838,
            "bert": 0.8683618569374084
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.351139683723449,
            "bert": 0.7965703672170639
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.646278386116028,
            "bert": 0.8620762133598328
        },
        "Whole dataset (NLVR)": {
            "bart": -4.746691832542419,
            "bert": 0.9584085977077484
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.11453057050705,
            "bert": 0.9212201595306396
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.336230392456055,
            "bert": 0.845580683350563
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.600034348964691,
            "bert": 0.8702579486370087
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.254642916917801,
            "bert": 0.8182458567619324
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.6887390685081485,
            "bert": 0.8682795959711075
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.651993761062622,
            "bert": 0.8244460189342498
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.5655553436279295,
            "bert": 0.8471983009576798
        },
        "Whole dataset (Slake)": {
            "bart": -5.2448004889488224,
            "bert": 0.9437742882966995
        },
        "Whole dataset (UCMerced)": {
            "bart": -7.388094687461853,
            "bert": 0.8150439727306366
        },
        "Whole dataset (VCR)": {
            "bart": -3.923129026889801,
            "bert": 0.8542442786693573
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.496581671237945,
            "bert": 0.9124948155879974
        },
        "Whole dataset (VQA)": {
            "bart": -6.244521975517273,
            "bert": 0.894763331413269
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.816785621643066,
            "bert": 0.8787051296234131
        },
        "Whole dataset (Winoground)": {
            "bart": -5.589976663589478,
            "bert": 0.8067425036430359
        },
        "random (POPE)": {
            "acc": 0.5602605863192183,
            "prec": 0.5355191256830601,
            "rec": 0.6621621621621622,
            "f1": 0.5921450151057401
        },
        "popular (POPE)": {
            "acc": 0.6123778501628665,
            "prec": 0.589622641509434,
            "rec": 0.7961783439490446,
            "f1": 0.6775067750677508
        },
        "adversarial (POPE)": {
            "acc": 0.5874125874125874,
            "prec": 0.535,
            "rec": 0.8106060606060606,
            "f1": 0.644578313253012
        }
    },
    "OpenFlamingo-9B-vitl-mpt7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.2600559260055926
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.22647427854454202
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.272
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.24928483857785044
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.22693877551020408
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2827586206896552
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2361111111111111
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2140490390987409
        },
        "Instance Location (SEED_2)": {
            "acc": 0.28118609406952966
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.23711340206185566
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.28594046865104494
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2785363189513927
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.3037974683544304
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.19696969696969696
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2800608828006088
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22743682310469315
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.16715542521994134
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.33534743202416917
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.35
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3606060606060606
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2663316582914573
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.24489795918367346
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6764705882352942,
            "prec": 0.631578947368421,
            "rec": 0.8470588235294118,
            "f1": 0.7236180904522614
        },
        "posters (MME)": {
            "acc": 0.46938775510204084,
            "prec": 0.40816326530612246,
            "rec": 0.1360544217687075,
            "f1": 0.20408163265306123
        },
        "position (MME)": {
            "acc": 0.4,
            "prec": 0.4375,
            "rec": 0.7,
            "f1": 0.5384615384615384
        },
        "scene (MME)": {
            "acc": 0.475,
            "prec": 0.4358974358974359,
            "rec": 0.17,
            "f1": 0.2446043165467626
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.2714285714285714,
            "prec": 0.336734693877551,
            "rec": 0.4714285714285714,
            "f1": 0.39285714285714285
        },
        "artwork (MME)": {
            "acc": 0.555,
            "prec": 0.569620253164557,
            "rec": 0.45,
            "f1": 0.5027932960893855
        },
        "landmark (MME)": {
            "acc": 0.44,
            "prec": 0.18421052631578946,
            "rec": 0.035,
            "f1": 0.05882352941176471
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.4666666666666667,
            "prec": 0.45454545454545453,
            "rec": 0.3333333333333333,
            "f1": 0.3846153846153846
        },
        "numerical_calculation (MME)": {
            "acc": 0.2,
            "prec": 0.2692307692307692,
            "rec": 0.35,
            "f1": 0.3043478260869565
        },
        "count (MME)": {
            "acc": 0.43333333333333335,
            "prec": 0.46296296296296297,
            "rec": 0.8333333333333334,
            "f1": 0.5952380952380952
        },
        "color (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.4897959183673469,
            "rec": 0.8,
            "f1": 0.6075949367088609
        },
        "OCR (MME)": {
            "acc": 0.325,
            "prec": 0.3333333333333333,
            "rec": 0.35,
            "f1": 0.3414634146341463
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.38953488372093026
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.23809523809523808
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.0670391061452514
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.29484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.33080808080808083
        },
        "ocr (MMBench_CN)": {
            "acc": 0.28846153846153844
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.12994350282485875
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.25886524822695034
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.405
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.3256578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.5397727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.3915094339622642
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.375
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2785714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.26744186046511625
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2571428571428571
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3153846153846154
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2968036529680365
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.25139664804469275
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2186046511627907
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.31203931203931207
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.39141414141414144
        },
        "ocr (MMBench_EN)": {
            "acc": 0.25
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2768361581920904
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.24113475177304963
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.295
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.3355263157894737
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.5056818181818182
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.33962264150943394
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.3787878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.2
        },
        "Sociology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.4383561643835616
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.20512820512820512
        },
        "State capitals (ScienceQA)": {
            "acc": 0.46474358974358976
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.589041095890411
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.6020408163265306
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.14634146341463414
        },
        "Geography (ScienceQA)": {
            "acc": 0.09523809523809523
        },
        "Magnets (ScienceQA)": {
            "acc": 0.33636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3620689655172414
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.17777777777777778
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.41304347826086957
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.3090909090909091
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.4375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.10869565217391304
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.0784313725490196
        },
        "Classification (ScienceQA)": {
            "acc": 0.5443037974683544
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6551724137931034
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.16901408450704225
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.2766497461928934
        },
        "3D Distance (CVBench)": {
            "acc": 0.515
        },
        "2D Relation (CVBench)": {
            "acc": 0.49846153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.5016666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.965607662200927,
            "bert": 0.7924147701263428
        },
        "Whole dataset (Enrico)": {
            "bart": -7.538041839599609,
            "bert": 0.9038200908899308
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.49578644990921,
            "bert": 0.9958856511116028
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.8662132668495177,
            "bert": 0.8405998343229294
        },
        "Whole dataset (GQA)": {
            "bart": -7.091536793708801,
            "bert": 0.865244266986847
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.691388199329376,
            "bert": 0.9360306316614151
        },
        "Whole dataset (INAT)": {
            "bart": -6.2426616787910465,
            "bert": 0.8030664330720901
        },
        "Whole dataset (IRFL)": {
            "bart": -5.89665057182312,
            "bert": 0.7915611511468887
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8971096062660218,
            "bert": 0.8797921395301819
        },
        "Whole dataset (Memotion)": {
            "bart": -5.788459000587463,
            "bert": 0.8553056120872498
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.986261291503906,
            "bert": 0.7845381373167037
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.264794402122497,
            "bert": 0.8661734336614608
        },
        "Whole dataset (NLVR)": {
            "bart": -3.043530526161194,
            "bert": 0.999242896437645
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.9798789596557618,
            "bert": 0.9616902303695679
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.3244830131530763,
            "bert": 0.8539724278450013
        },
        "Whole dataset (OKVQA)": {
            "bart": -5.879545192718506,
            "bert": 0.823796131014824
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.346530038118362,
            "bert": 0.7872770309448243
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.8406484413146975,
            "bert": 0.8263426142930984
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.374550305604934,
            "bert": 0.8016560244560241
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.480599274635315,
            "bert": 0.8570564943552017
        },
        "Whole dataset (Slake)": {
            "bart": -4.186178251504898,
            "bert": 0.9730584686994552
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.019936463832855,
            "bert": 0.7614771121740341
        },
        "Whole dataset (VCR)": {
            "bart": -3.4788875913619997,
            "bert": 0.8585898023843765
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.763720849752426,
            "bert": 0.8865054327249527
        },
        "Whole dataset (VQA)": {
            "bart": -6.3068816614151,
            "bert": 0.8614128422737122
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.486355285644532,
            "bert": 0.8256763935089111
        },
        "Whole dataset (Winoground)": {
            "bart": -4.6854740750789645,
            "bert": 0.9474033844470978
        },
        "random (POPE)": {
            "acc": 0.5374592833876222,
            "prec": 0.5170454545454546,
            "rec": 0.6148648648648649,
            "f1": 0.5617283950617284
        },
        "popular (POPE)": {
            "acc": 0.5830618892508144,
            "prec": 0.5828571428571429,
            "rec": 0.6496815286624203,
            "f1": 0.6144578313253012
        },
        "adversarial (POPE)": {
            "acc": 0.5874125874125874,
            "prec": 0.5443037974683544,
            "rec": 0.6515151515151515,
            "f1": 0.5931034482758619
        }
    },
    "Qwen-VL-Chat": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.33663153366315335
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2315369261477046
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.32685069008782935
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.486
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.25827543931344504
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.25387755102040815
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.15178571428571427
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.31809145129224653
        },
        "Instance Location (SEED_2)": {
            "acc": 0.29754601226993865
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.32989690721649484
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.33407219759341356
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.3063899508465319
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.7215189873417721
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.1893939393939394
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.18562874251497005
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2968036529680365
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2815884476534296
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.23362658846529813
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.36555891238670696
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.23333333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5060606060606061
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.4271356783919598
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2653061224489796
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.008823529411764706,
            "prec": 0.011695906432748537,
            "rec": 0.011764705882352941,
            "f1": 0.011730205278592375
        },
        "posters (MME)": {
            "acc": 0.1598639455782313,
            "prec": 0.17532467532467533,
            "rec": 0.1836734693877551,
            "f1": 0.17940199335548174
        },
        "position (MME)": {
            "acc": 0.03333333333333333,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "scene (MME)": {
            "acc": 0.02,
            "prec": 0.029411764705882353,
            "rec": 0.03,
            "f1": 0.0297029702970297
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.03571428571428571,
            "prec": 0.06666666666666667,
            "rec": 0.07142857142857142,
            "f1": 0.0689655172413793
        },
        "artwork (MME)": {
            "acc": 0.02,
            "prec": 0.02,
            "rec": 0.02,
            "f1": 0.02
        },
        "landmark (MME)": {
            "acc": 0.1375,
            "prec": 0.20883534136546184,
            "rec": 0.26,
            "f1": 0.23162583518930957
        },
        "text_translation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.016666666666666666,
            "prec": 0.03225806451612903,
            "rec": 0.03333333333333333,
            "f1": 0.03278688524590164
        },
        "numerical_calculation (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.016666666666666666,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "color (MME)": {
            "acc": 0.05,
            "prec": 0.034482758620689655,
            "rec": 0.03333333333333333,
            "f1": 0.03389830508474576
        },
        "OCR (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "code_reasoning (MME)": {
            "acc": 0.075,
            "prec": 0.13043478260869565,
            "rec": 0.15,
            "f1": 0.13953488372093023
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7034883720930233
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.3047619047619048
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2374429223744292
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.36879432624113473
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.44692737430167595
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.7255813953488373
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.5307125307125307
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.5505050505050505
        },
        "ocr (MMBench_CN)": {
            "acc": 0.36538461538461536
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2978723404255319
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.755
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.5427631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.7897727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_CN)": {
            "acc": 0.5188679245283019
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.4962121212121212
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.2866666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.42142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.4418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.29523809523809524
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.43846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2648401826484018
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3120567375886525
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4022346368715084
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.31627906976744186
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.4619164619164619
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.6287878787878788
        },
        "ocr (MMBench_EN)": {
            "acc": 0.26282051282051283
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.24822695035460993
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.59
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.6118421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.6818181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "image_style (MMBench_EN)": {
            "acc": 0.5047169811320755
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.4583333333333333
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.37857142857142856
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.0
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.2
        },
        "Sociology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.0
        },
        "Materials (MMMU)": {
            "acc": 0.0
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.0
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.0
        },
        "Biology (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.0
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1557377049180328
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.0873015873015873
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15196078431372548
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.2602739726027397
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.4391025641025641
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.4520547945205479
        },
        "States of matter (ScienceQA)": {
            "acc": 0.25
        },
        "Materials (ScienceQA)": {
            "acc": 0.4755244755244755
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.5510204081632653
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.2903225806451613
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3170731707317073
        },
        "Geography (ScienceQA)": {
            "acc": 0.19047619047619047
        },
        "Magnets (ScienceQA)": {
            "acc": 0.35454545454545455
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.25862068965517243
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.46875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.24074074074074073
        },
        "Maps (ScienceQA)": {
            "acc": 0.08695652173913043
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Classification (ScienceQA)": {
            "acc": 0.3291139240506329
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.3401015228426396
        },
        "3D Distance (CVBench)": {
            "acc": 0.165
        },
        "2D Relation (CVBench)": {
            "acc": 0.3476923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.30833333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.160097823143006,
            "bert": 0.7575552451610565
        },
        "Whole dataset (Enrico)": {
            "bart": -5.764184348583221,
            "bert": 0.9252673500776291
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -7.215224306583405,
            "bert": 0.9624762761592865
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.756361873149872,
            "bert": 0.8878017407655716
        },
        "Whole dataset (GQA)": {
            "bart": -4.9507285392284395,
            "bert": 0.9161708587408066
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.308042529821396,
            "bert": 0.9787142348289489
        },
        "Whole dataset (INAT)": {
            "bart": -6.951835423707962,
            "bert": 0.42121804058551787
        },
        "Whole dataset (IRFL)": {
            "bart": -5.072778202295304,
            "bert": 0.8726426130533218
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.313439540863037,
            "bert": 0.8516648006439209
        },
        "Whole dataset (Memotion)": {
            "bart": -4.703055727481842,
            "bert": 0.8255952805280685
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.265541288852692,
            "bert": 0.7948657166957855
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.85007390499115,
            "bert": 0.7983354192972183
        },
        "Whole dataset (NLVR)": {
            "bart": -5.918561792373657,
            "bert": 0.8220213103294373
        },
        "Whole dataset (NLVR2)": {
            "bart": -6.093962378501892,
            "bert": 0.8396991604566574
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.0208882784843443,
            "bert": 0.886722554564476
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.571008051633835,
            "bert": 0.8050669211149216
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.551495903730393,
            "bert": 0.774763000011444
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.256879712343216,
            "bert": 0.7990639621019363
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.926763552427292,
            "bert": 0.7883186620473862
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.348452043533325,
            "bert": 0.8142099606990815
        },
        "Whole dataset (Slake)": {
            "bart": -6.821511464118958,
            "bert": 0.8240603244304657
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.099891760349274,
            "bert": 0.7786705911159515
        },
        "Whole dataset (VCR)": {
            "bart": -2.7140033721923826,
            "bert": 0.8276972842216491
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.001541360020638,
            "bert": 0.8716356730461121
        },
        "Whole dataset (VQA)": {
            "bart": -6.14225713968277,
            "bert": 0.7952952003479004
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.440918192863465,
            "bert": 0.9031009775400162
        },
        "Whole dataset (Winoground)": {
            "bart": -6.06256713271141,
            "bert": 0.9583153408765793
        },
        "random (POPE)": {
            "acc": 0.10423452768729642,
            "prec": 0.055944055944055944,
            "rec": 0.05405405405405406,
            "f1": 0.05498281786941581
        },
        "popular (POPE)": {
            "acc": 0.14006514657980457,
            "prec": 0.08527131782945736,
            "rec": 0.07006369426751592,
            "f1": 0.07692307692307693
        },
        "adversarial (POPE)": {
            "acc": 0.12237762237762238,
            "prec": 0.06569343065693431,
            "rec": 0.06818181818181818,
            "f1": 0.06691449814126393
        }
    },
    "InternLM-XComposer-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.4721445472144547
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5968063872255489
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.4002509410288582
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.36127744510978044
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.49
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4572946465059256
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.4097959183673469
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5103448275862069
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.4017857142857143
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4552683896620278
        },
        "Instance Location (SEED_2)": {
            "acc": 0.47648261758691207
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7422680412371134
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.49493350221659277
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.4227198252321136
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8734177215189873
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.30303030303030304
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5369261477045908
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.410958904109589
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.34657039711191334
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.26588465298142716
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.5891238670694864
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.3
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6121212121212121
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5728643216080402
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2716049382716049
        },
        "celebrity (MME)": {
            "acc": 0.75,
            "prec": 0.6923076923076923,
            "rec": 0.9,
            "f1": 0.7826086956521738
        },
        "posters (MME)": {
            "acc": 0.7380952380952381,
            "prec": 0.8645833333333334,
            "rec": 0.564625850340136,
            "f1": 0.6831275720164608
        },
        "position (MME)": {
            "acc": 0.48333333333333334,
            "prec": 0.45454545454545453,
            "rec": 0.16666666666666666,
            "f1": 0.2439024390243902
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9736842105263158,
            "rec": 0.74,
            "f1": 0.8409090909090909
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6142857142857143,
            "prec": 0.8076923076923077,
            "rec": 0.3,
            "f1": 0.4375
        },
        "artwork (MME)": {
            "acc": 0.7725,
            "prec": 0.8187134502923976,
            "rec": 0.7,
            "f1": 0.7547169811320755
        },
        "landmark (MME)": {
            "acc": 0.88,
            "prec": 0.8689320388349514,
            "rec": 0.895,
            "f1": 0.8817733990147784
        },
        "text_translation (MME)": {
            "acc": 0.65,
            "prec": 0.8,
            "rec": 0.4,
            "f1": 0.5333333333333333
        },
        "existence (MME)": {
            "acc": 0.85,
            "prec": 1.0,
            "rec": 0.7,
            "f1": 0.8235294117647058
        },
        "numerical_calculation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.8636363636363636,
            "rec": 0.6333333333333333,
            "f1": 0.7307692307692307
        },
        "color (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7575757575757576,
            "rec": 0.8333333333333334,
            "f1": 0.7936507936507938
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6451612903225806,
            "rec": 1.0,
            "f1": 0.7843137254901961
        },
        "code_reasoning (MME)": {
            "acc": 0.375,
            "prec": 0.2727272727272727,
            "rec": 0.15,
            "f1": 0.19354838709677416
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8662790697674418
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.4666666666666667
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4931506849315068
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2695035460992908
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6871508379888268
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.9343434343434344
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5961538461538461
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.46099290780141844
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8848684210526315
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8915094339622641
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7424242424242424
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.34
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.7325581395348837
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.48253968253968255
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4794520547945205
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5319148936170213
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.40782122905027934
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.6046511627906976
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.6904176904176904
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8156565656565656
        },
        "ocr (MMBench_EN)": {
            "acc": 0.46794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.450354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.675
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8125
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8443396226415094
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.7651515151515151
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.3466666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.6071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.1
        },
        "Sociology (MMMU)": {
            "acc": 0.2
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.1
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.1
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.1
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.3360655737704918
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3006535947712418
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.43529411764705883
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5227272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "State capitals (ScienceQA)": {
            "acc": 0.6378205128205128
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9452054794520548
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.8181818181818182
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7959183673469388
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.1951219512195122
        },
        "Geography (ScienceQA)": {
            "acc": 0.40476190476190477
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.16666666666666666
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Classification (ScienceQA)": {
            "acc": 0.7341772151898734
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "2D Count (CVBench)": {
            "acc": 0.46954314720812185
        },
        "3D Distance (CVBench)": {
            "acc": 0.5366666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.5523076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.54
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.842830510139465,
            "bert": 0.7925996118783951
        },
        "Whole dataset (Enrico)": {
            "bart": -5.819122474193573,
            "bert": 0.9831367093324661
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.396881085634232,
            "bert": 0.9993129342794418
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.064762442111969,
            "bert": 0.8302531200647354
        },
        "Whole dataset (GQA)": {
            "bart": -4.7867369389534,
            "bert": 0.9773205155134201
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.33546317577362,
            "bert": 0.8677501124143601
        },
        "Whole dataset (INAT)": {
            "bart": -6.307193059921264,
            "bert": 0.7818501305580139
        },
        "Whole dataset (IRFL)": {
            "bart": -4.53485926747322,
            "bert": 0.9890018504858017
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8106550776958468,
            "bert": 0.86707534968853
        },
        "Whole dataset (Memotion)": {
            "bart": -4.460830426216125,
            "bert": 0.8997438883781433
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7296432024240493,
            "bert": 0.9027275890111923
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.332496241331101,
            "bert": 0.9494181215763092
        },
        "Whole dataset (NLVR)": {
            "bart": -2.9999997806549072,
            "bert": 0.9990297627449035
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.221329622268677,
            "bert": 0.9992284440994262
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5370044028759002,
            "bert": 0.8496037012338639
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.96777435362339,
            "bert": 0.9281119281053543
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.054925536513329,
            "bert": 0.8237725877761841
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.252971519231796,
            "bert": 0.8937002623081207
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.879330434799194,
            "bert": 0.8760391801595688
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.933777050971985,
            "bert": 0.8589136850833893
        },
        "Whole dataset (Slake)": {
            "bart": -3.7370961654186248,
            "bert": 0.9854151672124862
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.429000525474549,
            "bert": 0.9374940818548203
        },
        "Whole dataset (VCR)": {
            "bart": -3.597026356458664,
            "bert": 0.9165203273296356
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.65039787709713,
            "bert": 0.9153808999061585
        },
        "Whole dataset (VQA)": {
            "bart": -5.246062124967575,
            "bert": 0.9592921364307404
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.057457770109177,
            "bert": 0.925859723687172
        },
        "Whole dataset (Winoground)": {
            "bart": -4.978392264842987,
            "bert": 0.997784647345543
        },
        "random (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.8818897637795275,
            "rec": 0.7567567567567568,
            "f1": 0.8145454545454547
        },
        "popular (POPE)": {
            "acc": 0.7752442996742671,
            "prec": 0.782051282051282,
            "rec": 0.7770700636942676,
            "f1": 0.7795527156549521
        },
        "adversarial (POPE)": {
            "acc": 0.7587412587412588,
            "prec": 0.7032258064516129,
            "rec": 0.8257575757575758,
            "f1": 0.759581881533101
        }
    },
    "InternLM-XComposer-vl-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.600559260055926
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.1596806387225549
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.33877038895859474
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2834331337325349
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.696
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5312627707396812
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3069387755102041
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4045977011494253
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.17956349206349206
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.32803180914512925
        },
        "Instance Location (SEED_2)": {
            "acc": 0.4754601226993865
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5051546391752577
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.45218492716909436
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5778263244128892
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.7974683544303798
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.19696969696969696
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.16766467065868262
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4429223744292237
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.7328519855595668
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.03128054740957967
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.4471299093655589
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.10833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5181818181818182
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5879396984924623
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.12244897959183673
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.1111111111111111
        },
        "celebrity (MME)": {
            "acc": 0.8235294117647058,
            "prec": 0.7669902912621359,
            "rec": 0.9294117647058824,
            "f1": 0.8404255319148938
        },
        "posters (MME)": {
            "acc": 0.8401360544217688,
            "prec": 0.8378378378378378,
            "rec": 0.8435374149659864,
            "f1": 0.840677966101695
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6764705882352942,
            "rec": 0.7666666666666667,
            "f1": 0.71875
        },
        "scene (MME)": {
            "acc": 0.885,
            "prec": 0.963855421686747,
            "rec": 0.8,
            "f1": 0.8743169398907104
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7714285714285715,
            "prec": 0.7794117647058824,
            "rec": 0.7571428571428571,
            "f1": 0.7681159420289856
        },
        "artwork (MME)": {
            "acc": 0.7325,
            "prec": 0.7123287671232876,
            "rec": 0.78,
            "f1": 0.7446300715990454
        },
        "landmark (MME)": {
            "acc": 0.8325,
            "prec": 0.8341708542713567,
            "rec": 0.83,
            "f1": 0.8320802005012531
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.5769230769230769,
            "rec": 0.75,
            "f1": 0.6521739130434783
        },
        "existence (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.3684210526315789,
            "rec": 0.35,
            "f1": 0.358974358974359
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6451612903225806,
            "rec": 1.0,
            "f1": 0.7843137254901961
        },
        "code_reasoning (MME)": {
            "acc": 0.575,
            "prec": 0.5789473684210527,
            "rec": 0.55,
            "f1": 0.5641025641025641
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7790697674418605
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5301587301587302
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5159817351598174
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.3900709219858156
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4134078212290503
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8558139534883721
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.8820638820638821
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8636363636363636
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5064102564102564
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4350282485875706
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4219858156028369
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.75
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8519736842105263
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.7784090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.40425531914893614
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7783018867924528
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7159090909090909
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.6666666666666666
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.5348837209302325
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6603174603174603
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.3
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4794520547945205
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5531914893617021
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22346368715083798
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.4744186046511628
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.5823095823095823
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7727272727272727
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5384615384615384
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4432624113475177
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.465
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.7730263157894737
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.5397727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.6132075471698113
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.7424242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.68
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.7214285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.1
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.4
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.3
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.28688524590163933
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3137254901960784
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.32941176470588235
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.25
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.684931506849315
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.9230769230769231
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.684931506849315
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.7552447552447552
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9183673469387755
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1774193548387097
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.3
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.8611111111111112
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.9215686274509803
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.8888888888888888
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.7454545454545455
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.8717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2037037037037037
        },
        "Maps (ScienceQA)": {
            "acc": 0.8260869565217391
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.9215686274509803
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.9736842105263158
        },
        "2D Count (CVBench)": {
            "acc": 0.5431472081218274
        },
        "3D Distance (CVBench)": {
            "acc": 0.5083333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6184615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.515
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.79225394487381,
            "bert": 0.8240817934274673
        },
        "Whole dataset (Enrico)": {
            "bart": -6.22485823392868,
            "bert": 0.9502826797962188
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.5867255544662475,
            "bert": 0.9358293408155441
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2664905333518983,
            "bert": 0.8881587988138199
        },
        "Whole dataset (GQA)": {
            "bart": -5.057641520500183,
            "bert": 0.9450112515687943
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.740291035175323,
            "bert": 0.9498932212591171
        },
        "Whole dataset (INAT)": {
            "bart": -7.149572007656097,
            "bert": 0.7978765225410461
        },
        "Whole dataset (IRFL)": {
            "bart": -5.373598158359528,
            "bert": 0.9444566518068314
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.121512334346772,
            "bert": 0.8492291164398194
        },
        "Whole dataset (Memotion)": {
            "bart": -4.900471405982971,
            "bert": 0.8896581494808197
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9155987226963043,
            "bert": 0.8768908458948136
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.612424063682556,
            "bert": 0.951125962138176
        },
        "Whole dataset (NLVR)": {
            "bart": -5.1037641263008116,
            "bert": 0.947580731511116
        },
        "Whole dataset (NLVR2)": {
            "bart": -5.025722098350525,
            "bert": 0.9479458844661712
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.2877869191765785,
            "bert": 0.9196412825584411
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.489763884544373,
            "bert": 0.934100980758667
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.171575952768325,
            "bert": 0.8197659385204316
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.007683455944061,
            "bert": 0.8818188148736954
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.619721301794052,
            "bert": 0.8982467061281204
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.2865709829330445,
            "bert": 0.852831376194954
        },
        "Whole dataset (Slake)": {
            "bart": -4.889273521900177,
            "bert": 0.9433544427156448
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.647009115219117,
            "bert": 0.9032222563028336
        },
        "Whole dataset (VCR)": {
            "bart": -3.423182701468468,
            "bert": 0.9231251603364945
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.393608709573746,
            "bert": 0.9251636832952499
        },
        "Whole dataset (VQA)": {
            "bart": -5.515536458492279,
            "bert": 0.9355155462026596
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.355203227996826,
            "bert": 0.9106510633230209
        },
        "Whole dataset (Winoground)": {
            "bart": -5.400664186477661,
            "bert": 0.9440612453222275
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.8333333333333334,
            "rec": 0.9121621621621622,
            "f1": 0.8709677419354839
        },
        "popular (POPE)": {
            "acc": 0.7947882736156352,
            "prec": 0.7611111111111111,
            "rec": 0.8726114649681529,
            "f1": 0.8130563798219584
        },
        "adversarial (POPE)": {
            "acc": 0.8181818181818182,
            "prec": 0.7352941176470589,
            "rec": 0.946969696969697,
            "f1": 0.827814569536424
        }
    },
    "InternLM-XComposer2-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5751774575177457
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6027944111776448
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.4504391468005019
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.3373253493013972
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.742
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5884756845116469
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.39346938775510204
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.593103448275862
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2400793650793651
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4227965540092777
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5766871165644172
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6874604179860672
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5723648279628618
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8987341772151899
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3560606060606061
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.46107784431137727
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5098934550989346
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5631768953068592
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34408602150537637
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.375
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.49056603773584906
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6727272727272727
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6130653266331658
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.7823529411764706,
            "prec": 0.75,
            "rec": 0.8470588235294118,
            "f1": 0.7955801104972374
        },
        "posters (MME)": {
            "acc": 0.8877551020408163,
            "prec": 0.88,
            "rec": 0.8979591836734694,
            "f1": 0.888888888888889
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.8,
            "rec": 0.4,
            "f1": 0.5333333333333333
        },
        "scene (MME)": {
            "acc": 0.84,
            "prec": 0.9857142857142858,
            "rec": 0.69,
            "f1": 0.8117647058823529
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7071428571428572,
            "prec": 0.7457627118644068,
            "rec": 0.6285714285714286,
            "f1": 0.6821705426356589
        },
        "artwork (MME)": {
            "acc": 0.8725,
            "prec": 0.8860103626943006,
            "rec": 0.855,
            "f1": 0.8702290076335878
        },
        "landmark (MME)": {
            "acc": 0.75,
            "prec": 0.9629629629629629,
            "rec": 0.52,
            "f1": 0.6753246753246753
        },
        "text_translation (MME)": {
            "acc": 0.725,
            "prec": 0.6956521739130435,
            "rec": 0.8,
            "f1": 0.7441860465116279
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5833333333333334,
            "rec": 0.35,
            "f1": 0.4375
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.8888888888888888,
            "rec": 0.8,
            "f1": 0.8421052631578948
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.9,
            "rec": 0.9,
            "f1": 0.9
        },
        "OCR (MME)": {
            "acc": 0.85,
            "prec": 0.7916666666666666,
            "rec": 0.95,
            "f1": 0.8636363636363635
        },
        "code_reasoning (MME)": {
            "acc": 0.55,
            "prec": 0.625,
            "rec": 0.25,
            "f1": 0.35714285714285715
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.653968253968254
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5981735159817352
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.524822695035461
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.659217877094972
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8976744186046511
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9459459459459459
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8964646464646465
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5886524822695035
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8848684210526315
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6276595744680851
        },
        "image_style (MMBench_CN)": {
            "acc": 0.9009433962264151
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8446969696969697
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.6333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.7790697674418605
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6476190476190476
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5538461538461539
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4794520547945205
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5390070921985816
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8883720930232558
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9459459459459459
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8207070707070707
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5512820512820513
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.75
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9144736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6702127659574468
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9339622641509434
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8598484848484849
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5733333333333334
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.1
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.1
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.3
        },
        "Accounting (MMMU)": {
            "acc": 0.1
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.2
        },
        "Literature (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.26666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.32786885245901637
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.19047619047619047
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.38562091503267976
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.43529411764705883
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6704545454545454
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6986301369863014
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.7948717948717948
        },
        "State capitals (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.8881118881118881
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.7419354838709677
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.7272727272727273
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6388888888888888
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.8823529411764706
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.8205128205128205
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.71875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Classification (ScienceQA)": {
            "acc": 0.9620253164556962
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6056338028169014
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.8157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.5723350253807107
        },
        "3D Distance (CVBench)": {
            "acc": 0.6016666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.7584615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.6216666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.456574728488922,
            "bert": 0.8626784783601761
        },
        "Whole dataset (Enrico)": {
            "bart": -5.566638678312302,
            "bert": 0.9854504561424255
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.720981304645538,
            "bert": 0.9906596851348877
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.28148891210556,
            "bert": 0.7636071014404296
        },
        "Whole dataset (GQA)": {
            "bart": -4.585099865198135,
            "bert": 0.9920630353689194
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.63848726272583,
            "bert": 0.8185752487182617
        },
        "Whole dataset (INAT)": {
            "bart": -6.308098356723786,
            "bert": 0.7756268310546875
        },
        "Whole dataset (IRFL)": {
            "bart": -4.194415458440781,
            "bert": 0.9986742651462555
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.3611889350414277,
            "bert": 0.8782286912202835
        },
        "Whole dataset (Memotion)": {
            "bart": -4.777610039710998,
            "bert": 0.9024033010005951
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.293322894573212,
            "bert": 0.8975129449367523
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.894648776054383,
            "bert": 0.8504341781139374
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6243007659912108,
            "bert": 0.9994762909412384
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.436070580482483,
            "bert": 0.9995555061101914
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.845787878036499,
            "bert": 0.7734918695688248
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.8054055923223498,
            "bert": 0.9437958914041519
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.856670721769333,
            "bert": 0.853660770058632
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.924808406829834,
            "bert": 0.9091936504840851
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.83029734492302,
            "bert": 0.9319131296873092
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.604976649284363,
            "bert": 0.8535946893692017
        },
        "Whole dataset (Slake)": {
            "bart": -3.7047433006763457,
            "bert": 0.9947788423299789
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.342519779205322,
            "bert": 0.8077218818664551
        },
        "Whole dataset (VCR)": {
            "bart": -3.0794520771503446,
            "bert": 0.9316789162158966
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5571075969934465,
            "bert": 0.9023997980356216
        },
        "Whole dataset (VQA)": {
            "bart": -5.049176881313324,
            "bert": 0.9653323662281036
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.115017958879471,
            "bert": 0.9308823394775391
        },
        "Whole dataset (Winoground)": {
            "bart": -4.310589517354965,
            "bert": 0.9979545223712921
        },
        "random (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.9482758620689655,
            "rec": 0.7432432432432432,
            "f1": 0.8333333333333333
        },
        "popular (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.9140625,
            "rec": 0.7452229299363057,
            "f1": 0.8210526315789474
        },
        "adversarial (POPE)": {
            "acc": 0.8776223776223776,
            "prec": 0.9369369369369369,
            "rec": 0.7878787878787878,
            "f1": 0.8559670781893004
        }
    },
    "InternLM-XComposer2-vl-1_8b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6904710690471069
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.592814371257485
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5169385194479298
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.44510978043912175
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.782
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6093175316714344
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3681632653061224
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5586206896551724
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2857142857142857
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4049039098740888
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6206543967280164
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7311589613679544
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.693610049153468
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8987341772151899
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.4471057884231537
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.512937595129376
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.8194945848375451
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.386119257086999
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7250755287009063
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2833333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.3836477987421384
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7151515151515152
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5728643216080402
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.19753086419753085
        },
        "celebrity (MME)": {
            "acc": 0.788235294117647,
            "prec": 0.8266666666666667,
            "rec": 0.7294117647058823,
            "f1": 0.7749999999999999
        },
        "posters (MME)": {
            "acc": 0.8061224489795918,
            "prec": 0.8813559322033898,
            "rec": 0.7074829931972789,
            "f1": 0.7849056603773585
        },
        "position (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.84375,
            "rec": 0.9,
            "f1": 0.870967741935484
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.9617834394904459,
            "rec": 0.755,
            "f1": 0.8459383753501402
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.7,
            "rec": 0.6,
            "f1": 0.6461538461538462
        },
        "artwork (MME)": {
            "acc": 0.8975,
            "prec": 0.9877300613496932,
            "rec": 0.805,
            "f1": 0.8870523415977961
        },
        "landmark (MME)": {
            "acc": 0.74,
            "prec": 0.9528301886792453,
            "rec": 0.505,
            "f1": 0.6601307189542484
        },
        "text_translation (MME)": {
            "acc": 0.85,
            "prec": 0.8888888888888888,
            "rec": 0.8,
            "f1": 0.8421052631578948
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.1,
            "f1": 0.16666666666666669
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.875,
            "rec": 0.9333333333333333,
            "f1": 0.9032258064516129
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6551724137931034,
            "rec": 0.95,
            "f1": 0.7755102040816326
        },
        "code_reasoning (MME)": {
            "acc": 0.575,
            "prec": 0.8,
            "rec": 0.2,
            "f1": 0.32000000000000006
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6285714285714286
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5538461538461539
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5296803652968036
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.3617021276595745
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5754189944134078
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9434889434889435
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.8383838383838383
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6987179487179487
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5035460992907801
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.815
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7927631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6914893617021277
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7688679245283019
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7916666666666666
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.56
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9285714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9011627906976745
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6444444444444445
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6757990867579908
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6950354609929078
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8604651162790697
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8207070707070707
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3615819209039548
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5780141843971631
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.8085106382978723
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7264150943396226
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8939393939393939
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.6533333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.0
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.0
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.0
        },
        "Sociology (MMMU)": {
            "acc": 0.1
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.1
        },
        "Materials (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.0
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.1
        },
        "Art (MMMU)": {
            "acc": 0.1
        },
        "Accounting (MMMU)": {
            "acc": 0.0
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.0
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.0
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.1
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.0
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.3114754098360656
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.19047619047619047
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.32941176470588235
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2647058823529412
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5113636363636364
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8356164383561644
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 1.0
        },
        "State capitals (ScienceQA)": {
            "acc": 1.0
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.9370629370629371
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 1.0
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.975609756097561
        },
        "Geography (ScienceQA)": {
            "acc": 0.8809523809523809
        },
        "Magnets (ScienceQA)": {
            "acc": 0.9090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.9722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 1.0
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.9137931034482759
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 1.0
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 1.0
        },
        "Solutions (ScienceQA)": {
            "acc": 0.8148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.9803921568627451
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.9310344827586207
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 1.0
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.9736842105263158
        },
        "2D Count (CVBench)": {
            "acc": 0.6687817258883249
        },
        "3D Distance (CVBench)": {
            "acc": 0.5883333333333334
        },
        "2D Relation (CVBench)": {
            "acc": 0.6307692307692307
        },
        "3D Depth (CVBench)": {
            "acc": 0.6933333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.062904300689698,
            "bert": 0.7987588655948639
        },
        "Whole dataset (Enrico)": {
            "bart": -6.4954256916046145,
            "bert": 0.9844517874717712
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.373112334012985,
            "bert": 0.9993260592222214
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3043948364257814,
            "bert": 0.9072219240665436
        },
        "Whole dataset (GQA)": {
            "bart": -4.194385100603103,
            "bert": 0.9766138076782227
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.936356304883957,
            "bert": 0.9958570605516434
        },
        "Whole dataset (INAT)": {
            "bart": -6.229717679023743,
            "bert": 0.7808357673883438
        },
        "Whole dataset (IRFL)": {
            "bart": -4.256104632616043,
            "bert": 0.9969739812612534
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.80145409822464,
            "bert": 0.8800503468513489
        },
        "Whole dataset (Memotion)": {
            "bart": -4.594109346866608,
            "bert": 0.8999902522563934
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.801091042160988,
            "bert": 0.8873124980926513
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.483553901910782,
            "bert": 0.9972404110431671
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5881439542770384,
            "bert": 0.9995455890893936
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.436070580482483,
            "bert": 0.9996012204885483
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.230626045465469,
            "bert": 0.9379092913866043
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.954273926615715,
            "bert": 0.9313252002000809
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.229996088147163,
            "bert": 0.8140296405553817
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.167782547473908,
            "bert": 0.9032747453451156
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.78197414278984,
            "bert": 0.9138738358020783
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.879079957008361,
            "bert": 0.8594157600402832
        },
        "Whole dataset (Slake)": {
            "bart": -3.9276327764987946,
            "bert": 0.9853260946273804
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.287749347686767,
            "bert": 0.9172150927782059
        },
        "Whole dataset (VCR)": {
            "bart": -3.733041205406189,
            "bert": 0.911612463593483
        },
        "Whole dataset (VisualGenome)": {
            "bart": -2.9502748873829843,
            "bert": 0.9429499846696854
        },
        "Whole dataset (VQA)": {
            "bart": -4.939565260410308,
            "bert": 0.9407226270437241
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.964458427429199,
            "bert": 0.9405940014123917
        },
        "Whole dataset (Winoground)": {
            "bart": -4.633281193971634,
            "bert": 0.9973363506793976
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 1.0,
            "rec": 0.7364864864864865,
            "f1": 0.8482490272373542
        },
        "popular (POPE)": {
            "acc": 0.8664495114006515,
            "prec": 0.953125,
            "rec": 0.7770700636942676,
            "f1": 0.856140350877193
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9814814814814815,
            "rec": 0.803030303030303,
            "f1": 0.8833333333333332
        }
    },
    "InternLM-XComposer2-vl-7B": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7752204775220477
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.626746506986028
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.534504391468005
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.5808383233532934
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.87
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6898242746219861
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.433469387755102
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5701149425287356
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.4166666666666667
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.44201457919151754
        },
        "Instance Location (SEED_2)": {
            "acc": 0.7044989775051125
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7835051546391752
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7039265357821406
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.744401966138722
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.688622754491018
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.6118721461187214
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.7978339350180506
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3519061583577713
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7311178247734139
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.325
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4779874213836478
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8666666666666667
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6683417085427136
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.8352941176470589,
            "prec": 0.7714285714285715,
            "rec": 0.9529411764705882,
            "f1": 0.8526315789473684
        },
        "posters (MME)": {
            "acc": 0.8945578231292517,
            "prec": 0.8580246913580247,
            "rec": 0.9455782312925171,
            "f1": 0.8996763754045307
        },
        "position (MME)": {
            "acc": 0.8,
            "prec": 0.7647058823529411,
            "rec": 0.8666666666666667,
            "f1": 0.8125
        },
        "scene (MME)": {
            "acc": 0.8675,
            "prec": 0.9152542372881356,
            "rec": 0.81,
            "f1": 0.8594164456233423
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.8142857142857143,
            "prec": 0.8142857142857143,
            "rec": 0.8142857142857143,
            "f1": 0.8142857142857143
        },
        "artwork (MME)": {
            "acc": 0.95,
            "prec": 0.9545454545454546,
            "rec": 0.945,
            "f1": 0.949748743718593
        },
        "landmark (MME)": {
            "acc": 0.91,
            "prec": 0.9019607843137255,
            "rec": 0.92,
            "f1": 0.9108910891089109
        },
        "text_translation (MME)": {
            "acc": 0.8,
            "prec": 0.75,
            "rec": 0.9,
            "f1": 0.8181818181818182
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9354838709677419,
            "rec": 0.9666666666666667,
            "f1": 0.9508196721311476
        },
        "numerical_calculation (MME)": {
            "acc": 0.725,
            "prec": 0.7647058823529411,
            "rec": 0.65,
            "f1": 0.7027027027027027
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9375,
            "rec": 1.0,
            "f1": 0.967741935483871
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.6896551724137931,
            "rec": 1.0,
            "f1": 0.8163265306122449
        },
        "code_reasoning (MME)": {
            "acc": 0.675,
            "prec": 0.7333333333333333,
            "rec": 0.55,
            "f1": 0.6285714285714286
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.7174603174603175
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6461538461538462
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.6073059360730594
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.6871508379888268
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8744186046511628
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.9040404040404041
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5705128205128205
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.6843971631205674
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8618421052631579
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6702127659574468
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8584905660377359
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.9015151515151515
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.6066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9428571428571428
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8372093023255814
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.746031746031746
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6538461538461539
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6210045662100456
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6950354609929078
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7150837988826816
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.7441860465116279
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.7002457002457002
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8535353535353535
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6538461538461539
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4745762711864407
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.6843971631205674
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.725
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8782894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.723404255319149
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9150943396226415
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.64
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.6785714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.13333333333333333
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.3360655737704918
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.41830065359477125
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.4470588235294118
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2647058823529412
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5909090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.9041095890410958
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.9743589743589743
        },
        "State capitals (ScienceQA)": {
            "acc": 0.8589743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 1.0
        },
        "Materials (ScienceQA)": {
            "acc": 0.958041958041958
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9387755102040817
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.967741935483871
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.6829268292682927
        },
        "Geography (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Magnets (ScienceQA)": {
            "acc": 1.0
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.9166666666666666
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.9607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.8793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 1.0
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Maps (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.7105263157894737
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.8627450980392157
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 1.0
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6056338028169014
        },
        "Colonial America (ScienceQA)": {
            "acc": 1.0
        },
        "2D Count (CVBench)": {
            "acc": 0.6776649746192893
        },
        "3D Distance (CVBench)": {
            "acc": 0.695
        },
        "2D Relation (CVBench)": {
            "acc": 0.8061538461538461
        },
        "3D Depth (CVBench)": {
            "acc": 0.7766666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.6462216114997865,
            "bert": 0.8551745879650116
        },
        "Whole dataset (Enrico)": {
            "bart": -5.781115255355835,
            "bert": 0.9824086421728134
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.8585878229141235,
            "bert": 0.9976230651140213
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.153054721355438,
            "bert": 0.8897846293449402
        },
        "Whole dataset (GQA)": {
            "bart": -3.6124112677574156,
            "bert": 0.9969710409641266
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.2334981274604795,
            "bert": 0.9510270929336548
        },
        "Whole dataset (INAT)": {
            "bart": -6.392794619798661,
            "bert": 0.7863097500801086
        },
        "Whole dataset (IRFL)": {
            "bart": -4.032175513505936,
            "bert": 0.9969238603115081
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.5469929802417757,
            "bert": 0.8662655210494995
        },
        "Whole dataset (Memotion)": {
            "bart": -4.579670367240905,
            "bert": 0.901682865023613
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.325507370829582,
            "bert": 0.8989180314540863
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.483718398809433,
            "bert": 0.9983532762527466
        },
        "Whole dataset (NLVR)": {
            "bart": -2.9808520889282226,
            "bert": 0.9996847069263458
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.8125437927246093,
            "bert": 0.999712952375412
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.255313565731049,
            "bert": 0.8921907287836075
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.8550169181823732,
            "bert": 0.9758991408348083
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.847682340741158,
            "bert": 0.8535932403802872
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.982076798677444,
            "bert": 0.9125970333814621
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.368863337039947,
            "bert": 0.9419420605897904
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.7726871252059935,
            "bert": 0.856824666261673
        },
        "Whole dataset (Slake)": {
            "bart": -3.510659204721451,
            "bert": 0.9939649879932404
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.944214957952499,
            "bert": 0.9341679722070694
        },
        "Whole dataset (VCR)": {
            "bart": -2.985742303133011,
            "bert": 0.9351618474721909
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.402691726088524,
            "bert": 0.9166103416681289
        },
        "Whole dataset (VQA)": {
            "bart": -4.013770418167114,
            "bert": 0.9799607223272324
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.902027916908264,
            "bert": 0.9407885760068894
        },
        "Whole dataset (Winoground)": {
            "bart": -4.035552513599396,
            "bert": 0.998020424246788
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9736842105263158,
            "rec": 0.75,
            "f1": 0.8473282442748091
        },
        "popular (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.937984496124031,
            "rec": 0.7707006369426752,
            "f1": 0.8461538461538463
        },
        "adversarial (POPE)": {
            "acc": 0.8811188811188811,
            "prec": 0.9537037037037037,
            "rec": 0.7803030303030303,
            "f1": 0.8583333333333334
        }
    },
    "gpt-4o-2024-05-13": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.56
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.35
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.8144329896907216
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.47
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.74
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.58
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.64
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.66
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.24
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.78
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.71
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.67
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.59
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.67
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.62
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.67
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.55
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.46938775510204084
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.16049382716049382
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.75
        },
        "Instance Location (SEED_2)": {
            "acc": 0.71
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.51
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.93
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.83
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.66
        },
        "celebrity (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "posters (MME)": {
            "acc": 0.98,
            "prec": 0.9795918367346939,
            "rec": 0.9795918367346939,
            "f1": 0.9795918367346939
        },
        "position (MME)": {
            "acc": 0.8,
            "prec": 0.78125,
            "rec": 0.8333333333333334,
            "f1": 0.8064516129032259
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9722222222222222,
            "rec": 0.7291666666666666,
            "f1": 0.8333333333333333
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.95,
            "prec": 0.96,
            "rec": 0.9411764705882353,
            "f1": 0.9504950495049505
        },
        "landmark (MME)": {
            "acc": 0.9,
            "prec": 0.9056603773584906,
            "rec": 0.9056603773584906,
            "f1": 0.9056603773584906
        },
        "text_translation (MME)": {
            "acc": 0.95,
            "prec": 1.0,
            "rec": 0.9,
            "f1": 0.9473684210526316
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.8,
            "prec": 0.8333333333333334,
            "rec": 0.75,
            "f1": 0.7894736842105262
        },
        "artwork (MME)": {
            "acc": 0.8,
            "prec": 0.75,
            "rec": 0.9230769230769231,
            "f1": 0.8275862068965517
        },
        "count (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.9032258064516129,
            "rec": 0.9333333333333333,
            "f1": 0.9180327868852459
        },
        "color (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "OCR (MME)": {
            "acc": 0.95,
            "prec": 0.95,
            "rec": 0.95,
            "f1": 0.9500000000000001
        },
        "code_reasoning (MME)": {
            "acc": 0.9,
            "prec": 0.9444444444444444,
            "rec": 0.85,
            "f1": 0.8947368421052632
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.98
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.67
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.92
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.96
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.99
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.66
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.93
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.78
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.648936170212766
        },
        "image_style (MMBench_CN)": {
            "acc": 0.83
        },
        "ocr (MMBench_CN)": {
            "acc": 0.83
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.7
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.48
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.97
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.96
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.78
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.87
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.76
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.85
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.72
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.85
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.75
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.73
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.69
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.87
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.74
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.77
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_EN)": {
            "acc": 0.85
        },
        "ocr (MMBench_EN)": {
            "acc": 0.76
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.66
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.51
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 1.0
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.62
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.78
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.87
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.2
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.1
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.5
        },
        "Design (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.43
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.45
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.6
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.5058823529411764
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.35
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6818181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8904109589041096
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.7692307692307693
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9693877551020408
        },
        "State capitals (ScienceQA)": {
            "acc": 0.59
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.6129032258064516
        },
        "Materials (ScienceQA)": {
            "acc": 0.98
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.1951219512195122
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.64
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.75
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.7254901960784313
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7758620689655172
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.5777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9230769230769231
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.65625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.7608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.5
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7368421052631579
        },
        "2D Count (CVBench)": {
            "acc": 0.66
        },
        "3D Distance (CVBench)": {
            "acc": 0.79
        },
        "3D Depth (CVBench)": {
            "acc": 0.86
        },
        "2D Relation (CVBench)": {
            "acc": 0.78
        },
        "Whole dataset (DECIMER)": {
            "bart": -3.790899804830551,
            "bert": 0.8394446676969528
        },
        "Whole dataset (Enrico)": {
            "bart": -5.343900454044342,
            "bert": 0.9843641620874405
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.540995669364929,
            "bert": 0.8291792094707489
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.180212504863739,
            "bert": 0.8656533014774322
        },
        "Whole dataset (GQA)": {
            "bart": -3.7385445439815523,
            "bert": 0.9890310776233673
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.334787969589233,
            "bert": 0.8016433602571488
        },
        "Whole dataset (INAT)": {
            "bart": -5.290412459373474,
            "bert": 0.8073651021718979
        },
        "Whole dataset (IRFL)": {
            "bart": -3.921849068403244,
            "bert": 0.9960874491930007
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.54340523481369,
            "bert": 0.8528187924623489
        },
        "Whole dataset (Memotion)": {
            "bart": -4.552424650192261,
            "bert": 0.8797484886646271
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.3614152097702026,
            "bert": 0.878449776172638
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.487533252239228,
            "bert": 0.9098891246318818
        },
        "Whole dataset (NLVR)": {
            "bart": -2.867698929309845,
            "bert": 0.9956584864854813
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.417340841293335,
            "bert": 0.9987759357690811
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5205892729759216,
            "bert": 0.8750199508666993
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.828215784430504,
            "bert": 0.926292821764946
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.1070697444677355,
            "bert": 0.8433184576034546
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.056581453084946,
            "bert": 0.8938645827770233
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.311981136798859,
            "bert": 0.8358456236124039
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.489748966693878,
            "bert": 0.8471041095256805
        },
        "Whole dataset (Slake)": {
            "bart": -3.445442713499069,
            "bert": 0.9920181423425675
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.150857380628586,
            "bert": 0.8538433176279068
        },
        "Whole dataset (VCR)": {
            "bart": -3.0376009279489518,
            "bert": 0.8900162529945373
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.511276997923851,
            "bert": 0.9065599209070205
        },
        "Whole dataset (VQA)": {
            "bart": -4.033050380945205,
            "bert": 0.9757144337892533
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.552103729248047,
            "bert": 0.935418677330017
        },
        "Whole dataset (Winoground)": {
            "bart": -3.8180119609832763,
            "bert": 0.9827828699350357
        },
        "random (POPE)": {
            "acc": 0.8859934853420195,
            "prec": 0.959349593495935,
            "rec": 0.7972972972972973,
            "f1": 0.8708487084870848
        },
        "popular (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.900709219858156,
            "rec": 0.8089171974522293,
            "f1": 0.8523489932885906
        },
        "adversarial (POPE)": {
            "acc": 0.8951048951048951,
            "prec": 0.9473684210526315,
            "rec": 0.8181818181818182,
            "f1": 0.8780487804878049
        }
    },
    "gpt-4o-2024-08-06": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.47
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.43
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.865979381443299
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.46
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.74
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.49
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.64
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.68
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.26
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.82
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.68
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.64
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.7
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.53
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.73
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.51
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4897959183673469
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.69
        },
        "Instance Location (SEED_2)": {
            "acc": 0.67
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.51
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.94
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.78
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.63
        },
        "celebrity (MME)": {
            "acc": 0.0,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "posters (MME)": {
            "acc": 0.96,
            "prec": 0.9787234042553191,
            "rec": 0.9387755102040817,
            "f1": 0.9583333333333333
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.7142857142857143,
            "rec": 0.8333333333333334,
            "f1": 0.7692307692307692
        },
        "scene (MME)": {
            "acc": 0.85,
            "prec": 0.9230769230769231,
            "rec": 0.75,
            "f1": 0.8275862068965517
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.94,
            "prec": 0.9245283018867925,
            "rec": 0.9607843137254902,
            "f1": 0.9423076923076923
        },
        "landmark (MME)": {
            "acc": 0.82,
            "prec": 0.8723404255319149,
            "rec": 0.7735849056603774,
            "f1": 0.8200000000000001
        },
        "text_translation (MME)": {
            "acc": 0.975,
            "prec": 1.0,
            "rec": 0.95,
            "f1": 0.9743589743589743
        },
        "existence (MME)": {
            "acc": 1.0,
            "prec": 1.0,
            "rec": 1.0,
            "f1": 1.0
        },
        "numerical_calculation (MME)": {
            "acc": 0.85,
            "prec": 0.8888888888888888,
            "rec": 0.8,
            "f1": 0.8421052631578948
        },
        "artwork (MME)": {
            "acc": 0.8,
            "prec": 0.7352941176470589,
            "rec": 0.9615384615384616,
            "f1": 0.8333333333333333
        },
        "count (MME)": {
            "acc": 0.9,
            "prec": 0.8529411764705882,
            "rec": 0.9666666666666667,
            "f1": 0.90625
        },
        "color (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "OCR (MME)": {
            "acc": 0.975,
            "prec": 1.0,
            "rec": 0.95,
            "f1": 0.9743589743589743
        },
        "code_reasoning (MME)": {
            "acc": 0.9,
            "prec": 0.9444444444444444,
            "rec": 0.85,
            "f1": 0.8947368421052632
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.82
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.96
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.63
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.71
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.9
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.96
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.99
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.73
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.93
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.86
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.72
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6276595744680851
        },
        "image_style (MMBench_CN)": {
            "acc": 0.86
        },
        "ocr (MMBench_CN)": {
            "acc": 0.87
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.68
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.98
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.94
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.76
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.85
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.72
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.84
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.66
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.74
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.84
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.73
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.64
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.74
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.87
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.73
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.8
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.86
        },
        "ocr (MMBench_EN)": {
            "acc": 0.79
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.72
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.51
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.98
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.66
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.68
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.89
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.1
        },
        "Art_Theory (MMMU)": {
            "acc": 0.13333333333333333
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.5
        },
        "Psychology (MMMU)": {
            "acc": 0.4
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.1
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.1
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.47
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.43
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.56
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.5647058823529412
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.4
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6590909090909091
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8493150684931506
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.7948717948717948
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9693877551020408
        },
        "State capitals (ScienceQA)": {
            "acc": 0.63
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.5483870967741935
        },
        "Materials (ScienceQA)": {
            "acc": 0.99
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.21951219512195122
        },
        "Geography (ScienceQA)": {
            "acc": 0.7619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.64
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6388888888888888
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.6274509803921569
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.6222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4647887323943662
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.59
        },
        "3D Distance (CVBench)": {
            "acc": 0.74
        },
        "3D Depth (CVBench)": {
            "acc": 0.77
        },
        "2D Relation (CVBench)": {
            "acc": 0.81
        },
        "Whole dataset (DECIMER)": {
            "bart": -3.7685873341560363,
            "bert": 0.8762210321426391
        },
        "Whole dataset (Enrico)": {
            "bart": -5.092805318832397,
            "bert": 0.987119574546814
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.536591899394989,
            "bert": 0.865555232167244
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.447867679595947,
            "bert": 0.862633671760559
        },
        "Whole dataset (GQA)": {
            "bart": -4.933423948287964,
            "bert": 0.9515338075160981
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.645128931999206,
            "bert": 0.8030791980028152
        },
        "Whole dataset (INAT)": {
            "bart": -5.217048138380051,
            "bert": 0.8092620420455933
        },
        "Whole dataset (IRFL)": {
            "bart": -4.807885857820511,
            "bert": 0.9609482246637344
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.7723390102386474,
            "bert": 0.8586222958564759
        },
        "Whole dataset (Memotion)": {
            "bart": -4.270537867546081,
            "bert": 0.8925521123409271
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.1128383672237394,
            "bert": 0.9030110359191894
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.411009769439698,
            "bert": 0.9379087233543396
        },
        "Whole dataset (NLVR)": {
            "bart": -3.2477638578414916,
            "bert": 0.9839769077301025
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.659320642948151,
            "bert": 0.991564661860466
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.69844535112381,
            "bert": 0.885537342429161
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.9305748730897903,
            "bert": 0.9206809854507446
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.262304179668426,
            "bert": 0.8555467903614045
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.702940658330918,
            "bert": 0.8680102425813675
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.609526596069336,
            "bert": 0.9185129845142365
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.55054584980011,
            "bert": 0.8479426664113998
        },
        "Whole dataset (Slake)": {
            "bart": -4.688661462068557,
            "bert": 0.9467143511772156
        },
        "Whole dataset (UCMerced)": {
            "bart": -3.9942769372463225,
            "bert": 0.8941673773527146
        },
        "Whole dataset (VCR)": {
            "bart": -3.4787450790405274,
            "bert": 0.9033622634410858
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5853021943569185,
            "bert": 0.9123284339904785
        },
        "Whole dataset (VQA)": {
            "bart": -4.559794310331345,
            "bert": 0.9545700550079346
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.230819553136826,
            "bert": 0.9080743116140365
        },
        "Whole dataset (Winoground)": {
            "bart": -4.067857102155686,
            "bert": 0.9744606637954711
        },
        "random (POPE)": {
            "acc": 0.8827361563517915,
            "prec": 0.9444444444444444,
            "rec": 0.8040540540540541,
            "f1": 0.8686131386861313
        },
        "popular (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.8791946308724832,
            "rec": 0.8343949044585988,
            "f1": 0.8562091503267973
        },
        "adversarial (POPE)": {
            "acc": 0.8776223776223776,
            "prec": 0.9217391304347826,
            "rec": 0.803030303030303,
            "f1": 0.8582995951417004
        }
    },
    "gpt-4o-mini-2024-07-18": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.44
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.39
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.38
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.68
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.55
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.47
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.65
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.15
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.74
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.71
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.6
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.47
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.31
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.8
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.59
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2653061224489796
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.65
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.66
        },
        "Instance Location (SEED_2)": {
            "acc": 0.57
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.39
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.9
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.62
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.59
        },
        "celebrity (MME)": {
            "acc": 0.47,
            "prec": 0.05555555555555555,
            "rec": 0.02702702702702703,
            "f1": 0.03636363636363636
        },
        "posters (MME)": {
            "acc": 0.94,
            "prec": 1.0,
            "rec": 0.8775510204081632,
            "f1": 0.9347826086956522
        },
        "position (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5806451612903226,
            "rec": 0.6,
            "f1": 0.5901639344262295
        },
        "scene (MME)": {
            "acc": 0.89,
            "prec": 1.0,
            "rec": 0.7708333333333334,
            "f1": 0.8705882352941177
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.87,
            "prec": 0.88,
            "rec": 0.8627450980392157,
            "f1": 0.8712871287128714
        },
        "landmark (MME)": {
            "acc": 0.61,
            "prec": 0.8888888888888888,
            "rec": 0.3018867924528302,
            "f1": 0.45070422535211263
        },
        "text_translation (MME)": {
            "acc": 0.85,
            "prec": 1.0,
            "rec": 0.7,
            "f1": 0.8235294117647058
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9655172413793104,
            "rec": 0.9333333333333333,
            "f1": 0.9491525423728815
        },
        "numerical_calculation (MME)": {
            "acc": 0.75,
            "prec": 1.0,
            "rec": 0.5,
            "f1": 0.6666666666666666
        },
        "artwork (MME)": {
            "acc": 0.78,
            "prec": 0.7884615384615384,
            "rec": 0.7884615384615384,
            "f1": 0.7884615384615384
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.8888888888888888,
            "rec": 0.8,
            "f1": 0.8421052631578948
        },
        "color (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9375,
            "rec": 1.0,
            "f1": 0.967741935483871
        },
        "OCR (MME)": {
            "acc": 1.0,
            "prec": 1.0,
            "rec": 1.0,
            "f1": 1.0
        },
        "code_reasoning (MME)": {
            "acc": 0.725,
            "prec": 0.9090909090909091,
            "rec": 0.5,
            "f1": 0.6451612903225806
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.86
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.86
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.96
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.31
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.85
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.93
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.98
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.64
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.86
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.53
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_CN)": {
            "acc": 0.86
        },
        "ocr (MMBench_CN)": {
            "acc": 0.9
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.56
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.41
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 1.0
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.95
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.66
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.76
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.76
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.9
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.33
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.76
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.95
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.93
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.53
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.86
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.75
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.57
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.54
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.45
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.99
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.66
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.85
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Design (MMMU)": {
            "acc": 0.2
        },
        "Literature (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.26666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.43
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.34
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.46
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.5176470588235295
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.33
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.6363636363636364
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8493150684931506
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9183673469387755
        },
        "State capitals (ScienceQA)": {
            "acc": 0.59
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4838709677419355
        },
        "Materials (ScienceQA)": {
            "acc": 0.93
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.55
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6388888888888888
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7758620689655172
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.71875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7105263157894737
        },
        "2D Count (CVBench)": {
            "acc": 0.61
        },
        "3D Distance (CVBench)": {
            "acc": 0.6
        },
        "3D Depth (CVBench)": {
            "acc": 0.51
        },
        "2D Relation (CVBench)": {
            "acc": 0.58
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.176688530445099,
            "bert": 0.8336903184652329
        },
        "Whole dataset (Enrico)": {
            "bart": -6.297029855251313,
            "bert": 0.8722961401939392
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.7010318827629085,
            "bert": 0.8311486500501633
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.5339369595050814,
            "bert": 0.8560194951295853
        },
        "Whole dataset (GQA)": {
            "bart": -4.668075511455536,
            "bert": 0.9705355578660965
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.569810638427734,
            "bert": 0.7960137897729873
        },
        "Whole dataset (INAT)": {
            "bart": -6.081218047142029,
            "bert": 0.7799925392866135
        },
        "Whole dataset (IRFL)": {
            "bart": -3.924498963356018,
            "bert": 0.9977041614055634
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8220355200767515,
            "bert": 0.8532611942291259
        },
        "Whole dataset (Memotion)": {
            "bart": -4.870782063007355,
            "bert": 0.8727031457424164
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.6596344435214996,
            "bert": 0.8901942878961563
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -7.219493093490601,
            "bert": 0.8448171311616898
        },
        "Whole dataset (NLVR)": {
            "bart": -2.7867458772659304,
            "bert": 0.9995277005434037
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.4458016538619995,
            "bert": 0.9996617478132248
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.735849459171295,
            "bert": 0.8724708604812622
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.862439754605293,
            "bert": 0.9226667702198028
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.582834085822105,
            "bert": 0.8222634172439576
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.435051158666611,
            "bert": 0.8847962808609009
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.292639763355255,
            "bert": 0.8897049295902252
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.568328416347503,
            "bert": 0.8457278996706009
        },
        "Whole dataset (Slake)": {
            "bart": -4.460323591232299,
            "bert": 0.9705364054441452
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.69242094039917,
            "bert": 0.8316701805591583
        },
        "Whole dataset (VCR)": {
            "bart": -3.582807149887085,
            "bert": 0.8850350522994995
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.611427156925201,
            "bert": 0.9077492606639862
        },
        "Whole dataset (VQA)": {
            "bart": -4.7822479724884035,
            "bert": 0.9653922975063324
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.080727559328079,
            "bert": 0.92337941467762
        },
        "Whole dataset (Winoground)": {
            "bart": -4.314244233369827,
            "bert": 0.9975053125619888
        },
        "random (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.9439252336448598,
            "rec": 0.6824324324324325,
            "f1": 0.7921568627450981
        },
        "popular (POPE)": {
            "acc": 0.8045602605863192,
            "prec": 0.8702290076335878,
            "rec": 0.7261146496815286,
            "f1": 0.7916666666666666
        },
        "adversarial (POPE)": {
            "acc": 0.8461538461538461,
            "prec": 0.9230769230769231,
            "rec": 0.7272727272727273,
            "f1": 0.8135593220338984
        }
    },
    "gpt-4-turbo-2024-04-09": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.48
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.43
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.8041237113402062
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.8607594936708861
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.43
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.6
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.45
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.52
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.66
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.27
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.82
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.65
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.53
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.6
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.52
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.72
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.47
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.62
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2962962962962963
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.62
        },
        "Instance Location (SEED_2)": {
            "acc": 0.57
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.46
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.88
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.65
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.56
        },
        "celebrity (MME)": {
            "acc": 0.73,
            "prec": 0.6136363636363636,
            "rec": 0.7297297297297297,
            "f1": 0.6666666666666666
        },
        "posters (MME)": {
            "acc": 0.95,
            "prec": 0.9782608695652174,
            "rec": 0.9183673469387755,
            "f1": 0.9473684210526316
        },
        "position (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.5333333333333333,
            "f1": 0.5161290322580646
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 1.0,
            "rec": 0.7291666666666666,
            "f1": 0.8433734939759037
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.89,
            "prec": 0.9,
            "rec": 0.8823529411764706,
            "f1": 0.8910891089108911
        },
        "landmark (MME)": {
            "acc": 0.83,
            "prec": 0.9736842105263158,
            "rec": 0.6981132075471698,
            "f1": 0.8131868131868132
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.631578947368421,
            "rec": 0.6,
            "f1": 0.6153846153846154
        },
        "existence (MME)": {
            "acc": 0.9166666666666666,
            "prec": 1.0,
            "rec": 0.8333333333333334,
            "f1": 0.9090909090909091
        },
        "numerical_calculation (MME)": {
            "acc": 0.6,
            "prec": 0.6111111111111112,
            "rec": 0.55,
            "f1": 0.5789473684210527
        },
        "artwork (MME)": {
            "acc": 0.78,
            "prec": 0.9166666666666666,
            "rec": 0.6346153846153846,
            "f1": 0.75
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8928571428571429,
            "rec": 0.8333333333333334,
            "f1": 0.8620689655172413
        },
        "OCR (MME)": {
            "acc": 0.95,
            "prec": 0.95,
            "rec": 0.95,
            "f1": 0.9500000000000001
        },
        "code_reasoning (MME)": {
            "acc": 0.9,
            "prec": 1.0,
            "rec": 0.8,
            "f1": 0.888888888888889
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.93
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.97
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.48
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.89
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.95
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.99
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.66
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.91
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.66
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_CN)": {
            "acc": 0.84
        },
        "ocr (MMBench_CN)": {
            "acc": 0.79
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.54
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.37
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.99
        },
        "image_topic (MMBench_CN)": {
            "acc": 1.0
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.63
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.84
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.7
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.83
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.82
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.64
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.79
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.6
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.47
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.55
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.83
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.68
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.67
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_EN)": {
            "acc": 0.87
        },
        "ocr (MMBench_EN)": {
            "acc": 0.57
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.98
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.45
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.84
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.1
        },
        "Art_Theory (MMMU)": {
            "acc": 0.06666666666666667
        },
        "History (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.1
        },
        "Geography (MMMU)": {
            "acc": 0.0
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.0
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.1
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Music (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.4
        },
        "Design (MMMU)": {
            "acc": 0.0
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.0
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.38
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.39
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.53
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.47058823529411764
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.39
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.625
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8767123287671232
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9489795918367347
        },
        "State capitals (ScienceQA)": {
            "acc": 0.59
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.5161290322580645
        },
        "Materials (ScienceQA)": {
            "acc": 0.97
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.07317073170731707
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.59
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.7777777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.45098039215686275
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6896551724137931
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.1111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.6875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Classification (ScienceQA)": {
            "acc": 1.0
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5070422535211268
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.6052631578947368
        },
        "2D Count (CVBench)": {
            "acc": 0.57
        },
        "3D Distance (CVBench)": {
            "acc": 0.64
        },
        "3D Depth (CVBench)": {
            "acc": 0.77
        },
        "2D Relation (CVBench)": {
            "acc": 0.7
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.155452301502228,
            "bert": 0.8011188644170761
        },
        "Whole dataset (Enrico)": {
            "bart": -5.97755376458168,
            "bert": 0.880430064201355
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.80283679485321,
            "bert": 0.8214268857240676
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2287966072559358,
            "bert": 0.8636076927185059
        },
        "Whole dataset (GQA)": {
            "bart": -4.457721493244171,
            "bert": 0.9795397955179215
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.549714150428772,
            "bert": 0.791836913228035
        },
        "Whole dataset (INAT)": {
            "bart": -5.8823286724090575,
            "bert": 0.7830755233764648
        },
        "Whole dataset (IRFL)": {
            "bart": -4.454667543172836,
            "bert": 0.9748404330015182
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.852735574245453,
            "bert": 0.845382206439972
        },
        "Whole dataset (Memotion)": {
            "bart": -5.048353633880615,
            "bert": 0.8438220030069351
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7799600940942764,
            "bert": 0.8498301631212235
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.515080881118775,
            "bert": 0.809451898932457
        },
        "Whole dataset (NLVR)": {
            "bart": -3.016992585659027,
            "bert": 0.9908865702152252
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.864440634250641,
            "bert": 0.9888126331567765
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.537990303039551,
            "bert": 0.870482052564621
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.4923032480478287,
            "bert": 0.9198162430524826
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.121342868804931,
            "bert": 0.802525218129158
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.216680254936218,
            "bert": 0.8994115769863129
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.762510371208191,
            "bert": 0.8242373168468475
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.515684039592743,
            "bert": 0.8428976887464523
        },
        "Whole dataset (Slake)": {
            "bart": -3.9671134674549102,
            "bert": 0.9739569616317749
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.770508515834808,
            "bert": 0.8111113339662552
        },
        "Whole dataset (VCR)": {
            "bart": -3.4237101233005522,
            "bert": 0.8496661192178726
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7630518370866777,
            "bert": 0.9040498107671737
        },
        "Whole dataset (VQA)": {
            "bart": -4.254095431566238,
            "bert": 0.9742480063438416
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.746282119750976,
            "bert": 0.9305470764636994
        },
        "Whole dataset (Winoground)": {
            "bart": -4.425220282077789,
            "bert": 0.9728111332654953
        },
        "random (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.944954128440367,
            "rec": 0.6959459459459459,
            "f1": 0.8015564202334631
        },
        "popular (POPE)": {
            "acc": 0.7915309446254072,
            "prec": 0.9043478260869565,
            "rec": 0.6624203821656051,
            "f1": 0.7647058823529411
        },
        "adversarial (POPE)": {
            "acc": 0.8321678321678322,
            "prec": 0.9375,
            "rec": 0.6818181818181818,
            "f1": 0.7894736842105263
        }
    },
    "gemini-1.5-pro": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.32
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.46
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7525773195876289
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.52
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.68
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.63
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.57
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.75
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.36
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.78
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.65
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.81
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.63
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.7448979591836735
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.54
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.63
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.62
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.71
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.6
        },
        "Instance Location (SEED_2)": {
            "acc": 0.61
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.51
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.8888888888888888
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.68
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.6
        },
        "celebrity (MME)": {
            "acc": 0.86,
            "prec": 0.7674418604651163,
            "rec": 0.8918918918918919,
            "f1": 0.825
        },
        "posters (MME)": {
            "acc": 0.86,
            "prec": 0.926829268292683,
            "rec": 0.7755102040816326,
            "f1": 0.8444444444444446
        },
        "position (MME)": {
            "acc": 0.6,
            "prec": 0.6666666666666666,
            "rec": 0.4,
            "f1": 0.5
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.972972972972973,
            "rec": 0.75,
            "f1": 0.8470588235294119
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.77,
            "prec": 0.7592592592592593,
            "rec": 0.803921568627451,
            "f1": 0.780952380952381
        },
        "landmark (MME)": {
            "acc": 0.75,
            "prec": 0.9666666666666667,
            "rec": 0.5471698113207547,
            "f1": 0.6987951807228916
        },
        "text_translation (MME)": {
            "acc": 0.675,
            "prec": 0.6842105263157895,
            "rec": 0.65,
            "f1": 0.6666666666666667
        },
        "existence (MME)": {
            "acc": 0.9333333333333333,
            "prec": 1.0,
            "rec": 0.8666666666666667,
            "f1": 0.9285714285714286
        },
        "numerical_calculation (MME)": {
            "acc": 0.7,
            "prec": 0.6538461538461539,
            "rec": 0.85,
            "f1": 0.7391304347826088
        },
        "artwork (MME)": {
            "acc": 0.77,
            "prec": 0.8372093023255814,
            "rec": 0.6923076923076923,
            "f1": 0.7578947368421053
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.75,
            "rec": 0.8,
            "f1": 0.7741935483870969
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.9,
            "rec": 0.9,
            "f1": 0.9
        },
        "OCR (MME)": {
            "acc": 0.85,
            "prec": 0.7692307692307693,
            "rec": 1.0,
            "f1": 0.8695652173913044
        },
        "code_reasoning (MME)": {
            "acc": 0.8,
            "prec": 0.8333333333333334,
            "rec": 0.75,
            "f1": 0.7894736842105262
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.88
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.93
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.95
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.67
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.81
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.94
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.97
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.58
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.82
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.66
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_CN)": {
            "acc": 0.79
        },
        "ocr (MMBench_CN)": {
            "acc": 0.76
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.59
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.47
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.94
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8969072164948454
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.7
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.69
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.87
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.92
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.61
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.86
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.86
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.78
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.61
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.91
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.74
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.74
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.648936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.86
        },
        "ocr (MMBench_EN)": {
            "acc": 0.82
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.61
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.47
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.95
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.7628865979381443
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.69
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.85
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.1724137931034483
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.27586206896551724
        },
        "Physics (MMMU)": {
            "acc": 0.4
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Materials (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.5
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.3
        },
        "Literature (MMMU)": {
            "acc": 0.3
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.26666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.38
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.21
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.41
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.4117647058823529
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.32
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.821917808219178
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.7435897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.4931506849315068
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9693877551020408
        },
        "State capitals (ScienceQA)": {
            "acc": 0.77
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.6451612903225806
        },
        "Materials (ScienceQA)": {
            "acc": 0.93
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2926829268292683
        },
        "Geography (ScienceQA)": {
            "acc": 0.8095238095238095
        },
        "Magnets (ScienceQA)": {
            "acc": 0.7
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.75
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7413793103448276
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.5333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.6363636363636364
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 1.0
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5185185185185185
        },
        "Maps (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7843137254901961
        },
        "Classification (ScienceQA)": {
            "acc": 0.9873417721518988
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.7105263157894737
        },
        "2D Count (CVBench)": {
            "acc": 0.73
        },
        "3D Distance (CVBench)": {
            "acc": 0.66
        },
        "3D Depth (CVBench)": {
            "acc": 0.73
        },
        "2D Relation (CVBench)": {
            "acc": 0.86
        },
        "Whole dataset (DECIMER)": {
            "bart": -3.53818922996521,
            "bert": 0.8761899381875992
        },
        "Whole dataset (Enrico)": {
            "bart": -5.394196472167969,
            "bert": 0.9367954331636429
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.404076871871948,
            "bert": 0.8601196783781052
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.6945094799995424,
            "bert": 0.8556672096252441
        },
        "Whole dataset (GQA)": {
            "bart": -4.747132170200348,
            "bert": 0.9716888117790222
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.265399804115296,
            "bert": 0.7894046223163604
        },
        "Whole dataset (INAT)": {
            "bart": -5.653467022180557,
            "bert": 0.7774196362495422
        },
        "Whole dataset (IRFL)": {
            "bart": -4.334200258255005,
            "bert": 0.9883779352903366
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.624661900997162,
            "bert": 0.8476464688777924
        },
        "Whole dataset (Memotion)": {
            "bart": -4.311983971595764,
            "bert": 0.8840228551626206
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.069489113986492,
            "bert": 0.9002579802274704
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.9268353962898255,
            "bert": 0.878507861495018
        },
        "Whole dataset (NLVR)": {
            "bart": -3.243312840461731,
            "bert": 0.9977193140983581
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.7523704171180725,
            "bert": 0.9992019909620286
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.9235605704784393,
            "bert": 0.8787317740917205
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.914941138625145,
            "bert": 0.9275559496879577
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.186558110713959,
            "bert": 0.7845909875631333
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.431802176414652,
            "bert": 0.8906653751718238
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.053226771354676,
            "bert": 0.8867787212133408
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.676030759811401,
            "bert": 0.8563684433698654
        },
        "Whole dataset (Slake)": {
            "bart": -4.332045350074768,
            "bert": 0.9631537938117981
        },
        "Whole dataset (UCMerced)": {
            "bart": -3.971466932296753,
            "bert": 0.8895656871795654
        },
        "Whole dataset (VCR)": {
            "bart": -3.331059448122978,
            "bert": 0.875796063542366
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.689919292330742,
            "bert": 0.9088106995820999
        },
        "Whole dataset (VQA)": {
            "bart": -4.736547720432282,
            "bert": 0.9615972626209259
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.370185328722,
            "bert": 0.9227400600910187
        },
        "Whole dataset (Winoground)": {
            "bart": -5.226220560073853,
            "bert": 0.8736329555511475
        },
        "random (POPE)": {
            "acc": 0.8464052287581699,
            "prec": 0.9719626168224299,
            "rec": 0.7027027027027027,
            "f1": 0.815686274509804
        },
        "popular (POPE)": {
            "acc": 0.8032786885245902,
            "prec": 0.9137931034482759,
            "rec": 0.6794871794871795,
            "f1": 0.7794117647058824
        },
        "adversarial (POPE)": {
            "acc": 0.8,
            "prec": 0.8712871287128713,
            "rec": 0.6666666666666666,
            "f1": 0.7553648068669527
        }
    },
    "gemini-1.5-flash": {
        "Global Video Understanding (SEED_2)": {
            "acc": 0.52
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.32
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.5
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.67
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.71
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.55
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.75
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.29
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.72
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.74
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.75
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.48
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.5612244897959183
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.5
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.77
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.73
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.93
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.93
        },
        "Instance Location (SEED_2)": {
            "acc": 0.59
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.38
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.8686868686868687
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6
        },
        "Instance Attributes (SEED_2)": {
            "acc": 0.66
        },
        "celebrity (MME)": {
            "acc": 0.88,
            "prec": 0.8048780487804879,
            "rec": 0.8918918918918919,
            "f1": 0.8461538461538461
        },
        "posters (MME)": {
            "acc": 0.88,
            "prec": 0.9512195121951219,
            "rec": 0.7959183673469388,
            "f1": 0.8666666666666666
        },
        "position (MME)": {
            "acc": 0.4666666666666667,
            "prec": 0.4166666666666667,
            "rec": 0.16666666666666666,
            "f1": 0.23809523809523808
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9722222222222222,
            "rec": 0.7291666666666666,
            "f1": 0.8333333333333333
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.82,
            "prec": 0.8,
            "rec": 0.8627450980392157,
            "f1": 0.8301886792452831
        },
        "landmark (MME)": {
            "acc": 0.74,
            "prec": 1.0,
            "rec": 0.5094339622641509,
            "f1": 0.6749999999999999
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.8571428571428571,
            "rec": 0.3,
            "f1": 0.4444444444444444
        },
        "existence (MME)": {
            "acc": 0.9,
            "prec": 0.9285714285714286,
            "rec": 0.8666666666666667,
            "f1": 0.896551724137931
        },
        "numerical_calculation (MME)": {
            "acc": 0.65,
            "prec": 0.6071428571428571,
            "rec": 0.85,
            "f1": 0.7083333333333333
        },
        "artwork (MME)": {
            "acc": 0.75,
            "prec": 0.8,
            "rec": 0.6923076923076923,
            "f1": 0.7422680412371134
        },
        "count (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6585365853658537,
            "rec": 0.9,
            "f1": 0.7605633802816902
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8125,
            "rec": 0.8666666666666667,
            "f1": 0.8387096774193549
        },
        "OCR (MME)": {
            "acc": 0.875,
            "prec": 0.8260869565217391,
            "rec": 0.95,
            "f1": 0.8837209302325583
        },
        "code_reasoning (MME)": {
            "acc": 0.725,
            "prec": 0.68,
            "rec": 0.85,
            "f1": 0.7555555555555556
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.83
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.96
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.32
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.82
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.94
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.99
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.73
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.9
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.86
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.61
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_CN)": {
            "acc": 0.84
        },
        "ocr (MMBench_CN)": {
            "acc": 0.89
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.94
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9587628865979382
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.71
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.81
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.8
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.96
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.47
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.87
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.99
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.99
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.79
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.87
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.84
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9
        },
        "ocr (MMBench_EN)": {
            "acc": 0.94
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.53
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.96
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9072164948453608
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.65
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.82
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.6
        },
        "Public_Health (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.5
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.8
        },
        "History (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.4
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.6
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.4
        },
        "Agriculture (MMMU)": {
            "acc": 0.5666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.38
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.32
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.44
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.3764705882352941
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.28
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.5909090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.8356164383561644
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.8205128205128205
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.9489795918367347
        },
        "State capitals (ScienceQA)": {
            "acc": 0.95
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4838709677419355
        },
        "Materials (ScienceQA)": {
            "acc": 0.86
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.8333333333333334
        },
        "Magnets (ScienceQA)": {
            "acc": 0.69
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.6944444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.7413793103448276
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4888888888888889
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.9487179487179487
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.6851851851851852
        },
        "Maps (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.803921568627451
        },
        "Classification (ScienceQA)": {
            "acc": 0.9873417721518988
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.676056338028169
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.9473684210526315
        },
        "2D Count (CVBench)": {
            "acc": 0.68
        },
        "3D Distance (CVBench)": {
            "acc": 0.57
        },
        "3D Depth (CVBench)": {
            "acc": 0.78
        },
        "2D Relation (CVBench)": {
            "acc": 0.75
        },
        "Whole dataset (DECIMER)": {
            "bart": -3.822859529256821,
            "bert": 0.9051801896095276
        },
        "Whole dataset (Enrico)": {
            "bart": -5.496284326314926,
            "bert": 0.985725884437561
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.94974992275238,
            "bert": 0.9524074018001556
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.239363627433777,
            "bert": 0.838002291917801
        },
        "Whole dataset (GQA)": {
            "bart": -5.229442899227142,
            "bert": 0.9497250092029571
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.627222604751587,
            "bert": 0.8029564887285232
        },
        "Whole dataset (INAT)": {
            "bart": -7.271262423992157,
            "bert": 0.776885661482811
        },
        "Whole dataset (IRFL)": {
            "bart": -4.418855293989181,
            "bert": 0.989610498547554
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.4983801507949828,
            "bert": 0.866709503531456
        },
        "Whole dataset (Memotion)": {
            "bart": -4.65423662185669,
            "bert": 0.8939833503961563
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.2620261347293855,
            "bert": 0.8958747559785842
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.500758965015411,
            "bert": 0.9411125946044921
        },
        "Whole dataset (NLVR)": {
            "bart": -3.4567879486083983,
            "bert": 0.9828982746601105
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.845188810825348,
            "bert": 0.9942238521575928
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.342326294183731,
            "bert": 0.86496861577034
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.464460958242416,
            "bert": 0.9105950433015824
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.40974380493164,
            "bert": 0.7978787368535996
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.717645683187119,
            "bert": 0.8936376229245612
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.623417237997055,
            "bert": 0.9502417129278183
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.722165639400482,
            "bert": 0.8564683628082276
        },
        "Whole dataset (Slake)": {
            "bart": -4.3199044907093045,
            "bert": 0.9600981909036637
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.057582197189331,
            "bert": 0.9020012694597245
        },
        "Whole dataset (VCR)": {
            "bart": -2.8896359130740166,
            "bert": 0.9205769228935242
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6917849385738375,
            "bert": 0.9359231293201447
        },
        "Whole dataset (VQA)": {
            "bart": -5.131995339393615,
            "bert": 0.9426296746730805
        },
        "Whole dataset (VQARAD)": {
            "bart": -5.512514841556549,
            "bert": 0.9125846832990646
        },
        "Whole dataset (Winoground)": {
            "bart": -4.9848792064189915,
            "bert": 0.9629830819368362
        },
        "random (POPE)": {
            "acc": 0.8202614379084967,
            "prec": 0.9345794392523364,
            "rec": 0.6756756756756757,
            "f1": 0.7843137254901961
        },
        "popular (POPE)": {
            "acc": 0.7901639344262295,
            "prec": 0.9181818181818182,
            "rec": 0.6474358974358975,
            "f1": 0.7593984962406015
        },
        "adversarial (POPE)": {
            "acc": 0.8035087719298246,
            "prec": 0.8877551020408163,
            "rec": 0.6590909090909091,
            "f1": 0.7565217391304347
        }
    },
    "reproduction-llava-v15+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6625080662508066
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6806387225548902
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5282308657465495
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.85
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5717204740498569
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3485714285714286
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6597701149425287
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23809523809523808
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.41683233929754804
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6022494887525562
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7409753008233059
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6903331512834516
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.29545454545454547
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5289421157684631
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5144596651445966
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.516245487364621
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3421309872922776
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7643504531722054
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4083333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5157232704402516
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8363636363636363
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6482412060301508
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.7823529411764706,
            "prec": 0.9363636363636364,
            "rec": 0.6058823529411764,
            "f1": 0.7357142857142857
        },
        "posters (MME)": {
            "acc": 0.8469387755102041,
            "prec": 0.9322033898305084,
            "rec": 0.7482993197278912,
            "f1": 0.830188679245283
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6666666666666666,
            "rec": 0.9333333333333333,
            "f1": 0.7777777777777778
        },
        "scene (MME)": {
            "acc": 0.88,
            "prec": 0.8958333333333334,
            "rec": 0.86,
            "f1": 0.8775510204081632
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.6333333333333333,
            "rec": 0.8142857142857143,
            "f1": 0.7125
        },
        "artwork (MME)": {
            "acc": 0.7075,
            "prec": 0.6653386454183267,
            "rec": 0.835,
            "f1": 0.7405764966740577
        },
        "landmark (MME)": {
            "acc": 0.8875,
            "prec": 0.8571428571428571,
            "rec": 0.93,
            "f1": 0.8920863309352518
        },
        "text_translation (MME)": {
            "acc": 0.675,
            "prec": 0.7333333333333333,
            "rec": 0.55,
            "f1": 0.6285714285714286
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.325,
            "prec": 0.29411764705882354,
            "rec": 0.25,
            "f1": 0.27027027027027023
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.7368421052631579,
            "rec": 0.9333333333333333,
            "f1": 0.8235294117647058
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6666666666666666,
            "rec": 0.7,
            "f1": 0.6829268292682926
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8604651162790697
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5307692307692308
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5068493150684932
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4748603351955307
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7676767676767676
        },
        "ocr (MMBench_CN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.44680851063829785
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8026315789473685
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4787234042553192
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8349056603773585
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7727272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8857142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5968253968253968
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5153846153846153
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5363128491620112
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9803439803439803
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8712121212121212
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7051282051282052
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4858156028368794
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.819078947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_EN)": {
            "acc": 0.839622641509434
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6666666666666666
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.20915032679738563
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14705882352941177
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.7762237762237763
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.20967741935483872
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.78125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8607594936708861
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4647887323943662
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5355329949238579
        },
        "3D Distance (CVBench)": {
            "acc": 0.51
        },
        "2D Relation (CVBench)": {
            "acc": 0.6184615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.7016666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.774259459972382,
            "bert": 0.783818119764328
        },
        "Whole dataset (Enrico)": {
            "bart": -5.846827602386474,
            "bert": 0.9794223648309708
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.907647371292114,
            "bert": 0.8203474724292755
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.191381552219391,
            "bert": 0.884528574347496
        },
        "Whole dataset (GQA)": {
            "bart": -3.8124600791931154,
            "bert": 0.9922494918107987
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.224811058044434,
            "bert": 0.7919550812244416
        },
        "Whole dataset (INAT)": {
            "bart": -6.3422819828987125,
            "bert": 0.7918597930669784
        },
        "Whole dataset (IRFL)": {
            "bart": -4.236332433223724,
            "bert": 0.9986636704206466
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9661520636081695,
            "bert": 0.8495265483856201
        },
        "Whole dataset (Memotion)": {
            "bart": -5.011538417339325,
            "bert": 0.8475045931339263
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7604608082771303,
            "bert": 0.8718882668018341
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.927870855331421,
            "bert": 0.8193017619848252
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.2411762428283692,
            "bert": 0.9114274787902832
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.145553252696991,
            "bert": 0.9405174595117569
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.33458803832531,
            "bert": 0.8258679467439651
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.962557331323624,
            "bert": 0.9120776921510696
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.647787171602249,
            "bert": 0.9088830977678299
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.1813255977630615,
            "bert": 0.8542789375782013
        },
        "Whole dataset (Slake)": {
            "bart": -3.5608798587322235,
            "bert": 0.9945332139730454
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.357313072681427,
            "bert": 0.8249402385950089
        },
        "Whole dataset (VCR)": {
            "bart": -3.4324038338661196,
            "bert": 0.9067967689037323
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.622597550749779,
            "bert": 0.9060700362920762
        },
        "Whole dataset (VQA)": {
            "bart": -4.440135974884033,
            "bert": 0.9730229490995407
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.627265691757202,
            "bert": 0.9395037549734115
        },
        "Whole dataset (Winoground)": {
            "bart": -4.067694215774536,
            "bert": 0.998019168972969
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9823008849557522,
            "rec": 0.75,
            "f1": 0.8505747126436781
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.9097744360902256,
            "rec": 0.7707006369426752,
            "f1": 0.8344827586206898
        },
        "adversarial (POPE)": {
            "acc": 0.8916083916083916,
            "prec": 0.9809523809523809,
            "rec": 0.7803030303030303,
            "f1": 0.8691983122362869
        }
    },
    "reproduction-llava-v15+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6936975693697569
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6766467065868264
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5495608531994981
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2874251497005988
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.858
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5888843481814466
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.36653061224489797
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5586206896551724
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2926587301587302
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.43273691186216034
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6114519427402862
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7549081697276757
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7056253413435282
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.23484848484848486
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5548902195608783
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5296803652968036
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5379061371841155
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2756598240469208
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7854984894259819
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.39166666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5471698113207547
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8424242424242424
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.7705882352941177,
            "prec": 0.6900826446280992,
            "rec": 0.9823529411764705,
            "f1": 0.8106796116504854
        },
        "posters (MME)": {
            "acc": 0.8435374149659864,
            "prec": 0.8300653594771242,
            "rec": 0.8639455782312925,
            "f1": 0.8466666666666667
        },
        "position (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6041666666666666,
            "rec": 0.9666666666666667,
            "f1": 0.7435897435897435
        },
        "scene (MME)": {
            "acc": 0.85,
            "prec": 0.8125,
            "rec": 0.91,
            "f1": 0.8584905660377358
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.6421052631578947,
            "rec": 0.8714285714285714,
            "f1": 0.7393939393939394
        },
        "artwork (MME)": {
            "acc": 0.6475,
            "prec": 0.6020761245674741,
            "rec": 0.87,
            "f1": 0.7116564417177914
        },
        "landmark (MME)": {
            "acc": 0.88,
            "prec": 0.8392857142857143,
            "rec": 0.94,
            "f1": 0.8867924528301886
        },
        "text_translation (MME)": {
            "acc": 0.65,
            "prec": 0.6153846153846154,
            "rec": 0.8,
            "f1": 0.6956521739130435
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9354838709677419,
            "rec": 0.9666666666666667,
            "f1": 0.9508196721311476
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.4375,
            "rec": 0.7,
            "f1": 0.5384615384615384
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7317073170731707,
            "rec": 1.0,
            "f1": 0.8450704225352113
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.6785714285714286,
            "rec": 0.95,
            "f1": 0.7916666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.813953488372093
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6063492063492063
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6153846153846154
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.44692737430167595
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7550505050505051
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7628205128205128
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.795
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7840909090909091
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5714285714285714
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6164383561643836
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5139664804469274
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.898989898989899
        },
        "ocr (MMBench_EN)": {
            "acc": 0.782051282051282
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4124293785310734
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8256578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8537735849056604
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.875
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2786885245901639
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24183006535947713
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.19117647058823528
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9807692307692307
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8181818181818182
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.14516129032258066
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4818181818181818
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.603448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7435897435897436
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.46296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.13157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.9310344827586207
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6619718309859155
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.6040609137055838
        },
        "3D Distance (CVBench)": {
            "acc": 0.5783333333333334
        },
        "2D Relation (CVBench)": {
            "acc": 0.6846153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.745
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.647419700622558,
            "bert": 0.7976887208223343
        },
        "Whole dataset (Enrico)": {
            "bart": -5.25767693400383,
            "bert": 0.9891024219989777
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.123725941181183,
            "bert": 0.9930862158536911
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.191519113779068,
            "bert": 0.8801783961057663
        },
        "Whole dataset (GQA)": {
            "bart": -3.769923436641693,
            "bert": 0.9922460854053498
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.096144227981568,
            "bert": 0.7928371626138687
        },
        "Whole dataset (INAT)": {
            "bart": -6.3415808010101316,
            "bert": 0.7908164894580841
        },
        "Whole dataset (IRFL)": {
            "bart": -4.185808911323547,
            "bert": 0.9986731868982315
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9399489986896516,
            "bert": 0.8505815422534942
        },
        "Whole dataset (Memotion)": {
            "bart": -4.0121221089363095,
            "bert": 0.8981597530841827
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.4141185545921324,
            "bert": 0.8863478028774261
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.992978601455689,
            "bert": 0.9365185743570328
        },
        "Whole dataset (NLVR)": {
            "bart": -3.653083643913269,
            "bert": 0.9992854368686676
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.5283060932159422,
            "bert": 0.9992926931381225
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.280796685218811,
            "bert": 0.9044486230611801
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.9964889270067214,
            "bert": 0.9431732100248337
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.992866280674934,
            "bert": 0.8312638747692108
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.115236465930939,
            "bert": 0.9122084939479828
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.379505761861801,
            "bert": 0.902660517692566
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.1093240737915036,
            "bert": 0.8594903647899628
        },
        "Whole dataset (Slake)": {
            "bart": -3.714381242990494,
            "bert": 0.9945168739557266
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.322639948129654,
            "bert": 0.8859818029403687
        },
        "Whole dataset (VCR)": {
            "bart": -3.2313003188371656,
            "bert": 0.9206017380952836
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6856709963083265,
            "bert": 0.9049878948926926
        },
        "Whole dataset (VQA)": {
            "bart": -4.207686533927918,
            "bert": 0.9727925848960877
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.599694590568543,
            "bert": 0.9422929722070694
        },
        "Whole dataset (Winoground)": {
            "bart": -4.151528165340424,
            "bert": 0.9979979795217514
        },
        "random (POPE)": {
            "acc": 0.8859934853420195,
            "prec": 0.9747899159663865,
            "rec": 0.7837837837837838,
            "f1": 0.8689138576779025
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.916030534351145,
            "rec": 0.7643312101910829,
            "f1": 0.8333333333333334
        },
        "adversarial (POPE)": {
            "acc": 0.8951048951048951,
            "prec": 0.9722222222222222,
            "rec": 0.7954545454545454,
            "f1": 0.875
        }
    },
    "one-stage+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6919767691976769
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6666666666666666
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5432873274780426
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2634730538922156
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.768
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5864323661626482
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.36081632653061224
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5839080459770115
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25793650793650796
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.40755467196819084
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6226993865030674
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7216494845360825
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7469917669411019
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6892408519934462
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2803030303030303
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5189620758483033
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5159817351598174
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5342960288808665
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3460410557184751
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7794561933534743
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4779874213836478
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7151515151515152
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6231155778894473
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.30864197530864196
        },
        "celebrity (MME)": {
            "acc": 0.6882352941176471,
            "prec": 0.7807017543859649,
            "rec": 0.5235294117647059,
            "f1": 0.6267605633802817
        },
        "posters (MME)": {
            "acc": 0.8163265306122449,
            "prec": 0.911504424778761,
            "rec": 0.7006802721088435,
            "f1": 0.7923076923076923
        },
        "position (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.627906976744186,
            "rec": 0.9,
            "f1": 0.7397260273972602
        },
        "scene (MME)": {
            "acc": 0.8675,
            "prec": 0.9060773480662984,
            "rec": 0.82,
            "f1": 0.8608923884514436
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7071428571428572,
            "prec": 0.6526315789473685,
            "rec": 0.8857142857142857,
            "f1": 0.7515151515151516
        },
        "artwork (MME)": {
            "acc": 0.665,
            "prec": 0.6161971830985915,
            "rec": 0.875,
            "f1": 0.7231404958677685
        },
        "landmark (MME)": {
            "acc": 0.8475,
            "prec": 0.8262910798122066,
            "rec": 0.88,
            "f1": 0.8523002421307507
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.3684210526315789,
            "rec": 0.35,
            "f1": 0.358974358974359
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.75,
            "rec": 1.0,
            "f1": 0.8571428571428571
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6060606060606061,
            "rec": 1.0,
            "f1": 0.7547169811320755
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4520547945205479
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4972067039106145
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8976744186046511
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6742424242424242
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6538461538461539
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4293785310734463
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.40070921985815605
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7796052631578947
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7613636363636364
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8857142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5777777777777777
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6164383561643836
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4692737430167598
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7550505050505051
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6858974358974359
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4326241134751773
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8519736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8712121212121212
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.5
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.4
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24183006535947713
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1935483870967742
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.39436619718309857
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.5355329949238579
        },
        "3D Distance (CVBench)": {
            "acc": 0.5333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6215384615384615
        },
        "3D Depth (CVBench)": {
            "acc": 0.6183333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.8099920320510865,
            "bert": 0.7825441664457321
        },
        "Whole dataset (Enrico)": {
            "bart": -6.265662951469421,
            "bert": 0.9574611139297485
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.5662918829917905,
            "bert": 0.8392027658224106
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.341092939376831,
            "bert": 0.8642957943677902
        },
        "Whole dataset (GQA)": {
            "bart": -3.663322002887726,
            "bert": 0.9922891741991043
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.863775520324707,
            "bert": 0.795601156949997
        },
        "Whole dataset (INAT)": {
            "bart": -6.29991349697113,
            "bert": 0.7923721206188202
        },
        "Whole dataset (IRFL)": {
            "bart": -4.173749152421951,
            "bert": 0.9986769580841064
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9462947630882264,
            "bert": 0.8492695790529251
        },
        "Whole dataset (Memotion)": {
            "bart": -4.584451220035553,
            "bert": 0.8955384773015976
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8438804346323012,
            "bert": 0.8719532477855683
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.9258691024780275,
            "bert": 0.8335955786705017
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5296913158893584,
            "bert": 0.880409631729126
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1308604210615156,
            "bert": 0.9452993851900101
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.288323929905891,
            "bert": 0.8250424885749816
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.825806604623795,
            "bert": 0.9139457958936691
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.713061730861664,
            "bert": 0.9233826905488968
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.249593336582183,
            "bert": 0.85623794734478
        },
        "Whole dataset (Slake)": {
            "bart": -3.8722607243061065,
            "bert": 0.9939968144893646
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.15375688791275,
            "bert": 0.8450413846969604
        },
        "Whole dataset (VCR)": {
            "bart": -3.3418113493919375,
            "bert": 0.9138160753250122
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.606894652247429,
            "bert": 0.906882483959198
        },
        "Whole dataset (VQA)": {
            "bart": -4.481736428737641,
            "bert": 0.9727840054035187
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.643145841360092,
            "bert": 0.9423311394453049
        },
        "Whole dataset (Winoground)": {
            "bart": -4.139468406438827,
            "bert": 0.9980017507076263
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9823008849557522,
            "rec": 0.75,
            "f1": 0.8505747126436781
        },
        "popular (POPE)": {
            "acc": 0.8306188925081434,
            "prec": 0.9069767441860465,
            "rec": 0.7452229299363057,
            "f1": 0.8181818181818181
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9905660377358491,
            "rec": 0.7954545454545454,
            "f1": 0.8823529411764706
        }
    },
    "one-stage+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7048827704882771
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6786427145708582
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.548306148055207
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.778
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6052308949734369
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.35591836734693877
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5586206896551724
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.373015873015873
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4347249834327369
        },
        "Instance Location (SEED_2)": {
            "acc": 0.621676891615542
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7010309278350515
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7590246991766941
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7083560895685418
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2878787878787879
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5489021956087824
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5007610350076104
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5487364620938628
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2844574780058651
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5660377358490566
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7757575757575758
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.628140703517588
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.6970588235294117,
            "prec": 0.6283524904214559,
            "rec": 0.9647058823529412,
            "f1": 0.7610208816705336
        },
        "posters (MME)": {
            "acc": 0.8469387755102041,
            "prec": 0.8109756097560976,
            "rec": 0.9047619047619048,
            "f1": 0.8553054662379421
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.5918367346938775,
            "rec": 0.9666666666666667,
            "f1": 0.7341772151898733
        },
        "scene (MME)": {
            "acc": 0.83,
            "prec": 0.7796610169491526,
            "rec": 0.92,
            "f1": 0.8440366972477064
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6857142857142857,
            "prec": 0.6326530612244898,
            "rec": 0.8857142857142857,
            "f1": 0.7380952380952381
        },
        "artwork (MME)": {
            "acc": 0.6025,
            "prec": 0.5590778097982709,
            "rec": 0.97,
            "f1": 0.7093235831809873
        },
        "landmark (MME)": {
            "acc": 0.82,
            "prec": 0.7689075630252101,
            "rec": 0.915,
            "f1": 0.8356164383561645
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 0.5483870967741935,
            "rec": 0.85,
            "f1": 0.6666666666666665
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9090909090909091,
            "rec": 1.0,
            "f1": 0.9523809523809523
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.6976744186046512,
            "rec": 1.0,
            "f1": 0.8219178082191781
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8023255813953488
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5714285714285714
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5923076923076923
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4860335195530726
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.702020202020202
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7628205128205128
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5177304964539007
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.81
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.819078947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7028301886792453
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7575757575757576
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.41333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5555555555555556
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6076923076923076
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6301369863013698
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6595744680851063
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5418994413407822
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8308080808080808
        },
        "ocr (MMBench_EN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5602836879432624
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8113207547169812
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.49333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9214285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.6
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.4090909090909091
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.7972027972027972
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.14516129032258066
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.0975609756097561
        },
        "Geography (ScienceQA)": {
            "acc": 0.7619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4727272727272727
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.9113924050632911
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.9655172413793104
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6197183098591549
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.5850253807106599
        },
        "3D Distance (CVBench)": {
            "acc": 0.6116666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.6876923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.77
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7552573680877686,
            "bert": 0.7859595477581024
        },
        "Whole dataset (Enrico)": {
            "bart": -5.541119629144669,
            "bert": 0.9856661891937256
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.149963406324386,
            "bert": 0.9934467750787735
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3489467942714692,
            "bert": 0.8636524623632431
        },
        "Whole dataset (GQA)": {
            "bart": -3.5117501163482667,
            "bert": 0.9923127657175064
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.023345332145691,
            "bert": 0.7931820231676102
        },
        "Whole dataset (INAT)": {
            "bart": -6.395168271064758,
            "bert": 0.7905559957027435
        },
        "Whole dataset (IRFL)": {
            "bart": -4.311559835672378,
            "bert": 0.998641402721405
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.937586188316345,
            "bert": 0.8488044822216034
        },
        "Whole dataset (Memotion)": {
            "bart": -4.005369510650635,
            "bert": 0.9052341705560685
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.338484680056572,
            "bert": 0.8952626985311508
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.688155130147934,
            "bert": 0.9741211652755737
        },
        "Whole dataset (NLVR)": {
            "bart": -3.684066433906555,
            "bert": 0.9992349421977997
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.383678846359253,
            "bert": 0.9992959403991699
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.459947783946991,
            "bert": 0.881473776102066
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1964533704519273,
            "bert": 0.9444975960254669
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.387137078642845,
            "bert": 0.8530699342489243
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.029993324279785,
            "bert": 0.9133426856994629
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.209208096265793,
            "bert": 0.9059617197513581
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.250408434867859,
            "bert": 0.8572002345323563
        },
        "Whole dataset (Slake)": {
            "bart": -3.9229157853126524,
            "bert": 0.9961619132757187
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.205680124759674,
            "bert": 0.8847266352176666
        },
        "Whole dataset (VCR)": {
            "bart": -3.114722710251808,
            "bert": 0.9249791449308395
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.608989378809929,
            "bert": 0.9064701706171036
        },
        "Whole dataset (VQA)": {
            "bart": -4.277367776632309,
            "bert": 0.9723684394359589
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.858606879711151,
            "bert": 0.9404886597394944
        },
        "Whole dataset (Winoground)": {
            "bart": -4.400161164999008,
            "bert": 0.9979340517520905
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9824561403508771,
            "rec": 0.7567567567567568,
            "f1": 0.8549618320610687
        },
        "popular (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.9365079365079365,
            "rec": 0.7515923566878981,
            "f1": 0.8339222614840989
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9905660377358491,
            "rec": 0.7954545454545454,
            "f1": 0.8823529411764706
        }
    },
    "full-ft-multi-stage+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6425037642503765
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6746506986027944
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5094102885821832
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.22954091816367264
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.87
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5451573355128729
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.34530612244897957
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6206896551724138
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2390873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4148442677269715
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5511247443762781
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7197593413552882
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.661387220098307
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.29545454545454547
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5249500998003992
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.502283105022831
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5595667870036101
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34017595307917886
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7552870090634441
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.39166666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.42138364779874216
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.8636363636363636
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6381909547738693
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4897959183673469
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.7235294117647059,
            "prec": 0.8333333333333334,
            "rec": 0.5588235294117647,
            "f1": 0.6690140845070423
        },
        "posters (MME)": {
            "acc": 0.7857142857142857,
            "prec": 0.9772727272727273,
            "rec": 0.5850340136054422,
            "f1": 0.7319148936170213
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "scene (MME)": {
            "acc": 0.865,
            "prec": 0.8882978723404256,
            "rec": 0.835,
            "f1": 0.8608247422680413
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6555555555555556,
            "rec": 0.8428571428571429,
            "f1": 0.7375
        },
        "artwork (MME)": {
            "acc": 0.695,
            "prec": 0.6638655462184874,
            "rec": 0.79,
            "f1": 0.7214611872146119
        },
        "landmark (MME)": {
            "acc": 0.8875,
            "prec": 0.863849765258216,
            "rec": 0.92,
            "f1": 0.8910411622276031
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 1.0,
            "rec": 0.9,
            "f1": 0.9473684210526316
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.2222222222222222,
            "rec": 0.1,
            "f1": 0.13793103448275865
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7941176470588235,
            "rec": 0.9,
            "f1": 0.84375
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7777777777777778,
            "rec": 0.9333333333333333,
            "f1": 0.8484848484848485
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.72,
            "rec": 0.9,
            "f1": 0.7999999999999999
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7616279069767442
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.49206349206349204
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5769230769230769
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4246575342465753
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.4397163120567376
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.39664804469273746
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7904040404040404
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6666666666666666
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.38652482269503546
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9147727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7688679245283019
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7727272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5968253968253968
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.47692307692307695
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5342465753424658
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5886524822695035
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4245810055865922
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8790697674418605
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8939393939393939
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6923076923076923
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8388157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8915094339622641
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.875
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.21721311475409835
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2222222222222222
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12745098039215685
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.8251748251748252
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.19607843137254902
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5344827586206896
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.35555555555555557
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.8125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.5869565217391305
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4788732394366197
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5789473684210527
        },
        "2D Count (CVBench)": {
            "acc": 0.5076142131979695
        },
        "3D Distance (CVBench)": {
            "acc": 0.5133333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6
        },
        "3D Depth (CVBench)": {
            "acc": 0.6816666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.822092542648315,
            "bert": 0.7766635173559189
        },
        "Whole dataset (Enrico)": {
            "bart": -6.078570140600204,
            "bert": 0.9637039929628373
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -3.9525285184383394,
            "bert": 0.9858267724514007
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2070958113670347,
            "bert": 0.8808627474308014
        },
        "Whole dataset (GQA)": {
            "bart": -4.115587610006332,
            "bert": 0.9908638525009156
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.957101948261261,
            "bert": 0.8033868461847306
        },
        "Whole dataset (INAT)": {
            "bart": -6.310680208206176,
            "bert": 0.7913929200172425
        },
        "Whole dataset (IRFL)": {
            "bart": -4.191546609401703,
            "bert": 0.9986739057302475
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9864398765563966,
            "bert": 0.8502797991037369
        },
        "Whole dataset (Memotion)": {
            "bart": -4.391203277111053,
            "bert": 0.9035870981216431
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7586195176839827,
            "bert": 0.8725004166364669
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.978924441337585,
            "bert": 0.8237582343816757
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.242663261294365,
            "bert": 0.8980083405971527
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2405041152238847,
            "bert": 0.942826830148697
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.969571631550789,
            "bert": 0.8262510007619858
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.778304417133331,
            "bert": 0.9069909220933914
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.587944155931472,
            "bert": 0.9219914990663528
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.11650321483612,
            "bert": 0.8589122951030731
        },
        "Whole dataset (Slake)": {
            "bart": -3.8119478249549865,
            "bert": 0.9941893374919891
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.392409327626228,
            "bert": 0.9079735881090164
        },
        "Whole dataset (VCR)": {
            "bart": -3.519074434041977,
            "bert": 0.9108783650398254
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7137268620729444,
            "bert": 0.9065329730510712
        },
        "Whole dataset (VQA)": {
            "bart": -4.4518499910831455,
            "bert": 0.9732109051942825
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.595505146980286,
            "bert": 0.9403593492507935
        },
        "Whole dataset (Winoground)": {
            "bart": -4.265219330787659,
            "bert": 0.9979699665307998
        },
        "random (POPE)": {
            "acc": 0.8175895765472313,
            "prec": 1.0,
            "rec": 0.6216216216216216,
            "f1": 0.7666666666666667
        },
        "popular (POPE)": {
            "acc": 0.8143322475570033,
            "prec": 0.9545454545454546,
            "rec": 0.6687898089171974,
            "f1": 0.7865168539325843
        },
        "adversarial (POPE)": {
            "acc": 0.8391608391608392,
            "prec": 0.967391304347826,
            "rec": 0.6742424242424242,
            "f1": 0.7946428571428571
        }
    },
    "full-ft-one-stage+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6158313615831361
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6586826347305389
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5219573400250941
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2435129740518962
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.788
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5553739272578668
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3485714285714286
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6873563218390805
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2371031746031746
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3817097415506958
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5623721881390593
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6288659793814433
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7127929069031033
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6641179683233206
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.21212121212121213
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5049900199600799
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4733637747336377
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5342960288808665
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.32649071358748777
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7522658610271903
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.425
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7454545454545455
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6582914572864321
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6323529411764706,
            "prec": 0.6086956521739131,
            "rec": 0.7411764705882353,
            "f1": 0.6684350132625996
        },
        "posters (MME)": {
            "acc": 0.7891156462585034,
            "prec": 0.8195488721804511,
            "rec": 0.7414965986394558,
            "f1": 0.7785714285714286
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6666666666666666,
            "rec": 0.9333333333333333,
            "f1": 0.7777777777777778
        },
        "scene (MME)": {
            "acc": 0.8725,
            "prec": 0.8860103626943006,
            "rec": 0.855,
            "f1": 0.8702290076335878
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.65625,
            "rec": 0.9,
            "f1": 0.7590361445783134
        },
        "artwork (MME)": {
            "acc": 0.665,
            "prec": 0.6187050359712231,
            "rec": 0.86,
            "f1": 0.7196652719665272
        },
        "landmark (MME)": {
            "acc": 0.8475,
            "prec": 0.7883817427385892,
            "rec": 0.95,
            "f1": 0.8616780045351472
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.42105263157894735,
            "rec": 0.4,
            "f1": 0.41025641025641024
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7297297297297297,
            "rec": 0.9,
            "f1": 0.8059701492537312
        },
        "color (MME)": {
            "acc": 0.8,
            "prec": 0.7647058823529411,
            "rec": 0.8666666666666667,
            "f1": 0.8125
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.95,
            "f1": 0.6551724137931034
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.4222222222222222
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4611872146118721
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5815602836879432
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.44692737430167595
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.941031941031941
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6540404040404041
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6474358974358975
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.39361702127659576
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.43617021276595747
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7169811320754716
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.696969696969697
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8571428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.49523809523809526
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.589041095890411
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.48044692737430167
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8744186046511628
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9533169533169533
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7727272727272727
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6858974358974359
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4326241134751773
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8349056603773585
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8598484848484849
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26639344262295084
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3068181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9775641025641025
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.7412587412587412
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6619718309859155
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.6052631578947368
        },
        "2D Count (CVBench)": {
            "acc": 0.5114213197969543
        },
        "3D Distance (CVBench)": {
            "acc": 0.56
        },
        "2D Relation (CVBench)": {
            "acc": 0.5984615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.6483333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.808510355949402,
            "bert": 0.7764738041162491
        },
        "Whole dataset (Enrico)": {
            "bart": -6.89889417886734,
            "bert": 0.9338551670312881
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.776208398342132,
            "bert": 0.8264946156740188
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4072770142555235,
            "bert": 0.863361125588417
        },
        "Whole dataset (GQA)": {
            "bart": -3.9849694633483885,
            "bert": 0.993679216504097
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.297311692237854,
            "bert": 0.7980070668458938
        },
        "Whole dataset (INAT)": {
            "bart": -6.270155637264252,
            "bert": 0.7939588093757629
        },
        "Whole dataset (IRFL)": {
            "bart": -4.281118257045746,
            "bert": 0.9986534351110459
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.019512654542923,
            "bert": 0.8486627650260925
        },
        "Whole dataset (Memotion)": {
            "bart": -4.485709261894226,
            "bert": 0.8929925054311753
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8209468376636506,
            "bert": 0.8781955391168594
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.831359473466873,
            "bert": 0.8459035950899124
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5966404628753663,
            "bert": 0.8792689365148544
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.3736413890123367,
            "bert": 0.94152716755867
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.64682785153389,
            "bert": 0.8048004299402237
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.034746721982956,
            "bert": 0.9101098048686981
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.722325644493103,
            "bert": 0.9138827788829803
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.204734907150269,
            "bert": 0.85841222345829
        },
        "Whole dataset (Slake)": {
            "bart": -3.994999622106552,
            "bert": 0.9947088503837586
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.573393485546112,
            "bert": 0.8040330064296722
        },
        "Whole dataset (VCR)": {
            "bart": -3.3115464001893997,
            "bert": 0.9198473805189132
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6998565953969957,
            "bert": 0.9064973187446594
        },
        "Whole dataset (VQA)": {
            "bart": -4.28503126502037,
            "bert": 0.9753139370679855
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.666474483013153,
            "bert": 0.9406497949361801
        },
        "Whole dataset (Winoground)": {
            "bart": -4.39097025513649,
            "bert": 0.9979381823539734
        },
        "random (POPE)": {
            "acc": 0.8175895765472313,
            "prec": 1.0,
            "rec": 0.6216216216216216,
            "f1": 0.7666666666666667
        },
        "popular (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.940677966101695,
            "rec": 0.7070063694267515,
            "f1": 0.8072727272727273
        },
        "adversarial (POPE)": {
            "acc": 0.8496503496503497,
            "prec": 0.978494623655914,
            "rec": 0.6893939393939394,
            "f1": 0.8088888888888889
        }
    },
    "in1k-224px+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5558184555818455
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.469061876247505
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.4880803011292346
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25149700598802394
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.638
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.43073150796894155
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.32
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4091954022988506
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24801587301587302
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3499005964214712
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5071574642126789
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5876288659793815
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6466117796073464
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5472419442927362
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5189620758483033
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4520547945205479
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4151624548736462
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3069403714565005
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6706948640483383
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.45
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5031446540880503
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5242424242424243
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.49246231155778897
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.5088235294117647,
            "prec": 0.5107913669064749,
            "rec": 0.4176470588235294,
            "f1": 0.4595469255663431
        },
        "posters (MME)": {
            "acc": 0.5850340136054422,
            "prec": 0.6506024096385542,
            "rec": 0.3673469387755102,
            "f1": 0.4695652173913043
        },
        "position (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6086956521739131,
            "rec": 0.9333333333333333,
            "f1": 0.7368421052631579
        },
        "scene (MME)": {
            "acc": 0.8825,
            "prec": 0.9135135135135135,
            "rec": 0.845,
            "f1": 0.8779220779220779
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6571428571428571,
            "prec": 0.627906976744186,
            "rec": 0.7714285714285715,
            "f1": 0.6923076923076923
        },
        "artwork (MME)": {
            "acc": 0.62,
            "prec": 0.6008403361344538,
            "rec": 0.715,
            "f1": 0.6529680365296804
        },
        "landmark (MME)": {
            "acc": 0.7775,
            "prec": 0.7733990147783252,
            "rec": 0.785,
            "f1": 0.7791563275434242
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.6470588235294118,
            "rec": 0.55,
            "f1": 0.5945945945945946
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6666666666666666,
            "rec": 0.8666666666666667,
            "f1": 0.7536231884057971
        },
        "color (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6904761904761905,
            "rec": 0.9666666666666667,
            "f1": 0.8055555555555556
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5652173913043478,
            "rec": 0.65,
            "f1": 0.6046511627906976
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.75,
            "f1": 0.6
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.5523255813953488
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.41904761904761906
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4748858447488584
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.26256983240223464
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9090909090909091
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.5959595959595959
        },
        "ocr (MMBench_CN)": {
            "acc": 0.4358974358974359
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3546099290780142
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.64
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.8238636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.44680851063829785
        },
        "image_style (MMBench_CN)": {
            "acc": 0.5047169811320755
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.5757575757575758
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8571428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.6802325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5174603174603175
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5525114155251142
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.37988826815642457
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8604651162790697
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9213759213759214
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.6565656565656566
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5128205128205128
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.39361702127659576
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.63
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8026315789473685
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8409090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_EN)": {
            "acc": 0.589622641509434
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.6818181818181818
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8571428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.43333333333333335
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2336065573770492
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14215686274509803
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.6853146853146853
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Classification (ScienceQA)": {
            "acc": 0.8607594936708861
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6197183098591549
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.3629441624365482
        },
        "3D Distance (CVBench)": {
            "acc": 0.5183333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6076923076923076
        },
        "3D Depth (CVBench)": {
            "acc": 0.6583333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.878033218383789,
            "bert": 0.8193227410316467
        },
        "Whole dataset (Enrico)": {
            "bart": -7.116112966537475,
            "bert": 0.8390504252910614
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.170925235748291,
            "bert": 0.8204247236251831
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.5524834203720093,
            "bert": 0.8579579019546508
        },
        "Whole dataset (GQA)": {
            "bart": -4.182982006072998,
            "bert": 0.9936135917901993
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.0255984735488894,
            "bert": 0.793654636144638
        },
        "Whole dataset (INAT)": {
            "bart": -6.3597304165363315,
            "bert": 0.7941059243679046
        },
        "Whole dataset (IRFL)": {
            "bart": -4.367236692905426,
            "bert": 0.9986378139257431
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.03260064125061,
            "bert": 0.8472634983062745
        },
        "Whole dataset (Memotion)": {
            "bart": -4.453035681247711,
            "bert": 0.8981401741504669
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.824001268744469,
            "bert": 0.8706366258859635
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.005312414169311,
            "bert": 0.8424091112613677
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.7366510915756224,
            "bert": 0.8769469875097274
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.7052290868759155,
            "bert": 0.9385273587703705
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.132125368714332,
            "bert": 0.8432690221071243
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.091539982557297,
            "bert": 0.9103561103343963
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.952208821773529,
            "bert": 0.8863524281978608
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.621846833229065,
            "bert": 0.8452177125215531
        },
        "Whole dataset (Slake)": {
            "bart": -4.110617901086807,
            "bert": 0.9960939937829971
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.616796462535858,
            "bert": 0.8115099656581879
        },
        "Whole dataset (VCR)": {
            "bart": -3.4330717754364013,
            "bert": 0.9103209006786347
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7685163325071334,
            "bert": 0.9061718881130219
        },
        "Whole dataset (VQA)": {
            "bart": -4.945030614137649,
            "bert": 0.9749206805229187
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.601173663139344,
            "bert": 0.938905634880066
        },
        "Whole dataset (Winoground)": {
            "bart": -4.663138409852982,
            "bert": 0.9978719210624695
        },
        "random (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.9391304347826087,
            "rec": 0.7297297297297297,
            "f1": 0.8212927756653993
        },
        "popular (POPE)": {
            "acc": 0.8078175895765473,
            "prec": 0.8450704225352113,
            "rec": 0.7643312101910829,
            "f1": 0.802675585284281
        },
        "adversarial (POPE)": {
            "acc": 0.8216783216783217,
            "prec": 0.8403361344537815,
            "rec": 0.7575757575757576,
            "f1": 0.796812749003984
        }
    },
    "dinov2-224px+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.4628952462895246
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.44510978043912175
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5081555834378921
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.668
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4793624846751124
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3616326530612245
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3839080459770115
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24305555555555555
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3770709078860172
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5245398773006135
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5567010309278351
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6602279924002533
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5854724194429274
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.32575757575757575
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.46506986027944114
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.45662100456621
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4584837545126354
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.29716520039100686
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.649546827794562
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.425
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.44025157232704404
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5545454545454546
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5879396984924623
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.5735294117647058,
            "prec": 0.5984251968503937,
            "rec": 0.4470588235294118,
            "f1": 0.5117845117845119
        },
        "posters (MME)": {
            "acc": 0.6190476190476191,
            "prec": 0.7160493827160493,
            "rec": 0.3945578231292517,
            "f1": 0.5087719298245613
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9186046511627907,
            "rec": 0.79,
            "f1": 0.8494623655913979
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6266666666666667,
            "rec": 0.6714285714285714,
            "f1": 0.6482758620689655
        },
        "artwork (MME)": {
            "acc": 0.6575,
            "prec": 0.6451612903225806,
            "rec": 0.7,
            "f1": 0.6714628297362109
        },
        "landmark (MME)": {
            "acc": 0.785,
            "prec": 0.8607594936708861,
            "rec": 0.68,
            "f1": 0.7597765363128492
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.6666666666666666,
            "rec": 0.2,
            "f1": 0.30769230769230765
        },
        "existence (MME)": {
            "acc": 0.9,
            "prec": 1.0,
            "rec": 0.8,
            "f1": 0.888888888888889
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.7586206896551724,
            "rec": 0.7333333333333333,
            "f1": 0.7457627118644068
        },
        "color (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5714285714285714,
            "rec": 0.6666666666666666,
            "f1": 0.6153846153846153
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.6153846153846154,
            "rec": 0.8,
            "f1": 0.6956521739130435
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5172413793103449,
            "rec": 0.75,
            "f1": 0.6122448979591838
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.5755813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.3873015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4461538461538462
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4840182648401826
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5177304964539007
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.30726256983240224
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9238329238329238
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.5404040404040404
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5961538461538461
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.34397163120567376
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.64
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.8465909090909091
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.5566037735849056
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.5378787878787878
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.37333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8571428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.7325581395348837
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5555555555555556
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.46923076923076923
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.593607305936073
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6524822695035462
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.40782122905027934
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8697674418604651
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9385749385749386
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.6111111111111112
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5897435897435898
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3900709219858156
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.65
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8486842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.8920454545454546
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.6462264150943396
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.6060606060606061
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.43333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.85
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.7
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1323529411764706
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.7132867132867133
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.2903225806451613
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.5
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Classification (ScienceQA)": {
            "acc": 0.7974683544303798
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6619718309859155
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.4263959390862944
        },
        "3D Distance (CVBench)": {
            "acc": 0.55
        },
        "2D Relation (CVBench)": {
            "acc": 0.583076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.6633333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.693779029846191,
            "bert": 0.817546455860138
        },
        "Whole dataset (Enrico)": {
            "bart": -6.955071070194244,
            "bert": 0.8328286600112915
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.463257827758789,
            "bert": 0.8715290236473083
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4718056869506837,
            "bert": 0.8602701115608216
        },
        "Whole dataset (GQA)": {
            "bart": -4.0900907289981845,
            "bert": 0.993662651181221
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.237305088043213,
            "bert": 0.7998519450426101
        },
        "Whole dataset (INAT)": {
            "bart": -6.356785092353821,
            "bert": 0.7952658218145371
        },
        "Whole dataset (IRFL)": {
            "bart": -4.352892447710037,
            "bert": 0.9986360168457031
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.045914077758789,
            "bert": 0.848460305929184
        },
        "Whole dataset (Memotion)": {
            "bart": -4.520481667518616,
            "bert": 0.8971269994974136
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8468275278806687,
            "bert": 0.8739843386411666
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.1710153841972355,
            "bert": 0.9193014061450958
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.705955140590668,
            "bert": 0.8756323885917664
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.6331369960308075,
            "bert": 0.9397036767005921
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.9282720291614535,
            "bert": 0.8516278904676438
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.233823193311691,
            "bert": 0.9139405000209808
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.35591106057167,
            "bert": 0.8891116172075272
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.556268610954285,
            "bert": 0.8443280225992202
        },
        "Whole dataset (Slake)": {
            "bart": -4.1834690427780155,
            "bert": 0.9954031360149384
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.268200034499168,
            "bert": 0.8418621295690536
        },
        "Whole dataset (VCR)": {
            "bart": -3.352753735780716,
            "bert": 0.9189095705747604
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.9372446423769,
            "bert": 0.9076897668838501
        },
        "Whole dataset (VQA)": {
            "bart": -5.219871495962143,
            "bert": 0.96635373711586
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.796836125850677,
            "bert": 0.940332236289978
        },
        "Whole dataset (Winoground)": {
            "bart": -4.708508596420288,
            "bert": 0.9978564769029618
        },
        "random (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.940677966101695,
            "rec": 0.75,
            "f1": 0.8345864661654134
        },
        "popular (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.8985507246376812,
            "rec": 0.7898089171974523,
            "f1": 0.840677966101695
        },
        "adversarial (POPE)": {
            "acc": 0.8461538461538461,
            "prec": 0.860655737704918,
            "rec": 0.7954545454545454,
            "f1": 0.8267716535433071
        }
    },
    "clip-224px+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6629382662938267
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6726546906187625
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5244667503136763
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.792
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5451573355128729
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.363265306122449
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5770114942528736
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2390873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3903247183565275
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6053169734151329
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7295756808106396
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6695794647733478
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2803030303030303
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.4930139720558882
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4931506849315068
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.48736462093862815
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3479960899315738
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7552870090634441
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.475
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4716981132075472
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7636363636363637
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.40816326530612246
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6529411764705882,
            "prec": 0.9642857142857143,
            "rec": 0.3176470588235294,
            "f1": 0.47787610619469023
        },
        "posters (MME)": {
            "acc": 0.8027210884353742,
            "prec": 0.9009009009009009,
            "rec": 0.6802721088435374,
            "f1": 0.7751937984496123
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.8775,
            "prec": 0.9364161849710982,
            "rec": 0.81,
            "f1": 0.8686327077747988
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7285714285714285,
            "prec": 0.6904761904761905,
            "rec": 0.8285714285714286,
            "f1": 0.7532467532467533
        },
        "artwork (MME)": {
            "acc": 0.6975,
            "prec": 0.6680851063829787,
            "rec": 0.785,
            "f1": 0.7218390804597702
        },
        "landmark (MME)": {
            "acc": 0.79,
            "prec": 0.8452380952380952,
            "rec": 0.71,
            "f1": 0.7717391304347825
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 1.0,
            "rec": 0.15,
            "f1": 0.2608695652173913
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.4117647058823529,
            "rec": 0.35,
            "f1": 0.37837837837837834
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.7647058823529411,
            "rec": 0.8666666666666667,
            "f1": 0.8125
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.7083333333333334,
            "rec": 0.85,
            "f1": 0.7727272727272727
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.85,
            "f1": 0.6296296296296295
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8372093023255814
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5015873015873016
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5153846153846153
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5114155251141552
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9255813953488372
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6262626262626263
        },
        "ocr (MMBench_CN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4124293785310734
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.43617021276595747
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6981132075471698
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7537878787878788
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.41333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8785714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8837209302325582
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6031746031746031
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5538461538461539
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6073059360730594
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.441340782122905
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9877149877149877
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8005050505050505
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3615819209039548
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4716312056737589
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7688679245283019
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8409090909090909
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.37333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.5
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3068181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9807692307692307
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.8111888111888111
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4878048780487805
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8043478260869565
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Classification (ScienceQA)": {
            "acc": 0.8227848101265823
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6901408450704225
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.47588832487309646
        },
        "3D Distance (CVBench)": {
            "acc": 0.545
        },
        "2D Relation (CVBench)": {
            "acc": 0.6338461538461538
        },
        "3D Depth (CVBench)": {
            "acc": 0.63
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.879561796188354,
            "bert": 0.7639991021156312
        },
        "Whole dataset (Enrico)": {
            "bart": -6.163469045162201,
            "bert": 0.9838842880725861
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.468422756195069,
            "bert": 0.8493034493923187
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.352734009027481,
            "bert": 0.8646292906999588
        },
        "Whole dataset (GQA)": {
            "bart": -4.029323505163193,
            "bert": 0.9922283881902695
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.248306474685669,
            "bert": 0.798592922091484
        },
        "Whole dataset (INAT)": {
            "bart": -6.330341694355011,
            "bert": 0.7930489349365234
        },
        "Whole dataset (IRFL)": {
            "bart": -4.278249408006668,
            "bert": 0.9986530756950378
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9362644970417024,
            "bert": 0.8499186831712723
        },
        "Whole dataset (Memotion)": {
            "bart": -4.079385925531387,
            "bert": 0.8898676258325576
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.772367450594902,
            "bert": 0.8802545583248138
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.305143004655838,
            "bert": 0.9157260519266128
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5199540638923645,
            "bert": 0.8792250156402588
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2284210115671157,
            "bert": 0.9406293356418609
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.30181414604187,
            "bert": 0.829813386797905
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.953205214738846,
            "bert": 0.9119619983434677
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.83584998011589,
            "bert": 0.9179884403944015
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.208217771053314,
            "bert": 0.857662398815155
        },
        "Whole dataset (Slake)": {
            "bart": -3.638055782318115,
            "bert": 0.9961816674470901
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.07647264957428,
            "bert": 0.841683823466301
        },
        "Whole dataset (VCR)": {
            "bart": -3.2746268579363824,
            "bert": 0.9217789566516876
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.718300408124924,
            "bert": 0.9057879143953323
        },
        "Whole dataset (VQA)": {
            "bart": -4.341498135328293,
            "bert": 0.9736464428901672
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.562968797683716,
            "bert": 0.9417562407255172
        },
        "Whole dataset (Winoground)": {
            "bart": -4.516721179485321,
            "bert": 0.9979063981771469
        },
        "random (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9553571428571429,
            "rec": 0.722972972972973,
            "f1": 0.823076923076923
        },
        "popular (POPE)": {
            "acc": 0.8599348534201955,
            "prec": 0.9253731343283582,
            "rec": 0.7898089171974523,
            "f1": 0.8522336769759451
        },
        "adversarial (POPE)": {
            "acc": 0.8636363636363636,
            "prec": 0.926605504587156,
            "rec": 0.7651515151515151,
            "f1": 0.8381742738589212
        }
    },
    "siglip-224px+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6734781673478167
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6666666666666666
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5307402760351317
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.29141716566866266
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.778
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5929709848794442
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.33877551020408164
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5885057471264368
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24107142857142858
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4300861497680583
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6124744376278118
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7349588347055098
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.679410158383397
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.32575757575757575
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5169660678642715
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4946727549467275
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5667870036101083
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3509286412512219
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7643504531722054
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.425
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4339622641509434
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7484848484848485
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6130653266331658
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.6088235294117647,
            "prec": 0.9512195121951219,
            "rec": 0.22941176470588234,
            "f1": 0.3696682464454976
        },
        "posters (MME)": {
            "acc": 0.7789115646258503,
            "prec": 0.9555555555555556,
            "rec": 0.5850340136054422,
            "f1": 0.7257383966244726
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6744186046511628,
            "rec": 0.9666666666666667,
            "f1": 0.7945205479452055
        },
        "scene (MME)": {
            "acc": 0.85,
            "prec": 0.926829268292683,
            "rec": 0.76,
            "f1": 0.8351648351648352
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.6428571428571429,
            "rec": 0.7714285714285715,
            "f1": 0.7012987012987013
        },
        "artwork (MME)": {
            "acc": 0.67,
            "prec": 0.6808510638297872,
            "rec": 0.64,
            "f1": 0.6597938144329897
        },
        "landmark (MME)": {
            "acc": 0.81,
            "prec": 0.8827160493827161,
            "rec": 0.715,
            "f1": 0.7900552486187846
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.25,
            "rec": 0.1,
            "f1": 0.14285714285714288
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7741935483870968,
            "rec": 0.8,
            "f1": 0.7868852459016393
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.6071428571428571,
            "rec": 0.85,
            "f1": 0.7083333333333333
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4838709677419355,
            "rec": 0.75,
            "f1": 0.5882352941176471
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4520547945205479
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5815602836879432
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4692737430167598
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9441860465116279
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9459459459459459
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7146464646464646
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7243589743589743
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3900709219858156
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.785
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5638297872340425
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8068181818181818
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6095238095238096
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6301369863013698
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.48044692737430167
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8308080808080808
        },
        "ocr (MMBench_EN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4219858156028369
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.868421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7971698113207547
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.875
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3333333333333333
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1411764705882353
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7945205479452054
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.8461538461538461
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.5952380952380952
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.4782608695652174
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.8227848101265823
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5070422535211268
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.567258883248731
        },
        "3D Distance (CVBench)": {
            "acc": 0.5283333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6292307692307693
        },
        "3D Depth (CVBench)": {
            "acc": 0.66
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.615273616313934,
            "bert": 0.8054573553800582
        },
        "Whole dataset (Enrico)": {
            "bart": -6.686993479728699,
            "bert": 0.9740689516067504
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.959746890068054,
            "bert": 0.8182967352867127
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.26641560792923,
            "bert": 0.8689998573064804
        },
        "Whole dataset (GQA)": {
            "bart": -4.064654027223587,
            "bert": 0.9936636364459992
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.321001806259155,
            "bert": 0.7978811156749726
        },
        "Whole dataset (INAT)": {
            "bart": -6.269424288272858,
            "bert": 0.7926265954971313
        },
        "Whole dataset (IRFL)": {
            "bart": -4.179486850500107,
            "bert": 0.9986776769161224
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9555451536178587,
            "bert": 0.8488286256790161
        },
        "Whole dataset (Memotion)": {
            "bart": -4.503103036880493,
            "bert": 0.897072901725769
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.803224835395813,
            "bert": 0.8778974032402038
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.229038450717926,
            "bert": 0.9185348248481751
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4681949365139007,
            "bert": 0.8838418382406235
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.4308650028705596,
            "bert": 0.937655794620514
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.918702909350396,
            "bert": 0.8522843706607819
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.215652030706406,
            "bert": 0.9128578215837478
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.6200849723815915,
            "bert": 0.9162725156545639
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.122302422523498,
            "bert": 0.857017679810524
        },
        "Whole dataset (Slake)": {
            "bart": -4.018368308544159,
            "bert": 0.9953841143846511
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.432296333312988,
            "bert": 0.8299952298402786
        },
        "Whole dataset (VCR)": {
            "bart": -3.2673057743906977,
            "bert": 0.9177427482604981
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.620678598284721,
            "bert": 0.9071272796392441
        },
        "Whole dataset (VQA)": {
            "bart": -4.2152410042285915,
            "bert": 0.9752697521448135
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.71982367515564,
            "bert": 0.9407166850566864
        },
        "Whole dataset (Winoground)": {
            "bart": -4.382363708019256,
            "bert": 0.9979371041059494
        },
        "random (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.9642857142857143,
            "rec": 0.7297297297297297,
            "f1": 0.8307692307692307
        },
        "popular (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.8939393939393939,
            "rec": 0.7515923566878981,
            "f1": 0.8166089965397925
        },
        "adversarial (POPE)": {
            "acc": 0.8741258741258742,
            "prec": 0.9705882352941176,
            "rec": 0.75,
            "f1": 0.846153846153846
        }
    },
    "clip-336px-resize-crop+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6812217681221768
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6626746506986028
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5125470514429109
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.786
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5676338373518595
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.38448979591836735
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4827586206896552
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2569444444444444
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4042412193505633
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6165644171779141
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7010309278350515
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7384420519316023
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6941561987984708
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5329341317365269
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5053272450532724
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5415162454873647
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3567937438905181
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7764350453172205
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.44166666666666665
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4339622641509434
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7818181818181819
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6130653266331658
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.4489795918367347
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.18518518518518517
        },
        "celebrity (MME)": {
            "acc": 0.8029411764705883,
            "prec": 0.8551724137931035,
            "rec": 0.7294117647058823,
            "f1": 0.7873015873015873
        },
        "posters (MME)": {
            "acc": 0.8163265306122449,
            "prec": 0.8842975206611571,
            "rec": 0.7278911564625851,
            "f1": 0.7985074626865671
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6666666666666666,
            "rec": 0.8,
            "f1": 0.7272727272727272
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.9289940828402367,
            "rec": 0.785,
            "f1": 0.8509485094850948
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.6785714285714286,
            "rec": 0.8142857142857143,
            "f1": 0.7402597402597403
        },
        "artwork (MME)": {
            "acc": 0.69,
            "prec": 0.6496062992125984,
            "rec": 0.825,
            "f1": 0.7268722466960352
        },
        "landmark (MME)": {
            "acc": 0.8475,
            "prec": 0.8601036269430051,
            "rec": 0.83,
            "f1": 0.8447837150127224
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.375,
            "rec": 0.3,
            "f1": 0.33333333333333326
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7941176470588235,
            "rec": 0.9,
            "f1": 0.84375
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6666666666666666,
            "rec": 0.9,
            "f1": 0.7659574468085106
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.95,
            "f1": 0.6551724137931034
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8895348837209303
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5555555555555556
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.46923076923076923
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4657534246575342
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4692737430167598
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9255813953488372
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6868686868686869
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7243589743589743
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.40070921985815605
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5638297872340425
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7783018867924528
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7727272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.41333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.580952380952381
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5307692307692308
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5616438356164384
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6312056737588653
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.48044692737430167
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9255813953488372
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9803439803439803
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7777777777777778
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7435897435897436
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.450354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8371212121212122
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.35333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8857142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.7
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14215686274509803
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9743589743589743
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.8111888111888111
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5490196078431373
        },
        "Classification (ScienceQA)": {
            "acc": 0.8227848101265823
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6901408450704225
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.4796954314720812
        },
        "3D Distance (CVBench)": {
            "acc": 0.5566666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.6015384615384616
        },
        "3D Depth (CVBench)": {
            "acc": 0.6683333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.893283317089081,
            "bert": 0.7863594669103623
        },
        "Whole dataset (Enrico)": {
            "bart": -6.723132734298706,
            "bert": 0.9440724980831147
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.621484739780426,
            "bert": 0.8357334315776825
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2929923152923584,
            "bert": 0.8652878308296204
        },
        "Whole dataset (GQA)": {
            "bart": -4.033250484466553,
            "bert": 0.9936640185117721
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.687280278205872,
            "bert": 0.795672265291214
        },
        "Whole dataset (INAT)": {
            "bart": -6.285016334056854,
            "bert": 0.7936620324850082
        },
        "Whole dataset (IRFL)": {
            "bart": -4.403415969610214,
            "bert": 0.9986265003681183
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.956240417957306,
            "bert": 0.8499654376506806
        },
        "Whole dataset (Memotion)": {
            "bart": -4.495512247085571,
            "bert": 0.8938209438323974
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8617503482103346,
            "bert": 0.8729415506124496
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.986745958328247,
            "bert": 0.8325123858451843
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.509779088497162,
            "bert": 0.8790266287326812
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2228088539838793,
            "bert": 0.9429554933309555
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.372350792884827,
            "bert": 0.8182675957679748
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.840511937141418,
            "bert": 0.9101627296209336
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.74588958144188,
            "bert": 0.9182370054721832
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.255833580493927,
            "bert": 0.854152318239212
        },
        "Whole dataset (Slake)": {
            "bart": -3.9121411907672883,
            "bert": 0.9939923852682113
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.044032015800476,
            "bert": 0.8599614381790162
        },
        "Whole dataset (VCR)": {
            "bart": -3.27898311406374,
            "bert": 0.917200500369072
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.701127604842186,
            "bert": 0.9049108904600144
        },
        "Whole dataset (VQA)": {
            "bart": -4.203372775316239,
            "bert": 0.9752895683050156
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.55778558254242,
            "bert": 0.9418630135059357
        },
        "Whole dataset (Winoground)": {
            "bart": -4.1543970143795015,
            "bert": 0.9979983389377594
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.990990990990991,
            "rec": 0.7432432432432432,
            "f1": 0.8494208494208495
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.9291338582677166,
            "rec": 0.7515923566878981,
            "f1": 0.8309859154929579
        },
        "adversarial (POPE)": {
            "acc": 0.8426573426573427,
            "prec": 0.9306930693069307,
            "rec": 0.7121212121212122,
            "f1": 0.8068669527896996
        }
    },
    "clip-336px-resize-naive+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6936975693697569
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6387225548902196
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5288582183186951
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.782
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5905190028606457
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.37551020408163266
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6413793103448275
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2371031746031746
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4135188866799205
        },
        "Instance Location (SEED_2)": {
            "acc": 0.623721881390593
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7463584547181761
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7176406335335882
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5528942115768463
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4946727549467275
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5451263537906137
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3460410557184751
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7734138972809668
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.45
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4591194968553459
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7272727272727273
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.628140703517588
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.42857142857142855
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.7029411764705882,
            "prec": 0.9058823529411765,
            "rec": 0.45294117647058824,
            "f1": 0.6039215686274509
        },
        "posters (MME)": {
            "acc": 0.8061224489795918,
            "prec": 0.9326923076923077,
            "rec": 0.6598639455782312,
            "f1": 0.7729083665338646
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.8936170212765957,
            "rec": 0.84,
            "f1": 0.8659793814432989
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7214285714285714,
            "prec": 0.6823529411764706,
            "rec": 0.8285714285714286,
            "f1": 0.7483870967741937
        },
        "artwork (MME)": {
            "acc": 0.71,
            "prec": 0.668,
            "rec": 0.835,
            "f1": 0.7422222222222222
        },
        "landmark (MME)": {
            "acc": 0.8275,
            "prec": 0.8540540540540541,
            "rec": 0.79,
            "f1": 0.8207792207792207
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.3076923076923077,
            "rec": 0.2,
            "f1": 0.24242424242424246
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8055555555555556,
            "rec": 0.9666666666666667,
            "f1": 0.8787878787878789
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5757575757575758,
            "rec": 0.95,
            "f1": 0.7169811320754716
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4520547945205479
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5673758865248227
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6616161616161617
        },
        "ocr (MMBench_CN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.39361702127659576
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.89
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6063829787234043
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8018867924528302
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7689393939393939
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9476744186046512
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5746031746031746
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5251396648044693
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7474747474747475
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7243589743589743
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4124293785310734
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.40425531914893614
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8322368421052632
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8018867924528302
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.43333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24183006535947713
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.26136363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7123287671232876
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9775641025641025
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.7902097902097902
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5526315789473685
        },
        "2D Count (CVBench)": {
            "acc": 0.5228426395939086
        },
        "3D Distance (CVBench)": {
            "acc": 0.535
        },
        "2D Relation (CVBench)": {
            "acc": 0.6138461538461538
        },
        "3D Depth (CVBench)": {
            "acc": 0.6316666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7607074952125545,
            "bert": 0.7828394901752472
        },
        "Whole dataset (Enrico)": {
            "bart": -6.558378853797913,
            "bert": 0.9237199604511261
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.526537656784058,
            "bert": 0.8391284602880478
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3344674277305604,
            "bert": 0.8630291074514389
        },
        "Whole dataset (GQA)": {
            "bart": -3.820566771030426,
            "bert": 0.9922266513109207
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.804329485893249,
            "bert": 0.7956287014484406
        },
        "Whole dataset (INAT)": {
            "bart": -6.2898696780204775,
            "bert": 0.7929667246341705
        },
        "Whole dataset (IRFL)": {
            "bart": -4.236332433223724,
            "bert": 0.9986636704206466
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.972345713376999,
            "bert": 0.8495571541786194
        },
        "Whole dataset (Memotion)": {
            "bart": -4.543470784425735,
            "bert": 0.8927590000629425
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.793094642162323,
            "bert": 0.8841598927974701
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.920687074661255,
            "bert": 0.8398084431886673
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4972431051731108,
            "bert": 0.8794692957401276
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.239402029514313,
            "bert": 0.9398817121982574
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.364226151704788,
            "bert": 0.824335908293724
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.7891092002391815,
            "bert": 0.9110084646940231
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.940925091505051,
            "bert": 0.9089927846193313
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.223167839050293,
            "bert": 0.8531857830286026
        },
        "Whole dataset (Slake)": {
            "bart": -3.984547470808029,
            "bert": 0.9939945441484451
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.820710742473603,
            "bert": 0.8578181433677673
        },
        "Whole dataset (VCR)": {
            "bart": -3.1968643927574156,
            "bert": 0.9205814218521118
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.643704405426979,
            "bert": 0.9071142292022705
        },
        "Whole dataset (VQA)": {
            "bart": -4.2559797787666325,
            "bert": 0.9706801897287369
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.661935701370239,
            "bert": 0.941976117491722
        },
        "Whole dataset (Winoground)": {
            "bart": -4.253159571886062,
            "bert": 0.9979737377166749
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9821428571428571,
            "rec": 0.7432432432432432,
            "f1": 0.8461538461538461
        },
        "popular (POPE)": {
            "acc": 0.8371335504885994,
            "prec": 0.9147286821705426,
            "rec": 0.7515923566878981,
            "f1": 0.8251748251748251
        },
        "adversarial (POPE)": {
            "acc": 0.8951048951048951,
            "prec": 0.9636363636363636,
            "rec": 0.803030303030303,
            "f1": 0.8760330578512396
        }
    },
    "siglip-384px-letterbox+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6956334695633469
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6726546906187625
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.541405269761606
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.786
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6080915406620352
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3526530612244898
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5816091954022988
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2371031746031746
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4347249834327369
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6298568507157464
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7321089297023432
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.707809939923539
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5728542914171657
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5114155251141552
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5848375451263538
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.32649071358748777
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7794561933534743
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.43333333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4716981132075472
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7303030303030303
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3469387755102041
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.6235294117647059,
            "prec": 0.9565217391304348,
            "rec": 0.25882352941176473,
            "f1": 0.40740740740740744
        },
        "posters (MME)": {
            "acc": 0.8163265306122449,
            "prec": 0.9603960396039604,
            "rec": 0.6598639455782312,
            "f1": 0.782258064516129
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7,
            "rec": 0.9333333333333333,
            "f1": 0.8
        },
        "scene (MME)": {
            "acc": 0.8475,
            "prec": 0.9263803680981595,
            "rec": 0.755,
            "f1": 0.8319559228650139
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.651685393258427,
            "rec": 0.8285714285714286,
            "f1": 0.7295597484276731
        },
        "artwork (MME)": {
            "acc": 0.6825,
            "prec": 0.6852791878172588,
            "rec": 0.675,
            "f1": 0.6801007556675063
        },
        "landmark (MME)": {
            "acc": 0.81,
            "prec": 0.8690476190476191,
            "rec": 0.73,
            "f1": 0.7934782608695652
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.25,
            "rec": 0.1,
            "f1": 0.14285714285714288
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8125,
            "rec": 0.8666666666666667,
            "f1": 0.8387096774193549
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.68,
            "rec": 0.85,
            "f1": 0.7555555555555556
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.872093023255814
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5307692307692308
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5205479452054794
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5586592178770949
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9534883720930233
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.696969696969697
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4078014184397163
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7927631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7916666666666666
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36666666666666664
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6126984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6118721461187214
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6808510638297872
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.45251396648044695
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9582309582309583
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7878787878787878
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.881578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6276595744680851
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8349056603773585
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8484848484848485
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.41333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9571428571428572
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.4
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.5
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5
        },
        "Music (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2786885245901639
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24183006535947713
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15196078431372548
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.22727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8951048951048951
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.78125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.45098039215686275
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4647887323943662
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.5609137055837563
        },
        "3D Distance (CVBench)": {
            "acc": 0.5233333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.5907692307692308
        },
        "3D Depth (CVBench)": {
            "acc": 0.6533333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.646936845779419,
            "bert": 0.7921761077642441
        },
        "Whole dataset (Enrico)": {
            "bart": -6.758891017436981,
            "bert": 0.9579962563514709
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.854703860282898,
            "bert": 0.8208275324106217
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3278605329990385,
            "bert": 0.8638897806406021
        },
        "Whole dataset (GQA)": {
            "bart": -4.007689425945282,
            "bert": 0.992225900888443
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.175284700393677,
            "bert": 0.7997486144304276
        },
        "Whole dataset (INAT)": {
            "bart": -6.268309071063995,
            "bert": 0.7935004603862762
        },
        "Whole dataset (IRFL)": {
            "bart": -4.191546609401703,
            "bert": 0.9986739057302475
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9631786096096038,
            "bert": 0.8485567420721054
        },
        "Whole dataset (Memotion)": {
            "bart": -4.495903686285019,
            "bert": 0.8933578640222549
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7884901094436647,
            "bert": 0.8821151798963547
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.626390309333801,
            "bert": 0.8827282667160035
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4637929797172546,
            "bert": 0.8806434148550033
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.15166065633297,
            "bert": 0.9411475086212158
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901809872984886,
            "bert": 0.8358628231287003
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.117626042366028,
            "bert": 0.9120293909311294
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.968956260681153,
            "bert": 0.9111975342035293
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.127224621772766,
            "bert": 0.8573328143358231
        },
        "Whole dataset (Slake)": {
            "bart": -4.166278704404831,
            "bert": 0.9954185074567795
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.788904815912247,
            "bert": 0.8606068879365921
        },
        "Whole dataset (VCR)": {
            "bart": -3.1882072487473487,
            "bert": 0.9202283149957657
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.743360589146614,
            "bert": 0.9047609663009644
        },
        "Whole dataset (VQA)": {
            "bart": -4.167093806266784,
            "bert": 0.9748555654287339
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.600672354698181,
            "bert": 0.9407290643453599
        },
        "Whole dataset (Winoground)": {
            "bart": -4.3312558233737946,
            "bert": 0.9979518294334412
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9743589743589743,
            "rec": 0.7702702702702703,
            "f1": 0.8603773584905661
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9172932330827067,
            "rec": 0.7770700636942676,
            "f1": 0.8413793103448277
        },
        "adversarial (POPE)": {
            "acc": 0.8846153846153846,
            "prec": 0.9714285714285714,
            "rec": 0.7727272727272727,
            "f1": 0.8607594936708862
        }
    },
    "siglip-384px-resize-crop+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6726177672617767
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6367265469061876
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5244667503136763
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2694610778443114
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.826
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5700858193706579
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.38857142857142857
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.47586206896551725
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24107142857142858
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4274353876739563
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6083844580777096
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6701030927835051
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7308423052564914
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6908793009284544
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.30303030303030304
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5389221556886228
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5144596651445966
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5126353790613718
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34017595307917886
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7673716012084593
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4591194968553459
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7515151515151515
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6180904522613065
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.46938775510204084
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.7058823529411765,
            "prec": 0.7777777777777778,
            "rec": 0.5764705882352941,
            "f1": 0.6621621621621621
        },
        "posters (MME)": {
            "acc": 0.8231292517006803,
            "prec": 0.9203539823008849,
            "rec": 0.7074829931972789,
            "f1": 0.8
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "scene (MME)": {
            "acc": 0.855,
            "prec": 0.927710843373494,
            "rec": 0.77,
            "f1": 0.8415300546448088
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6627906976744186,
            "rec": 0.8142857142857143,
            "f1": 0.7307692307692307
        },
        "artwork (MME)": {
            "acc": 0.685,
            "prec": 0.6745283018867925,
            "rec": 0.715,
            "f1": 0.6941747572815534
        },
        "landmark (MME)": {
            "acc": 0.8,
            "prec": 0.879746835443038,
            "rec": 0.695,
            "f1": 0.7765363128491619
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.95,
            "prec": 0.9354838709677419,
            "rec": 0.9666666666666667,
            "f1": 0.9508196721311476
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.7142857142857143,
            "rec": 0.8333333333333334,
            "f1": 0.7692307692307692
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6296296296296297,
            "rec": 0.85,
            "f1": 0.723404255319149
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4864864864864865,
            "rec": 0.9,
            "f1": 0.631578947368421
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.526984126984127
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4429223744292237
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5177304964539007
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.48044692737430167
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7121212121212122
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.38652482269503546
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7735849056603774
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7689393939393939
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8837209302325582
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6095238095238096
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.589041095890411
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6524822695035462
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4860335195530726
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8131313131313131
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7692307692307693
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.44680851063829785
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8782894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8301886792452831
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8295454545454546
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.95
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.5
        },
        "Materials (MMMU)": {
            "acc": 0.4
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.23770491803278687
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 1.0
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.43478260869565216
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.47058823529411764
        },
        "Classification (ScienceQA)": {
            "acc": 0.8987341772151899
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.5038071065989848
        },
        "3D Distance (CVBench)": {
            "acc": 0.5283333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.583076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.6733333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.585637452602387,
            "bert": 0.8142293608188629
        },
        "Whole dataset (Enrico)": {
            "bart": -6.55477049946785,
            "bert": 0.9699753850698472
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.1365985631942745,
            "bert": 0.8163244009017945
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.327305030822754,
            "bert": 0.8635018134117126
        },
        "Whole dataset (GQA)": {
            "bart": -3.843431295156479,
            "bert": 0.9922370439767838
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.195694460868835,
            "bert": 0.7987036472558975
        },
        "Whole dataset (INAT)": {
            "bart": -6.26266773223877,
            "bert": 0.7938158524036407
        },
        "Whole dataset (IRFL)": {
            "bart": -4.191546609401703,
            "bert": 0.9986739057302475
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.990416638851166,
            "bert": 0.8485412132740021
        },
        "Whole dataset (Memotion)": {
            "bart": -4.561423271894455,
            "bert": 0.892614985704422
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8272939783334734,
            "bert": 0.8701663506031037
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.959573578834534,
            "bert": 0.8371678024530411
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4816061651706693,
            "bert": 0.8811245489120484
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1699173712730406,
            "bert": 0.9408480250835418
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.890232204794883,
            "bert": 0.854346114397049
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.897945171594619,
            "bert": 0.9100038343667984
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.7786124897003175,
            "bert": 0.9161826646327973
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.282527351379395,
            "bert": 0.8533372312784195
        },
        "Whole dataset (Slake)": {
            "bart": -3.76968758225441,
            "bert": 0.9954477208852768
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.812242810726166,
            "bert": 0.8629925894737244
        },
        "Whole dataset (VCR)": {
            "bart": -3.2870704883337023,
            "bert": 0.9197543650865555
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.639122721552849,
            "bert": 0.9050877642631531
        },
        "Whole dataset (VQA)": {
            "bart": -4.278459389209747,
            "bert": 0.9743509298563003
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.655759512186051,
            "bert": 0.9433194315433502
        },
        "Whole dataset (Winoground)": {
            "bart": -4.301398607492447,
            "bert": 0.9979586529731751
        },
        "random (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9478260869565217,
            "rec": 0.7364864864864865,
            "f1": 0.8288973384030419
        },
        "popular (POPE)": {
            "acc": 0.8273615635179153,
            "prec": 0.9,
            "rec": 0.7452229299363057,
            "f1": 0.8153310104529616
        },
        "adversarial (POPE)": {
            "acc": 0.8636363636363636,
            "prec": 0.9514563106796117,
            "rec": 0.7424242424242424,
            "f1": 0.8340425531914895
        }
    },
    "siglip-384px-resize-naive+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.697784469778447
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.656686626746507
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5388958594730239
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.766
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6162648140580302
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3689795918367347
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6022988505747127
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.23809523809523808
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.42876076872100727
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6441717791411042
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7381253958201394
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7110868377935554
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5249500998003992
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5068493150684932
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5956678700361011
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.31671554252199413
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7734138972809668
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4583333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5031446540880503
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6909090909090909
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6381909547738693
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3673469387755102
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.611764705882353,
            "prec": 0.8958333333333334,
            "rec": 0.2529411764705882,
            "f1": 0.39449541284403666
        },
        "posters (MME)": {
            "acc": 0.8027210884353742,
            "prec": 0.9405940594059405,
            "rec": 0.6462585034013606,
            "f1": 0.7661290322580645
        },
        "position (MME)": {
            "acc": 0.8,
            "prec": 0.725,
            "rec": 0.9666666666666667,
            "f1": 0.8285714285714285
        },
        "scene (MME)": {
            "acc": 0.84,
            "prec": 0.9096385542168675,
            "rec": 0.755,
            "f1": 0.825136612021858
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6590909090909091,
            "rec": 0.8285714285714286,
            "f1": 0.7341772151898734
        },
        "artwork (MME)": {
            "acc": 0.695,
            "prec": 0.6875,
            "rec": 0.715,
            "f1": 0.7009803921568628
        },
        "landmark (MME)": {
            "acc": 0.815,
            "prec": 0.8987341772151899,
            "rec": 0.71,
            "f1": 0.7932960893854749
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 1.0,
            "rec": 0.1,
            "f1": 0.18181818181818182
        },
        "existence (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.9642857142857143,
            "rec": 0.9,
            "f1": 0.9310344827586207
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8709677419354839,
            "rec": 0.9,
            "f1": 0.8852459016393444
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6842105263157895,
            "rec": 0.65,
            "f1": 0.6666666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4827586206896552,
            "rec": 0.7,
            "f1": 0.5714285714285714
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9011627906976745
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5396825396825397
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.45662100456621
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5957446808510638
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.553072625698324
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9395348837209302
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6666666666666666
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7243589743589743
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4219858156028369
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.81
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7358490566037735
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7840909090909091
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.35333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9285714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.593607305936073
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6524822695035462
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7676767676767676
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3389830508474576
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.450354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8717105263157895
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8160377358490566
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8560606060606061
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.5
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.7
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29918032786885246
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2222222222222222
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.25
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.8531468531468531
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.603448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.37037037037037035
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.43661971830985913
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5736040609137056
        },
        "3D Distance (CVBench)": {
            "acc": 0.5316666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.5907692307692308
        },
        "3D Depth (CVBench)": {
            "acc": 0.6766666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.603062906265259,
            "bert": 0.8034796720743179
        },
        "Whole dataset (Enrico)": {
            "bart": -6.756792633533478,
            "bert": 0.9689350628852844
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.793819556236267,
            "bert": 0.826693338751793
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2973549652099607,
            "bert": 0.8654920119047165
        },
        "Whole dataset (GQA)": {
            "bart": -4.0319909465312955,
            "bert": 0.9922329360246658
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.0666894483566285,
            "bert": 0.7995610308647155
        },
        "Whole dataset (INAT)": {
            "bart": -6.302385566234588,
            "bert": 0.7924665659666061
        },
        "Whole dataset (IRFL)": {
            "bart": -4.2455233430862425,
            "bert": 0.9986595398187638
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9703241634368895,
            "bert": 0.8492438101768494
        },
        "Whole dataset (Memotion)": {
            "bart": -4.719155186414719,
            "bert": 0.8727542012929916
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8277152705192568,
            "bert": 0.8777862066030502
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.164201154708862,
            "bert": 0.9487946999073028
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.420463924407959,
            "bert": 0.8819125169515609
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1934963685274123,
            "bert": 0.9410081744194031
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.811441230773926,
            "bert": 0.8419540083408356
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.290294377803803,
            "bert": 0.914356580376625
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.714790909290314,
            "bert": 0.9152265918254853
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.098510591983795,
            "bert": 0.8593647533655167
        },
        "Whole dataset (Slake)": {
            "bart": -4.194595028162002,
            "bert": 0.9954038947820664
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.961770135164261,
            "bert": 0.8531647604703904
        },
        "Whole dataset (VCR)": {
            "bart": -3.098521877527237,
            "bert": 0.9243702775239945
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6768806689977644,
            "bert": 0.9057853317260742
        },
        "Whole dataset (VQA)": {
            "bart": -3.9469737935066225,
            "bert": 0.9768780839443206
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.712610237598419,
            "bert": 0.940088232755661
        },
        "Whole dataset (Winoground)": {
            "bart": -4.328386974334717,
            "bert": 0.9979514700174331
        },
        "random (POPE)": {
            "acc": 0.8664495114006515,
            "prec": 0.9819819819819819,
            "rec": 0.7364864864864865,
            "f1": 0.8416988416988417
        },
        "popular (POPE)": {
            "acc": 0.8599348534201955,
            "prec": 0.9453125,
            "rec": 0.7707006369426752,
            "f1": 0.8491228070175438
        },
        "adversarial (POPE)": {
            "acc": 0.8811188811188811,
            "prec": 0.9711538461538461,
            "rec": 0.7651515151515151,
            "f1": 0.8559322033898304
        }
    },
    "dinoclip-336px-letterbox+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6670251667025167
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5109780439121756
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5282308657465495
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.712
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5692684920310584
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.37551020408163266
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5149425287356322
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24107142857142858
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4095427435387674
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6165644171779141
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6082474226804123
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7207093096896771
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6843255051884216
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3181818181818182
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.46506986027944114
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5114155251141552
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4151624548736462
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34506353861192574
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7190332326283988
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.43333333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4716981132075472
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6060606060606061
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6331658291457286
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.6029411764705882,
            "prec": 0.6576576576576577,
            "rec": 0.4294117647058823,
            "f1": 0.5195729537366548
        },
        "posters (MME)": {
            "acc": 0.6496598639455783,
            "prec": 0.671875,
            "rec": 0.5850340136054422,
            "f1": 0.6254545454545455
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "scene (MME)": {
            "acc": 0.875,
            "prec": 0.9032258064516129,
            "rec": 0.84,
            "f1": 0.8704663212435233
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6642857142857143,
            "prec": 0.6352941176470588,
            "rec": 0.7714285714285715,
            "f1": 0.6967741935483872
        },
        "artwork (MME)": {
            "acc": 0.6675,
            "prec": 0.6356275303643725,
            "rec": 0.785,
            "f1": 0.7024608501118568
        },
        "landmark (MME)": {
            "acc": 0.7925,
            "prec": 0.7695852534562212,
            "rec": 0.835,
            "f1": 0.8009592326139089
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.8333333333333334,
            "rec": 0.25,
            "f1": 0.3846153846153846
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.3333333333333333,
            "rec": 0.15,
            "f1": 0.20689655172413793
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.7419354838709677,
            "rec": 0.7666666666666667,
            "f1": 0.7540983606557377
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8108108108108109,
            "rec": 1.0,
            "f1": 0.8955223880597014
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5757575757575758,
            "rec": 0.95,
            "f1": 0.7169811320754716
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7383720930232558
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5555555555555556
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4840182648401826
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5390070921985816
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.39106145251396646
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9434889434889435
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.5732323232323232
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5897435897435898
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.36524822695035464
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.725
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7631578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6063829787234043
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6179245283018868
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.6893939393939394
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5682539682539682
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5981735159817352
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.44692737430167595
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8697674418604651
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.6161616161616161
        },
        "ocr (MMBench_EN)": {
            "acc": 0.5833333333333334
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3971631205673759
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.705
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.7993421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6063829787234043
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7547169811320755
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.803030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.3
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.7
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.25
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.20915032679738563
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.11274509803921569
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2159090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6986301369863014
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.7482517482517482
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.5952380952380952
        },
        "Magnets (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.8732394366197183
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.5520304568527918
        },
        "3D Distance (CVBench)": {
            "acc": 0.5266666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.5861538461538461
        },
        "3D Depth (CVBench)": {
            "acc": 0.635
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.922562084197998,
            "bert": 0.7966563034057618
        },
        "Whole dataset (Enrico)": {
            "bart": -6.46055777311325,
            "bert": 0.9839397364854813
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.470030007362365,
            "bert": 0.8483025193214416
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3731210327148435,
            "bert": 0.8649686884880066
        },
        "Whole dataset (GQA)": {
            "bart": -3.7179058170318604,
            "bert": 0.9923097932338715
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.5835330772399905,
            "bert": 0.7983495813608169
        },
        "Whole dataset (INAT)": {
            "bart": -6.301403486728669,
            "bert": 0.7937385469675065
        },
        "Whole dataset (IRFL)": {
            "bart": -4.374143116474151,
            "bert": 0.9986281150579452
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.046317913532257,
            "bert": 0.8479103404283523
        },
        "Whole dataset (Memotion)": {
            "bart": -4.498505585193634,
            "bert": 0.8977914017438888
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.659814420342445,
            "bert": 0.8771323150396347
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.76209432721138,
            "bert": 0.9930286520719528
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4930744552612305,
            "bert": 0.8824678725004196
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.3801761043071745,
            "bert": 0.9364596420526504
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.207590882182121,
            "bert": 0.8220255875587463
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.204935128688812,
            "bert": 0.913555006980896
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.209287464618683,
            "bert": 0.8932976871728897
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.473163833618164,
            "bert": 0.8520239824056626
        },
        "Whole dataset (Slake)": {
            "bart": -4.206819143295288,
            "bert": 0.9964753216505051
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.9137588572502136,
            "bert": 0.8086569005250931
        },
        "Whole dataset (VCR)": {
            "bart": -3.349069609642029,
            "bert": 0.91605593085289
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7144126850366592,
            "bert": 0.9030273866653442
        },
        "Whole dataset (VQA)": {
            "bart": -4.657222106456756,
            "bert": 0.9698590415716172
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.564471361637115,
            "bert": 0.9409563893079758
        },
        "Whole dataset (Winoground)": {
            "bart": -4.304267456531525,
            "bert": 0.997959012389183
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.975,
            "rec": 0.7905405405405406,
            "f1": 0.8731343283582089
        },
        "popular (POPE)": {
            "acc": 0.8631921824104235,
            "prec": 0.9259259259259259,
            "rec": 0.7961783439490446,
            "f1": 0.8561643835616438
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.9646017699115044,
            "rec": 0.8257575757575758,
            "f1": 0.8897959183673471
        }
    },
    "dinoclip-336px-resize-naive+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6816519681651968
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.5588822355289421
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5401505646173149
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.714
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5717204740498569
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3730612244897959
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.4436781609195402
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24206349206349206
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.3996023856858847
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6380368098159509
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.72102596580114
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6963407973784818
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.32575757575757575
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.48902195608782434
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5190258751902588
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.4259927797833935
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.35581622678396874
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.716012084592145
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.39166666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4716981132075472
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.5878787878787879
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6231155778894473
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3673469387755102
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.6323529411764706,
            "prec": 0.6859504132231405,
            "rec": 0.48823529411764705,
            "f1": 0.5704467353951891
        },
        "posters (MME)": {
            "acc": 0.6632653061224489,
            "prec": 0.7068965517241379,
            "rec": 0.5578231292517006,
            "f1": 0.6235741444866919
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6756756756756757,
            "rec": 0.8333333333333334,
            "f1": 0.746268656716418
        },
        "scene (MME)": {
            "acc": 0.8525,
            "prec": 0.873015873015873,
            "rec": 0.825,
            "f1": 0.8483290488431877
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.651685393258427,
            "rec": 0.8285714285714286,
            "f1": 0.7295597484276731
        },
        "artwork (MME)": {
            "acc": 0.67,
            "prec": 0.6452991452991453,
            "rec": 0.755,
            "f1": 0.695852534562212
        },
        "landmark (MME)": {
            "acc": 0.8075,
            "prec": 0.8219895287958116,
            "rec": 0.785,
            "f1": 0.80306905370844
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 0.6666666666666666,
            "rec": 0.3,
            "f1": 0.41379310344827586
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.8064516129032258,
            "rec": 0.8333333333333334,
            "f1": 0.819672131147541
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7631578947368421,
            "rec": 0.9666666666666667,
            "f1": 0.8529411764705883
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5757575757575758,
            "rec": 0.95,
            "f1": 0.7169811320754716
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.8,
            "f1": 0.6153846153846154
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8023255813953488
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5650793650793651
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.43846153846153846
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4520547945205479
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5390070921985816
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4022346368715084
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.941031941031941
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6060606060606061
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6282051282051282
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.75
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.75
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6509433962264151
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.696969696969697
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8785714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.872093023255814
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5841269841269842
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5570776255707762
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5886524822695035
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8697674418604651
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.6565656565656566
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6089743589743589
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3615819209039548
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3723404255319149
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.745
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.648936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.7992424242424242
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.38
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.19607843137254902
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12941176470588237
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12745098039215685
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.22727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.28205128205128205
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.7832167832167832
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.5476190476190477
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.8125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.35294117647058826
        },
        "Classification (ScienceQA)": {
            "acc": 0.8607594936708861
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.7183098591549296
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "2D Count (CVBench)": {
            "acc": 0.5431472081218274
        },
        "3D Distance (CVBench)": {
            "acc": 0.5533333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6353846153846154
        },
        "3D Depth (CVBench)": {
            "acc": 0.6833333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.661793539524078,
            "bert": 0.7964613246917724
        },
        "Whole dataset (Enrico)": {
            "bart": -6.652032308578491,
            "bert": 0.9770713073015213
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.652526240348816,
            "bert": 0.8406789582967759
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3713182961940764,
            "bert": 0.864858603477478
        },
        "Whole dataset (GQA)": {
            "bart": -3.666579711437225,
            "bert": 0.9922981423139572
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.627388582229615,
            "bert": 0.7959945553541183
        },
        "Whole dataset (INAT)": {
            "bart": -6.235143814086914,
            "bert": 0.7947418886423111
        },
        "Whole dataset (IRFL)": {
            "bart": -4.168011454343795,
            "bert": 0.9986762392520905
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.020693941116333,
            "bert": 0.8484982126951217
        },
        "Whole dataset (Memotion)": {
            "bart": -4.533153388500214,
            "bert": 0.895862478017807
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.705329712629318,
            "bert": 0.8833817839622498
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.750382742881775,
            "bert": 0.8668073111772537
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4984513580799104,
            "bert": 0.8818005985021591
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.256092388033867,
            "bert": 0.9379582989215851
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.370442865490913,
            "bert": 0.8206642627716064
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.349590260982513,
            "bert": 0.9122929006814957
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.127579900026322,
            "bert": 0.8979755282402039
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.518654456138611,
            "bert": 0.8489027935266494
        },
        "Whole dataset (Slake)": {
            "bart": -4.333579219579697,
            "bert": 0.9949999612569809
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.612241764068603,
            "bert": 0.8171083945035934
        },
        "Whole dataset (VCR)": {
            "bart": -3.2453281450271607,
            "bert": 0.917894914150238
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.699094703793526,
            "bert": 0.9054374951124191
        },
        "Whole dataset (VQA)": {
            "bart": -4.571033807992936,
            "bert": 0.9708977997303009
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.594475606679916,
            "bert": 0.9399742937088013
        },
        "Whole dataset (Winoground)": {
            "bart": -4.400161164999008,
            "bert": 0.9979340517520905
        },
        "random (POPE)": {
            "acc": 0.8859934853420195,
            "prec": 0.959349593495935,
            "rec": 0.7972972972972973,
            "f1": 0.8708487084870848
        },
        "popular (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9338235294117647,
            "rec": 0.8089171974522293,
            "f1": 0.8668941979522184
        },
        "adversarial (POPE)": {
            "acc": 0.8881118881118881,
            "prec": 0.9545454545454546,
            "rec": 0.7954545454545454,
            "f1": 0.8677685950413222
        }
    },
    "dinosiglip-384px-letterbox+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7147773714777371
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6387225548902196
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5464240903387704
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.784
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6215774417654271
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3706122448979592
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6436781609195402
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2390873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4254473161033797
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6370143149284253
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6907216494845361
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7435085497150095
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7225559803386128
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5049900199600799
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5342465753424658
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5523465703971119
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3548387096774194
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7764350453172205
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.425
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5031446540880503
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6532663316582915
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.6764705882352942,
            "prec": 0.8571428571428571,
            "rec": 0.4235294117647059,
            "f1": 0.5669291338582677
        },
        "posters (MME)": {
            "acc": 0.7959183673469388,
            "prec": 0.9306930693069307,
            "rec": 0.6394557823129252,
            "f1": 0.7580645161290323
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.9157303370786517,
            "rec": 0.815,
            "f1": 0.8624338624338624
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6785714285714286,
            "prec": 0.6404494382022472,
            "rec": 0.8142857142857143,
            "f1": 0.7169811320754716
        },
        "artwork (MME)": {
            "acc": 0.67,
            "prec": 0.640495867768595,
            "rec": 0.775,
            "f1": 0.7013574660633484
        },
        "landmark (MME)": {
            "acc": 0.8575,
            "prec": 0.8557213930348259,
            "rec": 0.86,
            "f1": 0.85785536159601
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.25,
            "rec": 0.1,
            "f1": 0.14285714285714288
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.7083333333333334,
            "rec": 0.85,
            "f1": 0.7727272727272727
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8837209302325582
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.45662100456621
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9488372093023256
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7146464646464646
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7435897435897436
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3829787234042553
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.46808510638297873
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8181818181818182
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.37333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9214285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6412698412698413
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5418994413407822
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7651515151515151
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.327683615819209
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8585526315789473
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8537735849056604
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8825757575757576
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.4
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.27419354838709675
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6739130434782609
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5098039215686274
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5492957746478874
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5596446700507615
        },
        "3D Distance (CVBench)": {
            "acc": 0.5383333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6569230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.69
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.5299194312095645,
            "bert": 0.8199260038137436
        },
        "Whole dataset (Enrico)": {
            "bart": -6.840089943408966,
            "bert": 0.9739964926242828
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.783225562572479,
            "bert": 0.8248201048374176
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.338545970916748,
            "bert": 0.8654129350185394
        },
        "Whole dataset (GQA)": {
            "bart": -3.5202349042892456,
            "bert": 0.9937423604726792
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.669495561122894,
            "bert": 0.7962993776798248
        },
        "Whole dataset (INAT)": {
            "bart": -6.281213529109955,
            "bert": 0.7933653348684311
        },
        "Whole dataset (IRFL)": {
            "bart": -4.167427091598511,
            "bert": 0.9986814481019973
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.964771875143051,
            "bert": 0.8497920012474061
        },
        "Whole dataset (Memotion)": {
            "bart": -4.565242176055908,
            "bert": 0.8901487904787063
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8238128805160523,
            "bert": 0.8677789831161499
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.837109560966492,
            "bert": 0.8520524489879608
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4273105335235594,
            "bert": 0.8810460466146469
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.273824617266655,
            "bert": 0.9370691442489624
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.408315255641937,
            "bert": 0.8148923569917679
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.937133851051331,
            "bert": 0.9066910123825074
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.749359253644943,
            "bert": 0.9020847463607788
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.094080178737641,
            "bert": 0.8595991969108582
        },
        "Whole dataset (Slake)": {
            "bart": -3.6598946607112883,
            "bert": 0.9940383738279343
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.132918863296509,
            "bert": 0.8420149040222168
        },
        "Whole dataset (VCR)": {
            "bart": -3.3667181372642516,
            "bert": 0.9150237607955932
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.697146391272545,
            "bert": 0.9059611028432846
        },
        "Whole dataset (VQA)": {
            "bart": -4.108288608789444,
            "bert": 0.9743377876281738
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.574830665588379,
            "bert": 0.9408603048324585
        },
        "Whole dataset (Winoground)": {
            "bart": -4.304267456531525,
            "bert": 0.997959012389183
        },
        "random (POPE)": {
            "acc": 0.8925081433224755,
            "prec": 0.9752066115702479,
            "rec": 0.7972972972972973,
            "f1": 0.8773234200743494
        },
        "popular (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.927536231884058,
            "rec": 0.8152866242038217,
            "f1": 0.8677966101694916
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.981651376146789,
            "rec": 0.8106060606060606,
            "f1": 0.8879668049792531
        }
    },
    "dinosiglip-384px-resize-naive+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7154226715422671
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6866267465069861
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5370138017565872
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2774451097804391
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.79
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6297507151614221
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3795918367346939
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6666666666666666
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2371031746031746
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4294234592445328
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6421267893660532
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7523749208359721
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7285636264336428
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2878787878787879
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.49101796407185627
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5159817351598174
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.592057761732852
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3626588465298143
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7824773413897281
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.425
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5345911949685535
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6909090909090909
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6381909547738693
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.40816326530612246
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.5941176470588235,
            "prec": 0.9705882352941176,
            "rec": 0.19411764705882353,
            "f1": 0.32352941176470584
        },
        "posters (MME)": {
            "acc": 0.8095238095238095,
            "prec": 0.941747572815534,
            "rec": 0.6598639455782312,
            "f1": 0.7759999999999999
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6842105263157895,
            "rec": 0.8666666666666667,
            "f1": 0.7647058823529413
        },
        "scene (MME)": {
            "acc": 0.865,
            "prec": 0.9101123595505618,
            "rec": 0.81,
            "f1": 0.8571428571428572
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.6785714285714286,
            "rec": 0.8142857142857143,
            "f1": 0.7402597402597403
        },
        "artwork (MME)": {
            "acc": 0.71,
            "prec": 0.6794871794871795,
            "rec": 0.795,
            "f1": 0.7327188940092165
        },
        "landmark (MME)": {
            "acc": 0.8225,
            "prec": 0.8816568047337278,
            "rec": 0.745,
            "f1": 0.8075880758807588
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.25,
            "rec": 0.05,
            "f1": 0.08333333333333334
        },
        "count (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.84375,
            "rec": 0.9,
            "f1": 0.870967741935484
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.72,
            "rec": 0.9,
            "f1": 0.7999999999999999
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4857142857142857,
            "rec": 0.85,
            "f1": 0.6181818181818183
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8895348837209303
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5159817351598174
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5460992907801419
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5195530726256983
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9488372093023256
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6717171717171717
        },
        "ocr (MMBench_CN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3672316384180791
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.41843971631205673
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.819078947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.48936170212765956
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7971698113207547
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8143939393939394
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.638095238095238
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.49230769230769234
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6210045662100456
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6382978723404256
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.553072625698324
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9441860465116279
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7777777777777778
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.43617021276595747
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.868421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9943181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_EN)": {
            "acc": 0.9009433962264151
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8863636363636364
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.37333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.4
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2581967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15196078431372548
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.25
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.7972027972027972
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.24193548387096775
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4647887323943662
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.6040609137055838
        },
        "3D Distance (CVBench)": {
            "acc": 0.51
        },
        "2D Relation (CVBench)": {
            "acc": 0.6046153846153847
        },
        "3D Depth (CVBench)": {
            "acc": 0.6783333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.745547661781311,
            "bert": 0.8051995426416397
        },
        "Whole dataset (Enrico)": {
            "bart": -6.331910448074341,
            "bert": 0.9813216477632523
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.704528591632843,
            "bert": 0.8236063545942307
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.284685151576996,
            "bert": 0.8655941885709763
        },
        "Whole dataset (GQA)": {
            "bart": -3.78751948595047,
            "bert": 0.9922472208738327
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.167542300224304,
            "bert": 0.8002979528903961
        },
        "Whole dataset (INAT)": {
            "bart": -6.314896807670594,
            "bert": 0.7921512931585312
        },
        "Whole dataset (IRFL)": {
            "bart": -4.179486850500107,
            "bert": 0.9986776769161224
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.021504578590393,
            "bert": 0.8482201284170151
        },
        "Whole dataset (Memotion)": {
            "bart": -4.556026366949081,
            "bert": 0.8884717535972595
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7456200432777407,
            "bert": 0.8771219259500503
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.6041257953643795,
            "bert": 0.8899454200267791
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330895614624024,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.45564316034317,
            "bert": 0.8812416809797287
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2555156642198564,
            "bert": 0.9387033671140671
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.7986891311407085,
            "bert": 0.8174117362499237
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.100971547365188,
            "bert": 0.906633915901184
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.511690158843994,
            "bert": 0.9254275679588317
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.15539261341095,
            "bert": 0.8582704824209213
        },
        "Whole dataset (Slake)": {
            "bart": -4.106056722402573,
            "bert": 0.9961517405509949
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.293547875881195,
            "bert": 0.8325640171766281
        },
        "Whole dataset (VCR)": {
            "bart": -3.3255466020107267,
            "bert": 0.9136922544240952
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7144535952806472,
            "bert": 0.9042139947414398
        },
        "Whole dataset (VQA)": {
            "bart": -4.3729005360603335,
            "bert": 0.9747501271963119
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.503540714979172,
            "bert": 0.9415597730875015
        },
        "Whole dataset (Winoground)": {
            "bart": -4.313458366394043,
            "bert": 0.9979548817873001
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.975,
            "rec": 0.7905405405405406,
            "f1": 0.8731343283582089
        },
        "popular (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9398496240601504,
            "rec": 0.7961783439490446,
            "f1": 0.8620689655172414
        },
        "adversarial (POPE)": {
            "acc": 0.9300699300699301,
            "prec": 0.9912280701754386,
            "rec": 0.8560606060606061,
            "f1": 0.9186991869918699
        }
    },
    "llama2+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6859539685953968
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.656686626746507
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5307402760351317
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.802
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5852063751532489
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.35183673469387755
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5954022988505747
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24206349206349206
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4201457919151756
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6083844580777096
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6288659793814433
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7454084863837872
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7056253413435282
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.42115768463073855
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5068493150684932
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5379061371841155
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.34310850439882695
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7673716012084593
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.35
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.4716981132075472
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.703030303030303
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.592964824120603
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2857142857142857
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.18518518518518517
        },
        "celebrity (MME)": {
            "acc": 0.6911764705882353,
            "prec": 0.9333333333333333,
            "rec": 0.4117647058823529,
            "f1": 0.5714285714285713
        },
        "posters (MME)": {
            "acc": 0.8163265306122449,
            "prec": 0.9894736842105263,
            "rec": 0.6394557823129252,
            "f1": 0.7768595041322315
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6944444444444444,
            "rec": 0.8333333333333334,
            "f1": 0.7575757575757577
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8994413407821229,
            "rec": 0.805,
            "f1": 0.8496042216358839
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7428571428571429,
            "prec": 0.7428571428571429,
            "rec": 0.7428571428571429,
            "f1": 0.7428571428571429
        },
        "artwork (MME)": {
            "acc": 0.7225,
            "prec": 0.7069767441860465,
            "rec": 0.76,
            "f1": 0.7325301204819278
        },
        "landmark (MME)": {
            "acc": 0.83,
            "prec": 0.8837209302325582,
            "rec": 0.76,
            "f1": 0.8172043010752689
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.35294117647058826,
            "rec": 0.3,
            "f1": 0.3243243243243243
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7714285714285715,
            "rec": 0.9,
            "f1": 0.8307692307692307
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8055555555555556,
            "rec": 0.9666666666666667,
            "f1": 0.8787878787878789
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6551724137931034,
            "rec": 0.95,
            "f1": 0.7755102040816326
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7441860465116279
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.4857142857142857
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.45662100456621
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5460992907801419
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.37988826815642457
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9754299754299754
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6944444444444444
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6346153846153846
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4148936170212766
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8026315789473685
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9261363636363636
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.43617021276595747
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7735849056603774
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7424242424242424
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8928571428571429
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5841269841269842
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5981735159817352
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4748603351955307
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8837209302325582
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9754299754299754
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7449494949494949
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4716312056737589
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8519736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8254716981132075
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.43333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26639344262295084
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1111111111111111
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.24705882352941178
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2107843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.32954545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7945205479452054
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 1.0
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8601398601398601
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4818181818181818
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3611111111111111
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.5869565217391305
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.567258883248731
        },
        "3D Distance (CVBench)": {
            "acc": 0.505
        },
        "2D Relation (CVBench)": {
            "acc": 0.6230769230769231
        },
        "3D Depth (CVBench)": {
            "acc": 0.6733333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.780148367881775,
            "bert": 0.7880793291330338
        },
        "Whole dataset (Enrico)": {
            "bart": -6.148799389600754,
            "bert": 0.987489829659462
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.963267711400985,
            "bert": 0.9090787023305893
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.344771192073822,
            "bert": 0.86412153840065
        },
        "Whole dataset (GQA)": {
            "bart": -3.7167290055751803,
            "bert": 0.9936796450614929
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.212956638336181,
            "bert": 0.8002384012937546
        },
        "Whole dataset (INAT)": {
            "bart": -6.292103636264801,
            "bert": 0.79300077855587
        },
        "Whole dataset (IRFL)": {
            "bart": -4.2455233430862425,
            "bert": 0.9986595398187638
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9489580166339873,
            "bert": 0.8526270818710328
        },
        "Whole dataset (Memotion)": {
            "bart": -4.595173012018204,
            "bert": 0.8904171353578567
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9398944216966627,
            "bert": 0.8403303080797195
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.173356177806855,
            "bert": 0.9981702905893326
        },
        "Whole dataset (NLVR)": {
            "bart": -3.0974093151092528,
            "bert": 0.9990702605247498
        },
        "Whole dataset (NLVR2)": {
            "bart": -2.8981182289123537,
            "bert": 0.9991574454307556
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5059205222129823,
            "bert": 0.8807820165157318
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.141135574579239,
            "bert": 0.9353082585334778
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901272956132889,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.873075779676437,
            "bert": 0.9147817128896714
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.978396224975586,
            "bert": 0.9127528655529022
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.97021562576294,
            "bert": 0.8546939158439636
        },
        "Whole dataset (Slake)": {
            "bart": -3.9831707000732424,
            "bert": 0.9940237754583359
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.143925323486328,
            "bert": 0.8885501116514206
        },
        "Whole dataset (VCR)": {
            "bart": -3.268823533654213,
            "bert": 0.9242169100046158
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.585471023917198,
            "bert": 0.9057716131210327
        },
        "Whole dataset (VQA)": {
            "bart": -4.304614058732986,
            "bert": 0.9719647109508515
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.809204324483871,
            "bert": 0.9430728781223298
        },
        "Whole dataset (Winoground)": {
            "bart": -5.032368998527527,
            "bert": 0.9977702814340591
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9821428571428571,
            "rec": 0.7432432432432432,
            "f1": 0.8461538461538461
        },
        "popular (POPE)": {
            "acc": 0.8338762214983714,
            "prec": 0.9206349206349206,
            "rec": 0.7388535031847133,
            "f1": 0.8197879858657243
        },
        "adversarial (POPE)": {
            "acc": 0.9125874125874126,
            "prec": 0.9908256880733946,
            "rec": 0.8181818181818182,
            "f1": 0.8962655601659751
        }
    },
    "llama2+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6926220692622069
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6666666666666666
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5426599749058971
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.808
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6003269309358398
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3706122448979592
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5885057471264368
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26091269841269843
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.44068919814446655
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6134969325153374
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7431918936035465
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7045330420535226
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5608782435129741
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.512937595129376
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5595667870036101
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.28347996089931576
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7854984894259819
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.3416666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.559748427672956
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7666666666666667
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.592964824120603
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.32653061224489793
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.7147058823529412,
            "prec": 1.0,
            "rec": 0.4294117647058823,
            "f1": 0.6008230452674896
        },
        "posters (MME)": {
            "acc": 0.8435374149659864,
            "prec": 0.963302752293578,
            "rec": 0.7142857142857143,
            "f1": 0.8203125
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6590909090909091,
            "rec": 0.9666666666666667,
            "f1": 0.7837837837837838
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.9005524861878453,
            "rec": 0.815,
            "f1": 0.8556430446194226
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6794871794871795,
            "rec": 0.7571428571428571,
            "f1": 0.7162162162162163
        },
        "artwork (MME)": {
            "acc": 0.715,
            "prec": 0.6902654867256637,
            "rec": 0.78,
            "f1": 0.732394366197183
        },
        "landmark (MME)": {
            "acc": 0.71,
            "prec": 0.9666666666666667,
            "rec": 0.435,
            "f1": 0.6
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.44,
            "rec": 0.55,
            "f1": 0.48888888888888893
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.7142857142857143,
            "rec": 0.75,
            "f1": 0.7317073170731706
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.46875,
            "rec": 0.75,
            "f1": 0.5769230769230769
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8430232558139535
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5746031746031746
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5799086757990868
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6843434343434344
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7692307692307693
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5602836879432624
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.805
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7894736842105263
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7075471698113207
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7765151515151515
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9214285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5523809523809524
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6666666666666666
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6879432624113475
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.553072625698324
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8358585858585859
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8076923076923077
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5177304964539007
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7547169811320755
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8787878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4866666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9285714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6666666666666666
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27459016393442626
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.27450980392156865
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20588235294117646
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.45454545454545453
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8041958041958042
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.826530612244898
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.22580645161290322
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9130434782608695
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7435897435897436
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5490196078431373
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.5761421319796954
        },
        "3D Distance (CVBench)": {
            "acc": 0.615
        },
        "2D Relation (CVBench)": {
            "acc": 0.6984615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.7716666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.828920519351959,
            "bert": 0.7659679287672043
        },
        "Whole dataset (Enrico)": {
            "bart": -5.799599891901016,
            "bert": 0.9819379734992981
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.070880870819092,
            "bert": 0.9969903802871705
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3286750721931457,
            "bert": 0.8646864372491837
        },
        "Whole dataset (GQA)": {
            "bart": -3.651993978023529,
            "bert": 0.9937272477149963
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.409828143119812,
            "bert": 0.798356323838234
        },
        "Whole dataset (INAT)": {
            "bart": -6.324213788509369,
            "bert": 0.7975349080562592
        },
        "Whole dataset (IRFL)": {
            "bart": -4.7961817133426665,
            "bert": 0.9985225272178649
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9754076170921326,
            "bert": 0.8474085009098054
        },
        "Whole dataset (Memotion)": {
            "bart": -3.836247193813324,
            "bert": 0.903432365655899
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.3998452615737915,
            "bert": 0.8866498178243637
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.669768117666244,
            "bert": 0.9983026361465455
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5844569873809813,
            "bert": 0.9992727494239807
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.363756957054138,
            "bert": 0.9993035018444061
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.480074484348297,
            "bert": 0.880760680437088
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.142382623553276,
            "bert": 0.9385432767868042
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.871424624323845,
            "bert": 0.8442462378740311
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.19373641371727,
            "bert": 0.9169755500555038
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.165478274822235,
            "bert": 0.9340136313438415
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.253037219047546,
            "bert": 0.8579136270284653
        },
        "Whole dataset (Slake)": {
            "bart": -4.012884738445282,
            "bert": 0.9946992641687393
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.047200827002525,
            "bert": 0.9177615028619767
        },
        "Whole dataset (VCR)": {
            "bart": -3.065653924047947,
            "bert": 0.9304520839452743
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.616690143942833,
            "bert": 0.9057699835300446
        },
        "Whole dataset (VQA)": {
            "bart": -4.386716122627258,
            "bert": 0.9706976228952408
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.795461249351502,
            "bert": 0.9240690612792969
        },
        "Whole dataset (Winoground)": {
            "bart": -5.077154822349549,
            "bert": 0.9977600461244583
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.9913793103448276,
            "rec": 0.777027027027027,
            "f1": 0.8712121212121211
        },
        "popular (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.9296875,
            "rec": 0.7579617834394905,
            "f1": 0.8350877192982457
        },
        "adversarial (POPE)": {
            "acc": 0.8951048951048951,
            "prec": 0.9722222222222222,
            "rec": 0.7954545454545454,
            "f1": 0.875
        }
    },
    "vicuna-no-cotraining+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6237900623790062
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2465495608531995
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.246
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5292194523906825
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2896551724137931
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2847222222222222
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24718356527501656
        },
        "Instance Location (SEED_2)": {
            "acc": 0.556237218813906
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6792273590880304
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.624795193883124
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4657534246575342
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22021660649819494
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2756598240469208
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6918429003021148
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.225
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.20754716981132076
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2787878787878788
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.22110552763819097
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7735294117647059,
            "prec": 0.8444444444444444,
            "rec": 0.6705882352941176,
            "f1": 0.7475409836065574
        },
        "posters (MME)": {
            "acc": 0.782312925170068,
            "prec": 0.9368421052631579,
            "rec": 0.6054421768707483,
            "f1": 0.7355371900826446
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.855,
            "prec": 0.9329268292682927,
            "rec": 0.765,
            "f1": 0.8406593406593407
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.6829268292682927,
            "rec": 0.8,
            "f1": 0.736842105263158
        },
        "artwork (MME)": {
            "acc": 0.7325,
            "prec": 0.7142857142857143,
            "rec": 0.775,
            "f1": 0.7434052757793765
        },
        "landmark (MME)": {
            "acc": 0.8075,
            "prec": 0.8219895287958116,
            "rec": 0.785,
            "f1": 0.80306905370844
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 1.0,
            "rec": 0.1,
            "f1": 0.18181818181818182
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.35,
            "prec": 0.2857142857142857,
            "rec": 0.2,
            "f1": 0.23529411764705882
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7777777777777778,
            "rec": 0.9333333333333333,
            "f1": 0.8484848484848485
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7837837837837838,
            "rec": 0.9666666666666667,
            "f1": 0.8656716417910447
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.4722222222222222,
            "rec": 0.85,
            "f1": 0.6071428571428571
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5428571428571428
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.46153846153846156
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4748858447488584
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5815602836879432
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5586592178770949
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6767676767676768
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6794871794871795
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4463276836158192
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4858156028368794
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7993421052631579
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7840909090909091
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5133333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.526984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5921787709497207
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.797979797979798
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6858974358974359
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8486842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.43617021276595747
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.49333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8857142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Geography (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.4
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.25
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.7972027972027972
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.27419354838709675
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6923076923076923
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5526315789473685
        },
        "2D Count (CVBench)": {
            "acc": 0.4860406091370558
        },
        "3D Distance (CVBench)": {
            "acc": 0.5566666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.7046153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.705
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.858625056743622,
            "bert": 0.7805242919921875
        },
        "Whole dataset (Enrico)": {
            "bart": -7.140215736627579,
            "bert": 0.9205493944883346
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.112053797245026,
            "bert": 0.9930056154727935
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3338713705539704,
            "bert": 0.8647147583961486
        },
        "Whole dataset (GQA)": {
            "bart": -3.9554852378368377,
            "bert": 0.9922005403041839
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.0037269473075865,
            "bert": 0.8089956188201904
        },
        "Whole dataset (INAT)": {
            "bart": -6.308770203590393,
            "bert": 0.7938131421804429
        },
        "Whole dataset (IRFL)": {
            "bart": -4.176618001461029,
            "bert": 0.9986773175001145
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9928666830062864,
            "bert": 0.8501352775096893
        },
        "Whole dataset (Memotion)": {
            "bart": -4.7419410014152525,
            "bert": 0.8545152521133423
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7192602276802065,
            "bert": 0.8788276129961013
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.535202468633652,
            "bert": 0.9948560285568238
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5143294560909273,
            "bert": 0.8816529303789139
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.0588229906558992,
            "bert": 0.9412592113018036
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.574656764268875,
            "bert": 0.8326470983028412
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.1112952589988705,
            "bert": 0.9136103367805481
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.879302643537521,
            "bert": 0.9167913407087326
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.245751686096192,
            "bert": 0.8565290540456771
        },
        "Whole dataset (Slake)": {
            "bart": -3.9954295706748963,
            "bert": 0.9939952647686005
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.604780995845795,
            "bert": 0.8940655499696731
        },
        "Whole dataset (VCR)": {
            "bart": -3.4948711907863617,
            "bert": 0.9117035526037216
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5644726288318633,
            "bert": 0.9072353327274323
        },
        "Whole dataset (VQA)": {
            "bart": -4.322216172218322,
            "bert": 0.9736598241329193
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.707797430753708,
            "bert": 0.9392774367332458
        },
        "Whole dataset (Winoground)": {
            "bart": -4.151528165340424,
            "bert": 0.9979979795217514
        },
        "random (POPE)": {
            "acc": 0.8859934853420195,
            "prec": 0.991304347826087,
            "rec": 0.7702702702702703,
            "f1": 0.8669201520912548
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9172932330827067,
            "rec": 0.7770700636942676,
            "f1": 0.8413793103448277
        },
        "adversarial (POPE)": {
            "acc": 0.9090909090909091,
            "prec": 0.9818181818181818,
            "rec": 0.8181818181818182,
            "f1": 0.8925619834710744
        }
    },
    "llama2-no-cotraining+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6121746612174661
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2465495608531995
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.234
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5218635063342869
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2310204081632653
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2827586206896552
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.27976190476190477
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25115970841616964
        },
        "Instance Location (SEED_2)": {
            "acc": 0.556237218813906
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6801773274224192
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6357181867831786
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4627092846270928
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22743682310469315
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2570869990224829
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.676737160120846
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2787878787878788
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2613065326633166
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2962962962962963
        },
        "celebrity (MME)": {
            "acc": 0.7529411764705882,
            "prec": 0.8771929824561403,
            "rec": 0.5882352941176471,
            "f1": 0.7042253521126761
        },
        "posters (MME)": {
            "acc": 0.8163265306122449,
            "prec": 0.9696969696969697,
            "rec": 0.6530612244897959,
            "f1": 0.7804878048780488
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.85,
            "prec": 0.9430379746835443,
            "rec": 0.745,
            "f1": 0.8324022346368716
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6571428571428571,
            "prec": 0.72,
            "rec": 0.5142857142857142,
            "f1": 0.6
        },
        "artwork (MME)": {
            "acc": 0.72,
            "prec": 0.7018348623853211,
            "rec": 0.765,
            "f1": 0.7320574162679426
        },
        "landmark (MME)": {
            "acc": 0.7775,
            "prec": 0.8775510204081632,
            "rec": 0.645,
            "f1": 0.7435158501440923
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.3,
            "rec": 0.15,
            "f1": 0.2
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.8888888888888888,
            "rec": 0.8,
            "f1": 0.8421052631578948
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.68,
            "rec": 0.85,
            "f1": 0.7555555555555556
        },
        "code_reasoning (MME)": {
            "acc": 0.4,
            "prec": 0.43333333333333335,
            "rec": 0.65,
            "f1": 0.5199999999999999
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5365079365079365
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4885844748858447
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.3854748603351955
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6792929292929293
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6987179487179487
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.423728813559322
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4326241134751773
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9204545454545454
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.39361702127659576
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7272727272727273
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9285714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.580952380952381
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5307692307692308
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5363128491620112
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7929292929292929
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6923076923076923
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4350282485875706
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5106382978723404
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.7927631578947368
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.39361702127659576
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8257575757575758
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.4
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2827868852459016
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3137254901960784
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20588235294117646
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3977272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9871794871794872
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.8251748251748252
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4818181818181818
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.15555555555555556
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9130434782608695
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Maps (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6274509803921569
        },
        "Classification (ScienceQA)": {
            "acc": 0.7974683544303798
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.22535211267605634
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.4200507614213198
        },
        "3D Distance (CVBench)": {
            "acc": 0.5416666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.6723076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.71
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.146560871601105,
            "bert": 0.7925231516361236
        },
        "Whole dataset (Enrico)": {
            "bart": -7.666489773988724,
            "bert": 0.9288366287946701
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.759625799655915,
            "bert": 0.9892241436243058
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3338116538524627,
            "bert": 0.8656674975156784
        },
        "Whole dataset (GQA)": {
            "bart": -3.526587723493576,
            "bert": 0.9923073053359985
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -3.957761077880859,
            "bert": 0.9861337816715241
        },
        "Whole dataset (INAT)": {
            "bart": -5.975697102546692,
            "bert": 0.792791673541069
        },
        "Whole dataset (IRFL)": {
            "bart": -4.239201282262802,
            "bert": 0.9986640298366547
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.951854147911072,
            "bert": 0.8534724861383438
        },
        "Whole dataset (Memotion)": {
            "bart": -3.8354443526268005,
            "bert": 0.9036740046739579
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.160339283943176,
            "bert": 0.827510597705841
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.167618479728699,
            "bert": 0.9981695717573166
        },
        "Whole dataset (NLVR)": {
            "bart": -3.4340490221977236,
            "bert": 0.9992631632089615
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.338659999370575,
            "bert": 0.9992613804340362
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4685624182224273,
            "bert": 0.8831026583909989
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.9501922899484634,
            "bert": 0.9366909348964692
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.98537618637085,
            "bert": 0.807967478632927
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.160912265777588,
            "bert": 0.9145545089244842
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.472091135978698,
            "bert": 0.9021352463960648
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.3085220384597775,
            "bert": 0.8590109020471572
        },
        "Whole dataset (Slake)": {
            "bart": -3.966723930835724,
            "bert": 0.9939931440353393
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.073198316097259,
            "bert": 0.9031162351369858
        },
        "Whole dataset (VCR)": {
            "bart": -3.9964855283498766,
            "bert": 0.8957544589042663
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.595878192782402,
            "bert": 0.9050265669822692
        },
        "Whole dataset (VQA)": {
            "bart": -4.304312534332276,
            "bert": 0.9737195450067521
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.7801602733135224,
            "bert": 0.9408015918731689
        },
        "Whole dataset (Winoground)": {
            "bart": -4.846319279670715,
            "bert": 0.9978209215402604
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9910714285714286,
            "rec": 0.75,
            "f1": 0.8538461538461538
        },
        "popular (POPE)": {
            "acc": 0.8599348534201955,
            "prec": 0.9318181818181818,
            "rec": 0.7834394904458599,
            "f1": 0.8512110726643599
        },
        "adversarial (POPE)": {
            "acc": 0.9090909090909091,
            "prec": 0.9818181818181818,
            "rec": 0.8181818181818182,
            "f1": 0.8925619834710744
        }
    },
    "train-1.25-epochs+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6313185631318563
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24090338770388958
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2315369261477046
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.256
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.53330608908868
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23183673469387756
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.29195402298850576
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.28273809523809523
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24453280318091453
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5398773006134969
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6795440151994934
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6324412889131622
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2215568862275449
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4809741248097412
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.19494584837545126
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.270772238514174
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6676737160120846
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.24166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.20754716981132076
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2606060606060606
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.22110552763819097
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7558823529411764,
            "prec": 0.8270676691729323,
            "rec": 0.6470588235294118,
            "f1": 0.7260726072607261
        },
        "posters (MME)": {
            "acc": 0.826530612244898,
            "prec": 0.8692307692307693,
            "rec": 0.7687074829931972,
            "f1": 0.8158844765342961
        },
        "position (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.627906976744186,
            "rec": 0.9,
            "f1": 0.7397260273972602
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.8854166666666666,
            "rec": 0.85,
            "f1": 0.8673469387755102
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6458333333333334,
            "rec": 0.8857142857142857,
            "f1": 0.7469879518072288
        },
        "artwork (MME)": {
            "acc": 0.6875,
            "prec": 0.6245847176079734,
            "rec": 0.94,
            "f1": 0.7504990019960079
        },
        "landmark (MME)": {
            "acc": 0.835,
            "prec": 0.7991071428571429,
            "rec": 0.895,
            "f1": 0.8443396226415095
        },
        "text_translation (MME)": {
            "acc": 0.675,
            "prec": 0.64,
            "rec": 0.8,
            "f1": 0.7111111111111111
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.4,
            "rec": 0.4,
            "f1": 0.4000000000000001
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.7368421052631579,
            "rec": 0.9333333333333333,
            "f1": 0.8235294117647058
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8662790697674418
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.49206349206349204
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4657534246575342
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5177304964539007
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6717171717171717
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5576923076923077
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3672316384180791
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.40425531914893614
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7613636363636364
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.546031746031746
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5662100456621004
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.553072625698324
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8883720930232558
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7550505050505051
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6730769230769231
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4397163120567376
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.84
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8560606060606061
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9214285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27459016393442626
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3068181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9775641025641025
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.8111888111888111
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.8125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.5
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.7974683544303798
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7241379310344828
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5789473684210527
        },
        "2D Count (CVBench)": {
            "acc": 0.5279187817258884
        },
        "3D Distance (CVBench)": {
            "acc": 0.5233333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6138461538461538
        },
        "3D Depth (CVBench)": {
            "acc": 0.6583333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.589415338039398,
            "bert": 0.8134904557466507
        },
        "Whole dataset (Enrico)": {
            "bart": -6.4245737719535825,
            "bert": 0.9470421624183655
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.233794372081757,
            "bert": 0.8205124175548554
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3117127335071563,
            "bert": 0.8633033204078674
        },
        "Whole dataset (GQA)": {
            "bart": -3.472368847131729,
            "bert": 0.9937485361099243
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.161709885597229,
            "bert": 0.7918840891122818
        },
        "Whole dataset (INAT)": {
            "bart": -5.992241024971008,
            "bert": 0.7925217670202255
        },
        "Whole dataset (IRFL)": {
            "bart": -4.5107850742340085,
            "bert": 0.9986029773950577
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9695709776878356,
            "bert": 0.8480753791332245
        },
        "Whole dataset (Memotion)": {
            "bart": -4.572342519760132,
            "bert": 0.8887656152248382
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7025277799367906,
            "bert": 0.8752897679805756
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.12973952293396,
            "bert": 0.8148641419410706
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4857075452804565,
            "bert": 0.8797575235366821
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.0106051105260847,
            "bert": 0.941129332780838
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.423134596347809,
            "bert": 0.8102991384267807
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.899414391517639,
            "bert": 0.9106520915031433
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.766124334335327,
            "bert": 0.9185068970918655
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.159792175292969,
            "bert": 0.8554724723100662
        },
        "Whole dataset (Slake)": {
            "bart": -3.7445020473003385,
            "bert": 0.9919453656673431
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.360545012950897,
            "bert": 0.8069036114215851
        },
        "Whole dataset (VCR)": {
            "bart": -3.3043882888555527,
            "bert": 0.9140238165855408
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5842333525419234,
            "bert": 0.9087757170200348
        },
        "Whole dataset (VQA)": {
            "bart": -4.388510097265243,
            "bert": 0.9720331656932831
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.491260224580765,
            "bert": 0.9428939867019653
        },
        "Whole dataset (Winoground)": {
            "bart": -4.0734319138526915,
            "bert": 0.9980198878049851
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9823008849557522,
            "rec": 0.75,
            "f1": 0.8505747126436781
        },
        "popular (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.9166666666666666,
            "rec": 0.7707006369426752,
            "f1": 0.8373702422145329
        },
        "adversarial (POPE)": {
            "acc": 0.9090909090909091,
            "prec": 0.9907407407407407,
            "rec": 0.8106060606060606,
            "f1": 0.8916666666666666
        }
    },
    "train-1.5-epochs+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6323940632394063
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25094102885821834
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.242
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.534940743767879
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23510204081632652
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2850574712643678
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2757936507936508
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24519549370444002
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5480572597137015
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6082474226804123
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6735275490816973
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6313489896231568
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.22784810126582278
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3106060606060606
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.23952095808383234
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.471841704718417
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23826714801444043
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.25317693059628543
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6978851963746223
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18867924528301888
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2696969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21608040201005024
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "celebrity (MME)": {
            "acc": 0.7264705882352941,
            "prec": 0.8290598290598291,
            "rec": 0.5705882352941176,
            "f1": 0.6759581881533101
        },
        "posters (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8602941176470589,
            "rec": 0.7959183673469388,
            "f1": 0.8268551236749118
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.675,
            "rec": 0.9,
            "f1": 0.7714285714285714
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.8936170212765957,
            "rec": 0.84,
            "f1": 0.8659793814432989
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.6595744680851063,
            "rec": 0.8857142857142857,
            "f1": 0.7560975609756098
        },
        "artwork (MME)": {
            "acc": 0.665,
            "prec": 0.6114864864864865,
            "rec": 0.905,
            "f1": 0.7298387096774194
        },
        "landmark (MME)": {
            "acc": 0.835,
            "prec": 0.7913043478260869,
            "rec": 0.91,
            "f1": 0.8465116279069768
        },
        "text_translation (MME)": {
            "acc": 0.65,
            "prec": 0.6071428571428571,
            "rec": 0.85,
            "f1": 0.7083333333333333
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.6,
            "f1": 0.5454545454545454
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.75,
            "rec": 1.0,
            "f1": 0.8571428571428571
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5405405405405406,
            "rec": 1.0,
            "f1": 0.7017543859649124
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.4857142857142857
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4429223744292237
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5673758865248227
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5586592178770949
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6641414141414141
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5448717948717948
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.425531914893617
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7894736842105263
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7452830188679245
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7424242424242424
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.546031746031746
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.593607305936073
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5698324022346368
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8883720930232558
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7752525252525253
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7051282051282052
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8421052631578947
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7783018867924528
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4533333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2581967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.30718954248366015
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.24705882352941178
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12745098039215685
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9775641025641025
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8461538461538461
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.35555555555555557
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.6962025316455697
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7241379310344828
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "2D Count (CVBench)": {
            "acc": 0.5203045685279187
        },
        "3D Distance (CVBench)": {
            "acc": 0.5433333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6030769230769231
        },
        "3D Depth (CVBench)": {
            "acc": 0.6483333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.719840030670166,
            "bert": 0.8152543842792511
        },
        "Whole dataset (Enrico)": {
            "bart": -6.187087978124619,
            "bert": 0.9458883410692215
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.156780540943146,
            "bert": 0.8426296192407609
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2961209404468534,
            "bert": 0.8645319467782975
        },
        "Whole dataset (GQA)": {
            "bart": -3.5920672965049745,
            "bert": 0.9922719329595566
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.178851227760315,
            "bert": 0.7909572684764862
        },
        "Whole dataset (INAT)": {
            "bart": -5.9747303867340085,
            "bert": 0.7920362222194671
        },
        "Whole dataset (IRFL)": {
            "bart": -4.543511139154434,
            "bert": 0.9985965132713318
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.995349724292755,
            "bert": 0.8479150342941284
        },
        "Whole dataset (Memotion)": {
            "bart": -4.524656052589417,
            "bert": 0.8901246744394302
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.648101017475128,
            "bert": 0.8810944521427154
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.999657278060913,
            "bert": 0.8190786719322205
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4873185753822327,
            "bert": 0.8802341341972351
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.136388648748398,
            "bert": 0.9389513111114502
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.339785673618317,
            "bert": 0.8111834132671356
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.045862536430359,
            "bert": 0.9099231082201004
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.795197515487671,
            "bert": 0.9278495508432388
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.127407202720642,
            "bert": 0.8560665148496628
        },
        "Whole dataset (Slake)": {
            "bart": -3.769435842037201,
            "bert": 0.9938438183069229
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.03373881816864,
            "bert": 0.8534951430559158
        },
        "Whole dataset (VCR)": {
            "bart": -3.0977556724846362,
            "bert": 0.9219350951910019
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5744176667928698,
            "bert": 0.9067039549350738
        },
        "Whole dataset (VQA)": {
            "bart": -4.242638938426971,
            "bert": 0.9684484285116196
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.50203187584877,
            "bert": 0.9411798352003098
        },
        "Whole dataset (Winoground)": {
            "bart": -4.115348888635635,
            "bert": 0.9980092930793762
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.9830508474576272,
            "rec": 0.7837837837837838,
            "f1": 0.8721804511278195
        },
        "popular (POPE)": {
            "acc": 0.8469055374592834,
            "prec": 0.9044117647058824,
            "rec": 0.7834394904458599,
            "f1": 0.8395904436860069
        },
        "adversarial (POPE)": {
            "acc": 0.9125874125874126,
            "prec": 0.9908256880733946,
            "rec": 0.8181818181818182,
            "f1": 0.8962655601659751
        }
    },
    "train-2-epochs+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6218541621854162
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2572145545796738
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.26
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5369840621168778
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2342857142857143
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2735632183908046
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2787698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24585818422796554
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5480572597137015
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.669094363521216
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6269797924631348
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3181818181818182
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4581430745814307
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22382671480144403
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24926686217008798
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7069486404833837
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2636363636363636
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.17587939698492464
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7323529411764705,
            "prec": 0.708994708994709,
            "rec": 0.788235294117647,
            "f1": 0.7465181058495821
        },
        "posters (MME)": {
            "acc": 0.8367346938775511,
            "prec": 0.8778625954198473,
            "rec": 0.782312925170068,
            "f1": 0.8273381294964028
        },
        "position (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.627906976744186,
            "rec": 0.9,
            "f1": 0.7397260273972602
        },
        "scene (MME)": {
            "acc": 0.88,
            "prec": 0.9130434782608695,
            "rec": 0.84,
            "f1": 0.8749999999999999
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.6421052631578947,
            "rec": 0.8714285714285714,
            "f1": 0.7393939393939394
        },
        "artwork (MME)": {
            "acc": 0.6775,
            "prec": 0.6163934426229508,
            "rec": 0.94,
            "f1": 0.7445544554455445
        },
        "landmark (MME)": {
            "acc": 0.8275,
            "prec": 0.8075117370892019,
            "rec": 0.86,
            "f1": 0.8329297820823245
        },
        "text_translation (MME)": {
            "acc": 0.65,
            "prec": 0.7142857142857143,
            "rec": 0.5,
            "f1": 0.588235294117647
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.35,
            "prec": 0.375,
            "rec": 0.45,
            "f1": 0.4090909090909091
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5757575757575758,
            "rec": 0.95,
            "f1": 0.7169811320754716
        },
        "code_reasoning (MME)": {
            "acc": 0.55,
            "prec": 0.5263157894736842,
            "rec": 1.0,
            "f1": 0.6896551724137931
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8837209302325582
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.49523809523809526
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.45662100456621
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.574468085106383
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5418994413407822
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6691919191919192
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5833333333333334
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3971631205673759
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7467105263157895
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.75
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7840909090909091
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.85
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5396825396825397
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5296803652968036
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6201117318435754
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8837209302325582
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7777777777777778
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4645390070921986
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.425531914893617
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8598484848484849
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.48
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.5
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.29545454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9807692307692307
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.8041958041958042
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8163265306122449
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.24193548387096775
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.5952380952380952
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8695652173913043
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5641025641025641
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5686274509803921
        },
        "Classification (ScienceQA)": {
            "acc": 0.7848101265822784
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5532994923857868
        },
        "3D Distance (CVBench)": {
            "acc": 0.5283333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.56
        },
        "3D Depth (CVBench)": {
            "acc": 0.6983333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.642657177448273,
            "bert": 0.8142627739906311
        },
        "Whole dataset (Enrico)": {
            "bart": -6.29750018954277,
            "bert": 0.9750748980045318
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.0527862715721135,
            "bert": 0.8939114916324615
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.33331835269928,
            "bert": 0.8648058962821961
        },
        "Whole dataset (GQA)": {
            "bart": -3.627168798446655,
            "bert": 0.9922780454158783
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.298530864715576,
            "bert": 0.789695109128952
        },
        "Whole dataset (INAT)": {
            "bart": -6.001089797019959,
            "bert": 0.7923418879508972
        },
        "Whole dataset (IRFL)": {
            "bart": -4.453939491510392,
            "bert": 0.9986169838905334
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.95795360326767,
            "bert": 0.8485272854566575
        },
        "Whole dataset (Memotion)": {
            "bart": -4.436955990791321,
            "bert": 0.8994421178102493
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.6362525355815887,
            "bert": 0.8816091597080231
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.940259863138198,
            "bert": 0.8479964423179627
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.495953675508499,
            "bert": 0.8823961961269379
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1118566638231275,
            "bert": 0.9426727837324143
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.428201988935471,
            "bert": 0.8211806660890579
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.881802616119384,
            "bert": 0.9103794324398041
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.700506086349487,
            "bert": 0.9068013525009155
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.182027463912964,
            "bert": 0.8557736593484878
        },
        "Whole dataset (Slake)": {
            "bart": -3.96434396982193,
            "bert": 0.9947219443321228
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.461718209981918,
            "bert": 0.919904745221138
        },
        "Whole dataset (VCR)": {
            "bart": -3.118362548947334,
            "bert": 0.9230173510313034
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5333352667093276,
            "bert": 0.909453107714653
        },
        "Whole dataset (VQA)": {
            "bart": -4.386763671636581,
            "bert": 0.9716275072097779
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.56815316081047,
            "bert": 0.94046526491642
        },
        "Whole dataset (Winoground)": {
            "bart": -4.05563445687294,
            "bert": 0.998022940158844
        },
        "random (POPE)": {
            "acc": 0.8827361563517915,
            "prec": 0.9912280701754386,
            "rec": 0.7635135135135135,
            "f1": 0.8625954198473282
        },
        "popular (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9242424242424242,
            "rec": 0.7770700636942676,
            "f1": 0.8442906574394464
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.981651376146789,
            "rec": 0.8106060606060606,
            "f1": 0.8879668049792531
        }
    },
    "train-3-epochs+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6220692622069263
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2603513174404015
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.266
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.526767470371884
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23673469387755103
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2689655172413793
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.28174603174603174
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2577866136514248
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5449897750511248
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6701030927835051
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6646611779607347
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6313489896231568
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4672754946727549
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24548736462093862
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24731182795698925
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6978851963746223
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.25
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2389937106918239
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2696969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.20603015075376885
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.3950617283950617
        },
        "celebrity (MME)": {
            "acc": 0.7352941176470589,
            "prec": 0.8571428571428571,
            "rec": 0.5647058823529412,
            "f1": 0.6808510638297872
        },
        "posters (MME)": {
            "acc": 0.8299319727891157,
            "prec": 0.907563025210084,
            "rec": 0.7346938775510204,
            "f1": 0.81203007518797
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7105263157894737,
            "rec": 0.9,
            "f1": 0.7941176470588235
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.8918918918918919,
            "rec": 0.825,
            "f1": 0.8571428571428571
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.651685393258427,
            "rec": 0.8285714285714286,
            "f1": 0.7295597484276731
        },
        "artwork (MME)": {
            "acc": 0.705,
            "prec": 0.664,
            "rec": 0.83,
            "f1": 0.7377777777777779
        },
        "landmark (MME)": {
            "acc": 0.81,
            "prec": 0.848314606741573,
            "rec": 0.755,
            "f1": 0.798941798941799
        },
        "text_translation (MME)": {
            "acc": 0.675,
            "prec": 0.64,
            "rec": 0.8,
            "f1": 0.7111111111111111
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.38095238095238093,
            "rec": 0.4,
            "f1": 0.3902439024390244
        },
        "count (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6666666666666666,
            "rec": 0.9333333333333333,
            "f1": 0.7777777777777778
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.9,
            "f1": 0.6428571428571429
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5142857142857142
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4474885844748858
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8976744186046511
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6414141414141414
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6025641025641025
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.39361702127659576
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7028301886792453
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7575757575757576
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.546031746031746
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5384615384615384
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5707762557077626
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.664804469273743
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8976744186046511
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7777777777777778
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6602564102564102
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4067796610169492
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.450354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.805921052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8207547169811321
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8409090909090909
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.49333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4666666666666667
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.28688524590163933
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.27450980392156865
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1323529411764706
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9711538461538461
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 0.7857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.22580645161290322
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.47619047619047616
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.17647058823529413
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.9375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.5869565217391305
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5490196078431373
        },
        "Classification (ScienceQA)": {
            "acc": 0.7341772151898734
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4507042253521127
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5393401015228426
        },
        "3D Distance (CVBench)": {
            "acc": 0.5383333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6523076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.625
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.674089894294739,
            "bert": 0.8073033779859543
        },
        "Whole dataset (Enrico)": {
            "bart": -6.196563075780869,
            "bert": 0.9814552670717239
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.248737480640411,
            "bert": 0.8864562582969665
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.296267513036728,
            "bert": 0.8719007349014283
        },
        "Whole dataset (GQA)": {
            "bart": -3.4903417205810547,
            "bert": 0.9937286025285721
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.384857387542724,
            "bert": 0.7882412928342819
        },
        "Whole dataset (INAT)": {
            "bart": -6.003687858581543,
            "bert": 0.787701216340065
        },
        "Whole dataset (IRFL)": {
            "bart": -4.483796707391739,
            "bert": 0.9986101603507995
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.950121138095856,
            "bert": 0.8475949192047119
        },
        "Whole dataset (Memotion)": {
            "bart": -4.338277592658996,
            "bert": 0.8978261423110961
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.625395325422287,
            "bert": 0.8874436050653458
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.39183455824852,
            "bert": 0.9015953463315963
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4112588346004484,
            "bert": 0.893322435617447
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.081671133041382,
            "bert": 0.9429940193891525
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.18061707675457,
            "bert": 0.8240735685825348
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.885801296234131,
            "bert": 0.9077486592531204
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.611006851196289,
            "bert": 0.9055181801319122
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.123940377235413,
            "bert": 0.8547624772787095
        },
        "Whole dataset (Slake)": {
            "bart": -4.018922808170319,
            "bert": 0.9910232198238372
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.81427109003067,
            "bert": 0.9115243428945541
        },
        "Whole dataset (VCR)": {
            "bart": -3.2042132833600045,
            "bert": 0.9210153913497925
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5751490426063537,
            "bert": 0.9091988182067872
        },
        "Whole dataset (VQA)": {
            "bart": -4.387259007692337,
            "bert": 0.9700087422132492
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.60374195933342,
            "bert": 0.9405577397346496
        },
        "Whole dataset (Winoground)": {
            "bart": -4.2261712050437925,
            "bert": 0.9979809206724167
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9826086956521739,
            "rec": 0.7635135135135135,
            "f1": 0.8593155893536122
        },
        "popular (POPE)": {
            "acc": 0.8403908794788274,
            "prec": 0.9090909090909091,
            "rec": 0.7643312101910829,
            "f1": 0.8304498269896194
        },
        "adversarial (POPE)": {
            "acc": 0.8916083916083916,
            "prec": 0.9719626168224299,
            "rec": 0.7878787878787878,
            "f1": 0.8702928870292888
        }
    },
    "llava-lvis4v+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6293826629382663
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2603513174404015
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.234
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5561912545974663
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23346938775510204
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.30344827586206896
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2837301587301587
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24320742213386348
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5787321063394683
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6823939202026599
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6340797378481704
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3106060606060606
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2315369261477046
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5038051750380518
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20938628158844766
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.26295210166177907
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7009063444108762
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.20125786163522014
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2696969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21105527638190955
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7676470588235295,
            "prec": 0.8137931034482758,
            "rec": 0.6941176470588235,
            "f1": 0.7492063492063492
        },
        "posters (MME)": {
            "acc": 0.8503401360544217,
            "prec": 0.9327731092436975,
            "rec": 0.7551020408163265,
            "f1": 0.8345864661654134
        },
        "position (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7073170731707317,
            "rec": 0.9666666666666667,
            "f1": 0.8169014084507042
        },
        "scene (MME)": {
            "acc": 0.8725,
            "prec": 0.907103825136612,
            "rec": 0.83,
            "f1": 0.8668407310704961
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7071428571428572,
            "prec": 0.6464646464646465,
            "rec": 0.9142857142857143,
            "f1": 0.757396449704142
        },
        "artwork (MME)": {
            "acc": 0.665,
            "prec": 0.6161971830985915,
            "rec": 0.875,
            "f1": 0.7231404958677685
        },
        "landmark (MME)": {
            "acc": 0.865,
            "prec": 0.8762886597938144,
            "rec": 0.85,
            "f1": 0.8629441624365483
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.7272727272727273,
            "rec": 0.4,
            "f1": 0.5161290322580645
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.47368421052631576,
            "rec": 0.45,
            "f1": 0.46153846153846156
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7567567567567568,
            "rec": 0.9333333333333333,
            "f1": 0.835820895522388
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7692307692307693,
            "rec": 1.0,
            "f1": 0.8695652173913044
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.68,
            "rec": 0.85,
            "f1": 0.7555555555555556
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5523809523809524
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4307692307692308
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4337899543378995
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5602836879432624
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5251396648044693
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9162790697674419
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6767676767676768
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5833333333333334
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4406779661016949
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.35815602836879434
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7735849056603774
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7651515151515151
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4266666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.43846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5707762557077626
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6312056737588653
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4860335195530726
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7929292929292929
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6858974358974359
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4463276836158192
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8486842105263158
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 1.0
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8207547169811321
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5133333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.7
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2336065573770492
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1411764705882353
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1715686274509804
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7123287671232876
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.7972027972027972
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.22580645161290322
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2926829268292683
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8260869565217391
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.8227848101265823
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.29577464788732394
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5786802030456852
        },
        "3D Distance (CVBench)": {
            "acc": 0.555
        },
        "2D Relation (CVBench)": {
            "acc": 0.6876923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.6866666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7635538864135745,
            "bert": 0.7836493682861329
        },
        "Whole dataset (Enrico)": {
            "bart": -6.017967214584351,
            "bert": 0.9705495768785477
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.533250153064728,
            "bert": 0.8142346209287643
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.41541250705719,
            "bert": 0.860997992157936
        },
        "Whole dataset (GQA)": {
            "bart": -3.637312778234482,
            "bert": 0.9908537381887436
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.6733380317687985,
            "bert": 0.7971435081958771
        },
        "Whole dataset (INAT)": {
            "bart": -5.964620766639709,
            "bert": 0.7915944194793701
        },
        "Whole dataset (IRFL)": {
            "bart": -4.483796707391739,
            "bert": 0.9986101603507995
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.931775543689728,
            "bert": 0.8497265303134918
        },
        "Whole dataset (Memotion)": {
            "bart": -4.92184380531311,
            "bert": 0.85330346763134
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.7800610089302062,
            "bert": 0.8655937325954437
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.857841680049896,
            "bert": 0.847011883854866
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5323666596412657,
            "bert": 0.8782942855358123
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.163564956188202,
            "bert": 0.9393123841285705
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.823487422466278,
            "bert": 0.8178753763437271
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.979966223239899,
            "bert": 0.9101381033658982
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.515001478195191,
            "bert": 0.919126524925232
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.165054059028625,
            "bert": 0.8531594800949097
        },
        "Whole dataset (Slake)": {
            "bart": -4.0055084693431855,
            "bert": 0.991770532131195
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.4512927901744845,
            "bert": 0.8585310858488083
        },
        "Whole dataset (VCR)": {
            "bart": -3.1193119767308235,
            "bert": 0.927802345752716
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7117602258920668,
            "bert": 0.9061486339569091
        },
        "Whole dataset (VQA)": {
            "bart": -4.237136144638061,
            "bert": 0.9709186983108521
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.59104208111763,
            "bert": 0.9421703577041626
        },
        "Whole dataset (Winoground)": {
            "bart": -4.088360521793366,
            "bert": 0.9980164760351181
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9823008849557522,
            "rec": 0.75,
            "f1": 0.8505747126436781
        },
        "popular (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9307692307692308,
            "rec": 0.7707006369426752,
            "f1": 0.843205574912892
        },
        "adversarial (POPE)": {
            "acc": 0.9125874125874126,
            "prec": 0.9819819819819819,
            "rec": 0.8257575757575758,
            "f1": 0.897119341563786
        }
    },
    "llava-lrv+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6440094644009464
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25972396486825594
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2694610778443114
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.252
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5435226808336738
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2896551724137931
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.28174603174603174
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2504970178926441
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5777096114519428
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6874604179860672
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6553795740032768
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4748858447488584
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23465703971119134
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.26881720430107525
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6827794561933535
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.19166666666666668
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18867924528301888
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21608040201005024
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7470588235294118,
            "prec": 0.7413793103448276,
            "rec": 0.7588235294117647,
            "f1": 0.75
        },
        "posters (MME)": {
            "acc": 0.8401360544217688,
            "prec": 0.8968253968253969,
            "rec": 0.7687074829931972,
            "f1": 0.8278388278388278
        },
        "position (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6086956521739131,
            "rec": 0.9333333333333333,
            "f1": 0.7368421052631579
        },
        "scene (MME)": {
            "acc": 0.8675,
            "prec": 0.8585365853658536,
            "rec": 0.88,
            "f1": 0.8691358024691358
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.71875,
            "rec": 0.6571428571428571,
            "f1": 0.6865671641791045
        },
        "artwork (MME)": {
            "acc": 0.7025,
            "prec": 0.6401384083044983,
            "rec": 0.925,
            "f1": 0.7566462167689162
        },
        "landmark (MME)": {
            "acc": 0.8425,
            "prec": 0.8624338624338624,
            "rec": 0.815,
            "f1": 0.8380462724935733
        },
        "text_translation (MME)": {
            "acc": 0.575,
            "prec": 0.5714285714285714,
            "rec": 0.6,
            "f1": 0.5853658536585366
        },
        "existence (MME)": {
            "acc": 1.0,
            "prec": 1.0,
            "rec": 1.0,
            "f1": 1.0
        },
        "numerical_calculation (MME)": {
            "acc": 0.325,
            "prec": 0.29411764705882354,
            "rec": 0.25,
            "f1": 0.27027027027027023
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7435897435897436,
            "rec": 0.9666666666666667,
            "f1": 0.8405797101449275
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.59375,
            "rec": 0.95,
            "f1": 0.7307692307692308
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8895348837209303
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5047619047619047
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4474885844748858
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5363128491620112
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6717171717171717
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5961538461538461
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.425531914893617
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7631578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7916666666666666
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4666666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9534883720930233
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.547945205479452
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6737588652482269
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5810055865921788
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9754299754299754
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7702020202020202
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6923076923076923
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4858156028368794
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8355263157894737
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5638297872340425
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8901515151515151
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2581967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3137254901960784
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8041958041958042
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7959183673469388
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3387096774193548
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.75
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.45652173913043476
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4507042253521127
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5329949238578681
        },
        "3D Distance (CVBench)": {
            "acc": 0.5466666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.6384615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.7066666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.021985421180725,
            "bert": 0.7985286623239517
        },
        "Whole dataset (Enrico)": {
            "bart": -5.931497163772583,
            "bert": 0.9906744182109832
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.853701729774475,
            "bert": 0.999048233628273
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3144666635990143,
            "bert": 0.9030925208330154
        },
        "Whole dataset (GQA)": {
            "bart": -3.3167469692230225,
            "bert": 0.9923801058530808
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.006028823852539,
            "bert": 0.9932339286804199
        },
        "Whole dataset (INAT)": {
            "bart": -6.297410798072815,
            "bert": 0.7792697513103485
        },
        "Whole dataset (IRFL)": {
            "bart": -4.650880243778229,
            "bert": 0.9985729902982712
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8554800021648408,
            "bert": 0.8764979410171508
        },
        "Whole dataset (Memotion)": {
            "bart": -4.652666401863098,
            "bert": 0.9014553046226501
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.039699051380158,
            "bert": 0.835870623588562
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.429741665124893,
            "bert": 0.9983676421642304
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.3586394423246384,
            "bert": 0.9324248701333999
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2166382706165315,
            "bert": 0.9402977567911148
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.135944257974625,
            "bert": 0.8123781937360763
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.961850790977478,
            "bert": 0.9112206119298935
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.740791635513306,
            "bert": 0.8741073435544968
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.10187903881073,
            "bert": 0.8478377938270569
        },
        "Whole dataset (Slake)": {
            "bart": -3.508602542877197,
            "bert": 0.9969069808721542
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.930042243003845,
            "bert": 0.9060219669342041
        },
        "Whole dataset (VCR)": {
            "bart": -2.9272259368002413,
            "bert": 0.9304473036527634
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.496803005337715,
            "bert": 0.9107026928663253
        },
        "Whole dataset (VQA)": {
            "bart": -4.322020372152329,
            "bert": 0.9720490944385528
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.7099850666522975,
            "bert": 0.9386364603042603
        },
        "Whole dataset (Winoground)": {
            "bart": -4.046443547010422,
            "bert": 0.9980270707607269
        },
        "random (POPE)": {
            "acc": 0.8925081433224755,
            "prec": 0.9831932773109243,
            "rec": 0.7905405405405406,
            "f1": 0.8764044943820225
        },
        "popular (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9117647058823529,
            "rec": 0.7898089171974523,
            "f1": 0.8464163822525598
        },
        "adversarial (POPE)": {
            "acc": 0.916083916083916,
            "prec": 0.9736842105263158,
            "rec": 0.8409090909090909,
            "f1": 0.9024390243902439
        }
    },
    "llava-lvis4v-lrv+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6420735642073564
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25031367628607276
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.22355289421157684
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.254
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5431140171638741
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23183673469387756
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.31264367816091954
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.28273809523809523
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.23724320742213387
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5725971370143149
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6903103229892337
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6466411796832332
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4809741248097412
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22382671480144403
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.26295210166177907
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7039274924471299
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18867924528301888
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23115577889447236
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.24489795918367346
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.35802469135802467
        },
        "celebrity (MME)": {
            "acc": 0.7029411764705882,
            "prec": 0.6699507389162561,
            "rec": 0.8,
            "f1": 0.7292225201072385
        },
        "posters (MME)": {
            "acc": 0.8741496598639455,
            "prec": 0.9365079365079365,
            "rec": 0.8027210884353742,
            "f1": 0.8644688644688644
        },
        "position (MME)": {
            "acc": 0.6,
            "prec": 0.56,
            "rec": 0.9333333333333333,
            "f1": 0.7000000000000001
        },
        "scene (MME)": {
            "acc": 0.8875,
            "prec": 0.9015544041450777,
            "rec": 0.87,
            "f1": 0.8854961832061069
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.7,
            "rec": 0.7,
            "f1": 0.7
        },
        "artwork (MME)": {
            "acc": 0.705,
            "prec": 0.6576923076923077,
            "rec": 0.855,
            "f1": 0.7434782608695654
        },
        "landmark (MME)": {
            "acc": 0.865,
            "prec": 0.8348623853211009,
            "rec": 0.91,
            "f1": 0.8708133971291867
        },
        "text_translation (MME)": {
            "acc": 0.675,
            "prec": 0.6296296296296297,
            "rec": 0.85,
            "f1": 0.723404255319149
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.3888888888888889,
            "rec": 0.35,
            "f1": 0.36842105263157887
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7575757575757576,
            "rec": 0.8333333333333334,
            "f1": 0.7936507936507938
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7692307692307693,
            "rec": 1.0,
            "f1": 0.8695652173913044
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.6896551724137931,
            "rec": 1.0,
            "f1": 0.8163265306122449
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5492063492063493
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.36923076923076925
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4429223744292237
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6843434343434344
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5705128205128205
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.41134751773049644
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7730263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7641509433962265
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7537878787878788
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.5
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8785714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5841269841269842
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4307692307692308
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6256983240223464
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9803439803439803
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7853535353535354
        },
        "ocr (MMBench_EN)": {
            "acc": 0.6987179487179487
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5177304964539007
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8223684210526315
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9886363636363636
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5106382978723404
        },
        "image_style (MMBench_EN)": {
            "acc": 0.839622641509434
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.47333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.6
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.3
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.43333333333333335
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.3
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26639344262295084
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24836601307189543
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9315068493150684
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8928571428571429
        },
        "Materials (ScienceQA)": {
            "acc": 0.8111888111888111
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3387096774193548
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.5952380952380952
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.6304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5882352941176471
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.43661971830985913
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.549492385786802
        },
        "3D Distance (CVBench)": {
            "acc": 0.5383333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6615384615384615
        },
        "3D Depth (CVBench)": {
            "acc": 0.7383333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.846222658157348,
            "bert": 0.8401602631807328
        },
        "Whole dataset (Enrico)": {
            "bart": -6.110960874557495,
            "bert": 0.9789944618940354
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.9733030891418455,
            "bert": 0.9547742956876755
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.336983324289322,
            "bert": 0.9040394324064255
        },
        "Whole dataset (GQA)": {
            "bart": -3.4202578508853914,
            "bert": 0.9923670160770416
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.433205189704895,
            "bert": 0.8967836391925812
        },
        "Whole dataset (INAT)": {
            "bart": -6.471917290687561,
            "bert": 0.7797757506370544
        },
        "Whole dataset (IRFL)": {
            "bart": -4.561308596134186,
            "bert": 0.9985934609174728
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8172745084762574,
            "bert": 0.8740748959779739
        },
        "Whole dataset (Memotion)": {
            "bart": -4.858389501571655,
            "bert": 0.8476676839590073
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.058689284324646,
            "bert": 0.8326566159725189
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.973930990695953,
            "bert": 0.9961263036727905
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6330887126922606,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4090675002336503,
            "bert": 0.9346354323625564
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2166034960746765,
            "bert": 0.9419032418727875
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.177662610411644,
            "bert": 0.8666558051109314
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.896234664916992,
            "bert": 0.9111434584856033
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.639998722076416,
            "bert": 0.8967201590538025
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.080932192802429,
            "bert": 0.84635788500309
        },
        "Whole dataset (Slake)": {
            "bart": -3.699041987657547,
            "bert": 0.9934141856431961
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.361357400417328,
            "bert": 0.9205162274837494
        },
        "Whole dataset (VCR)": {
            "bart": -3.0167904613912104,
            "bert": 0.9277946531772614
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.525890312194824,
            "bert": 0.9110728448629379
        },
        "Whole dataset (VQA)": {
            "bart": -4.072075002193451,
            "bert": 0.9698842662572861
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.481262813806534,
            "bert": 0.9387158071994781
        },
        "Whole dataset (Winoground)": {
            "bart": -4.100420280694961,
            "bert": 0.9980127048492432
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9508196721311475,
            "rec": 0.7837837837837838,
            "f1": 0.8592592592592593
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9111111111111111,
            "rec": 0.7834394904458599,
            "f1": 0.8424657534246577
        },
        "adversarial (POPE)": {
            "acc": 0.916083916083916,
            "prec": 0.9576271186440678,
            "rec": 0.8560606060606061,
            "f1": 0.904
        }
    },
    "prism-clip-controlled+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6379866637986664
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25149700598802394
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2440401505646173
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2754491017964072
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.23
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5435226808336738
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2310204081632653
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2942528735632184
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.28174603174603174
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24718356527501656
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5511247443762781
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.682077264091197
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6444565811032222
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.29545454545454547
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.26147704590818366
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.471841704718417
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.21299638989169675
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.26588465298142716
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6978851963746223
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.225
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2613065326633166
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.6823529411764706,
            "prec": 0.8690476190476191,
            "rec": 0.4294117647058823,
            "f1": 0.5748031496062992
        },
        "posters (MME)": {
            "acc": 0.8435374149659864,
            "prec": 0.9902912621359223,
            "rec": 0.6938775510204082,
            "f1": 0.8160000000000001
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6666666666666666,
            "rec": 0.8666666666666667,
            "f1": 0.7536231884057971
        },
        "scene (MME)": {
            "acc": 0.8625,
            "prec": 0.8795811518324608,
            "rec": 0.84,
            "f1": 0.8593350383631715
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6944444444444444,
            "rec": 0.7142857142857143,
            "f1": 0.7042253521126761
        },
        "artwork (MME)": {
            "acc": 0.7125,
            "prec": 0.673469387755102,
            "rec": 0.825,
            "f1": 0.7415730337078652
        },
        "landmark (MME)": {
            "acc": 0.8475,
            "prec": 0.8797814207650273,
            "rec": 0.805,
            "f1": 0.8407310704960835
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.3333333333333333,
            "rec": 0.2,
            "f1": 0.25
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8709677419354839,
            "rec": 0.9,
            "f1": 0.8852459016393444
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8529411764705882,
            "rec": 0.9666666666666667,
            "f1": 0.90625
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.6666666666666666,
            "rec": 0.8,
            "f1": 0.7272727272727272
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4857142857142857,
            "rec": 0.85,
            "f1": 0.6181818181818183
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8488372093023255
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5523809523809524
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3835616438356164
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5460992907801419
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.3128491620111732
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7070707070707071
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7115384615384616
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3723404255319149
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.88
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7730263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.44680851063829785
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7196969696969697
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.43333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8857142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9418604651162791
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5777777777777777
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5707762557077626
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4581005586592179
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9023255813953488
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9656019656019657
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7777777777777778
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7115384615384616
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.46808510638297873
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.88
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8256578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9829545454545454
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.48936170212765956
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7971698113207547
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.33986928104575165
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3522727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8061224489795918
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.24193548387096775
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4634146341463415
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8043478260869565
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8275862068965517
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5545685279187818
        },
        "3D Distance (CVBench)": {
            "acc": 0.5
        },
        "2D Relation (CVBench)": {
            "acc": 0.6292307692307693
        },
        "3D Depth (CVBench)": {
            "acc": 0.6
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.703683168888092,
            "bert": 0.7968106317520142
        },
        "Whole dataset (Enrico)": {
            "bart": -6.8843290078639985,
            "bert": 0.841948921084404
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.069225752353669,
            "bert": 0.9695846283435822
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3300271904468537,
            "bert": 0.8639115160703659
        },
        "Whole dataset (GQA)": {
            "bart": -3.545389168262482,
            "bert": 0.9923212331533432
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.778798217773438,
            "bert": 0.8200863391160965
        },
        "Whole dataset (INAT)": {
            "bart": -5.968601403236389,
            "bert": 0.7931949621438981
        },
        "Whole dataset (IRFL)": {
            "bart": -4.22140382528305,
            "bert": 0.9986670821905136
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.863677830696106,
            "bert": 0.8536843192577362
        },
        "Whole dataset (Memotion)": {
            "bart": -4.601532406806946,
            "bert": 0.8935212516784667
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9674532544612884,
            "bert": 0.8406081163883209
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.81044764995575,
            "bert": 0.998267440199852
        },
        "Whole dataset (NLVR)": {
            "bart": -2.8531711721420288,
            "bert": 0.9991113150119781
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.1261186838150024,
            "bert": 0.9991096413135528
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.5072880744934083,
            "bert": 0.879620510339737
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.152530806064606,
            "bert": 0.9376471984386444
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.912369887828827,
            "bert": 0.8538782048225403
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.024049744606018,
            "bert": 0.9124709093570709
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.490508041381836,
            "bert": 0.90179807305336
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.2021807146072385,
            "bert": 0.8548032593727112
        },
        "Whole dataset (Slake)": {
            "bart": -3.9063467955589295,
            "bert": 0.9947459852695465
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.565026015043259,
            "bert": 0.8299304813146591
        },
        "Whole dataset (VCR)": {
            "bart": -3.4256841230392454,
            "bert": 0.9197642076015472
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6628722459077836,
            "bert": 0.9049106705188751
        },
        "Whole dataset (VQA)": {
            "bart": -4.357558701038361,
            "bert": 0.9696074342727661
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.576514037847519,
            "bert": 0.9409989047050477
        },
        "Whole dataset (Winoground)": {
            "bart": -4.807271153926849,
            "bert": 0.9978318756818771
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9824561403508771,
            "rec": 0.7567567567567568,
            "f1": 0.8549618320610687
        },
        "popular (POPE)": {
            "acc": 0.8306188925081434,
            "prec": 0.8947368421052632,
            "rec": 0.7579617834394905,
            "f1": 0.8206896551724138
        },
        "adversarial (POPE)": {
            "acc": 0.8916083916083916,
            "prec": 0.963302752293578,
            "rec": 0.7954545454545454,
            "f1": 0.8713692946058091
        }
    },
    "prism-clip-controlled+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.701871370187137
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6806387225548902
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5301129234629862
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.812
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.623212096444626
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3689795918367347
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.35813492063492064
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.44201457919151754
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6411042944785276
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7495250158328056
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7269251774986346
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.23484848484848486
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5129740518962076
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5190258751902588
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5018050541516246
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3128054740957967
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.375
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5471698113207547
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7636363636363637
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6130653266331658
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.32653061224489793
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.7705882352941177,
            "prec": 0.8770491803278688,
            "rec": 0.6294117647058823,
            "f1": 0.732876712328767
        },
        "posters (MME)": {
            "acc": 0.8639455782312925,
            "prec": 0.9212598425196851,
            "rec": 0.7959183673469388,
            "f1": 0.854014598540146
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6511627906976745,
            "rec": 0.9333333333333333,
            "f1": 0.767123287671233
        },
        "scene (MME)": {
            "acc": 0.8675,
            "prec": 0.893048128342246,
            "rec": 0.835,
            "f1": 0.8630490956072352
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7214285714285714,
            "prec": 0.6867469879518072,
            "rec": 0.8142857142857143,
            "f1": 0.7450980392156863
        },
        "artwork (MME)": {
            "acc": 0.7125,
            "prec": 0.6720647773279352,
            "rec": 0.83,
            "f1": 0.7427293064876956
        },
        "landmark (MME)": {
            "acc": 0.74,
            "prec": 0.9528301886792453,
            "rec": 0.505,
            "f1": 0.6601307189542484
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.3076923076923077,
            "rec": 0.2,
            "f1": 0.24242424242424246
        },
        "count (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.725,
            "prec": 0.6956521739130435,
            "rec": 0.8,
            "f1": 0.7441860465116279
        },
        "code_reasoning (MME)": {
            "acc": 0.35,
            "prec": 0.36363636363636365,
            "rec": 0.4,
            "f1": 0.380952380952381
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8604651162790697
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5968253968253968
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5844748858447488
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5083798882681564
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7045454545454546
        },
        "ocr (MMBench_CN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8355263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5638297872340425
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8068181818181818
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6031746031746031
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.6
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5981735159817352
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6382978723404256
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5865921787709497
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8106060606060606
        },
        "ocr (MMBench_EN)": {
            "acc": 0.782051282051282
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3954802259887006
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.49645390070921985
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.89
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7830188679245284
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8863636363636364
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.47333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8928571428571429
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.3
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.4
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.4
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29098360655737704
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3137254901960784
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2549019607843137
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.375
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7945205479452054
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.958904109589041
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8251748251748252
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.2903225806451613
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.7619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.603448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9565217391304348
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.35185185185185186
        },
        "Maps (ScienceQA)": {
            "acc": 0.5
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.7848101265822784
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "2D Count (CVBench)": {
            "acc": 0.583756345177665
        },
        "3D Distance (CVBench)": {
            "acc": 0.6333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6861538461538461
        },
        "3D Depth (CVBench)": {
            "acc": 0.7716666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.920208263397217,
            "bert": 0.7760637521743774
        },
        "Whole dataset (Enrico)": {
            "bart": -5.7349999403953555,
            "bert": 0.967097801566124
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.052630889415741,
            "bert": 0.9952126723527909
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3160802006721495,
            "bert": 0.8660068571567535
        },
        "Whole dataset (GQA)": {
            "bart": -3.6927757275104525,
            "bert": 0.9936550390720368
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.04936158657074,
            "bert": 0.8008308118581772
        },
        "Whole dataset (INAT)": {
            "bart": -6.355461144447327,
            "bert": 0.7951548707485199
        },
        "Whole dataset (IRFL)": {
            "bart": -4.344285900592804,
            "bert": 0.9986349385976792
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.978014986515045,
            "bert": 0.8498856818675995
        },
        "Whole dataset (Memotion)": {
            "bart": -4.351984198093414,
            "bert": 0.900815778374672
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.478283170461655,
            "bert": 0.8832865983247757
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.483718398809433,
            "bert": 0.9983532762527466
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5881439542770384,
            "bert": 0.9992584383487702
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.416148691177368,
            "bert": 0.9993094396591187
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4535485816001894,
            "bert": 0.8821736079454422
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.170016947388649,
            "bert": 0.9380715477466584
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.268165600299835,
            "bert": 0.861350246667862
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.103693082332611,
            "bert": 0.9144354873895645
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.116001652479172,
            "bert": 0.9311477267742156
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.095445022583008,
            "bert": 0.8600175601243972
        },
        "Whole dataset (Slake)": {
            "bart": -4.084658929109573,
            "bert": 0.9946846514940262
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.308660079836845,
            "bert": 0.9096476501226425
        },
        "Whole dataset (VCR)": {
            "bart": -2.8301729387044907,
            "bert": 0.9373916709423065
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5869786328077318,
            "bert": 0.9048628824949264
        },
        "Whole dataset (VQA)": {
            "bart": -4.14133683681488,
            "bert": 0.9703447014093399
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.678714118003845,
            "bert": 0.9234856921434402
        },
        "Whole dataset (Winoground)": {
            "bart": -4.763069692850113,
            "bert": 0.9978369021415711
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9910714285714286,
            "rec": 0.75,
            "f1": 0.8538461538461538
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.937007874015748,
            "rec": 0.7579617834394905,
            "f1": 0.8380281690140845
        },
        "adversarial (POPE)": {
            "acc": 0.8986013986013986,
            "prec": 0.963963963963964,
            "rec": 0.8106060606060606,
            "f1": 0.8806584362139919
        }
    },
    "prism-clip+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6573456657345665
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25031367628607276
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.246
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5659991826726604
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24326530612244898
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.31264367816091954
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2619047619047619
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24983432736911862
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5950920245398773
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6871437618746041
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6602949208083014
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.26582278481012656
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.502283105022831
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.21660649819494585
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2561094819159335
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7039274924471299
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.175
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.27575757575757576
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.271356783919598
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.19753086419753085
        },
        "celebrity (MME)": {
            "acc": 0.6147058823529412,
            "prec": 0.5730337078651685,
            "rec": 0.9,
            "f1": 0.700228832951945
        },
        "posters (MME)": {
            "acc": 0.8741496598639455,
            "prec": 0.9910714285714286,
            "rec": 0.7551020408163265,
            "f1": 0.8571428571428571
        },
        "position (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.5769230769230769,
            "rec": 1.0,
            "f1": 0.7317073170731707
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8557213930348259,
            "rec": 0.86,
            "f1": 0.85785536159601
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6571428571428571,
            "prec": 0.6222222222222222,
            "rec": 0.8,
            "f1": 0.7000000000000001
        },
        "artwork (MME)": {
            "acc": 0.6825,
            "prec": 0.622895622895623,
            "rec": 0.925,
            "f1": 0.744466800804829
        },
        "landmark (MME)": {
            "acc": 0.8,
            "prec": 0.7290076335877863,
            "rec": 0.955,
            "f1": 0.8268398268398269
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.5294117647058824,
            "rec": 0.9,
            "f1": 0.6666666666666667
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.46875,
            "rec": 0.75,
            "f1": 0.5769230769230769
        },
        "count (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.58,
            "rec": 0.9666666666666667,
            "f1": 0.725
        },
        "color (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.6976744186046512,
            "rec": 1.0,
            "f1": 0.8219178082191781
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.625,
            "rec": 1.0,
            "f1": 0.7692307692307693
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.877906976744186
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5365079365079365
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.46153846153846156
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4155251141552511
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4581005586592179
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9606879606879607
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6893939393939394
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5705128205128205
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.36524822695035464
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7401315789473685
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.40425531914893614
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6933962264150944
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7121212121212122
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9244186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6095238095238096
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5707762557077626
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6312056737588653
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6815642458100558
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8308080808080808
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4180790960451977
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.43617021276595747
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8322368421052632
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7264150943396226
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4666666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.8714285714285714
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.6
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.33986928104575165
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.27058823529411763
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6986301369863014
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9711538461538461
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9315068493150684
        },
        "States of matter (ScienceQA)": {
            "acc": 0.6785714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3064516129032258
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.24390243902439024
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8727272727272727
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6410256410256411
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6862745098039216
        },
        "Classification (ScienceQA)": {
            "acc": 0.7974683544303798
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5526315789473685
        },
        "2D Count (CVBench)": {
            "acc": 0.5812182741116751
        },
        "3D Distance (CVBench)": {
            "acc": 0.5666666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.6523076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.7616666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.922218413352966,
            "bert": 0.8360245543718338
        },
        "Whole dataset (Enrico)": {
            "bart": -6.653181052207946,
            "bert": 0.9899534440040588
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.782591359615326,
            "bert": 0.9953807109594345
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.385889345407486,
            "bert": 0.8961845958232879
        },
        "Whole dataset (GQA)": {
            "bart": -3.318615049123764,
            "bert": 0.9937858027219773
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.246055512428284,
            "bert": 0.965741581916809
        },
        "Whole dataset (INAT)": {
            "bart": -6.0670743179321285,
            "bert": 0.7862455236911774
        },
        "Whole dataset (IRFL)": {
            "bart": -4.400547120571137,
            "bert": 0.9986261409521103
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.877976679801941,
            "bert": 0.8743713444471359
        },
        "Whole dataset (Memotion)": {
            "bart": -4.587972803115845,
            "bert": 0.9019030207395553
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.975340062379837,
            "bert": 0.8366325801610947
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.541551352739334,
            "bert": 0.9974452418088913
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992867928743362
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.4596784722805025,
            "bert": 0.9992800056934357
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4561942446231844,
            "bert": 0.9264698231220245
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.9186823838949203,
            "bert": 0.9390802091360092
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.979343087673187,
            "bert": 0.8435325127840042
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.780983698368073,
            "bert": 0.9076455301046371
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.540107679367066,
            "bert": 0.8826703423261643
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.151615869998932,
            "bert": 0.8439867800474167
        },
        "Whole dataset (Slake)": {
            "bart": -4.018813283443451,
            "bert": 0.9939580017328262
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.928201993703842,
            "bert": 0.83463219165802
        },
        "Whole dataset (VCR)": {
            "bart": -3.221253266185522,
            "bert": 0.9243526673316955
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6724578911066055,
            "bert": 0.9076018822193146
        },
        "Whole dataset (VQA)": {
            "bart": -4.082834161520005,
            "bert": 0.972195143699646
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.709229108095169,
            "bert": 0.935331820845604
        },
        "Whole dataset (Winoground)": {
            "bart": -4.184254230260849,
            "bert": 0.9980225318670273
        },
        "random (POPE)": {
            "acc": 0.8827361563517915,
            "prec": 0.9827586206896551,
            "rec": 0.7702702702702703,
            "f1": 0.8636363636363636
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.9224806201550387,
            "rec": 0.7579617834394905,
            "f1": 0.8321678321678323
        },
        "adversarial (POPE)": {
            "acc": 0.8881118881118881,
            "prec": 0.9716981132075472,
            "rec": 0.7803030303030303,
            "f1": 0.8655462184873949
        }
    },
    "prism-clip+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7278984727898473
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.688622754491018
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5520702634880803
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.281437125748503
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.836
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6244380874540253
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.42775510204081635
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6459770114942529
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.3531746031746032
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.43803843605036447
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6595092024539877
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.711340206185567
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7615579480683977
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.766794101583834
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9493670886075949
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2196969696969697
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5828343313373253
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5692541856925418
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5379061371841155
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3626588465298143
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.8036253776435045
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.36666666666666664
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5534591194968553
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7666666666666667
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6030150753768844
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2716049382716049
        },
        "celebrity (MME)": {
            "acc": 0.8029411764705883,
            "prec": 0.7288888888888889,
            "rec": 0.9647058823529412,
            "f1": 0.8303797468354431
        },
        "posters (MME)": {
            "acc": 0.8673469387755102,
            "prec": 0.8214285714285714,
            "rec": 0.9387755102040817,
            "f1": 0.8761904761904763
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8095238095238095,
            "rec": 0.935,
            "f1": 0.8677494199535963
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.65,
            "prec": 0.6082474226804123,
            "rec": 0.8428571428571429,
            "f1": 0.7065868263473053
        },
        "artwork (MME)": {
            "acc": 0.615,
            "prec": 0.5660919540229885,
            "rec": 0.985,
            "f1": 0.718978102189781
        },
        "landmark (MME)": {
            "acc": 0.8225,
            "prec": 0.7676348547717843,
            "rec": 0.925,
            "f1": 0.8390022675736961
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.5555555555555556,
            "rec": 1.0,
            "f1": 0.7142857142857143
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9375,
            "rec": 1.0,
            "f1": 0.967741935483871
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.5277777777777778,
            "rec": 0.95,
            "f1": 0.6785714285714285
        },
        "count (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5454545454545454,
            "rec": 1.0,
            "f1": 0.7058823529411764
        },
        "color (MME)": {
            "acc": 0.75,
            "prec": 0.6666666666666666,
            "rec": 1.0,
            "f1": 0.8
        },
        "OCR (MME)": {
            "acc": 0.575,
            "prec": 0.5405405405405406,
            "rec": 1.0,
            "f1": 0.7017543859649124
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.95,
            "f1": 0.6551724137931034
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8488372093023255
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.580952380952381
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4931506849315068
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5027932960893855
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9459459459459459
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7171717171717171
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6923076923076923
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4787234042553192
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7697368421052632
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9659090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.6702127659574468
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7311320754716981
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7992424242424242
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3933333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9428571428571428
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.936046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.5873015873015873
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.46923076923076923
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5570776255707762
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.7163120567375887
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.851010101010101
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7884615384615384
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5425531914893617
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8157894736842105
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.7021276595744681
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.928030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5466666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.4
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Electronics (MMMU)": {
            "acc": 0.3
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.7
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2581967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.25882352941176473
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20098039215686275
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.4772727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9967948717948718
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8671328671328671
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.12195121951219512
        },
        "Geography (ScienceQA)": {
            "acc": 0.7619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6896551724137931
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.35555555555555557
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6666666666666666
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.7721518987341772
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.6338028169014085
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.6027918781725888
        },
        "3D Distance (CVBench)": {
            "acc": 0.6066666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.6846153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.6733333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.916876835823059,
            "bert": 0.8324528259038925
        },
        "Whole dataset (Enrico)": {
            "bart": -6.397005808353424,
            "bert": 0.9322125750780106
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.098306407928467,
            "bert": 0.9991874122619628
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.4299276852607727,
            "bert": 0.8968093878030777
        },
        "Whole dataset (GQA)": {
            "bart": -3.6992023479938507,
            "bert": 0.993629721403122
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.264246463775635,
            "bert": 0.9584327590465546
        },
        "Whole dataset (INAT)": {
            "bart": -6.186061955690384,
            "bert": 0.7942460805177689
        },
        "Whole dataset (IRFL)": {
            "bart": -4.480927858352661,
            "bert": 0.9986098009347916
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8892730975151064,
            "bert": 0.8742340785264969
        },
        "Whole dataset (Memotion)": {
            "bart": -4.64948281288147,
            "bert": 0.8999222421646118
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.4433711767196655,
            "bert": 0.892408223748207
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.558361438512802,
            "bert": 0.9983362174034118
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6169268321990966,
            "bert": 0.9992862486839295
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.620541605949402,
            "bert": 0.9992835080623627
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4437686681747435,
            "bert": 0.9256270688772201
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.9224360316991804,
            "bert": 0.9408127051591874
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.9372864371538165,
            "bert": 0.83067478120327
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.055160813331604,
            "bert": 0.9119795280694961
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.057694832086563,
            "bert": 0.89892991065979
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.986137697696686,
            "bert": 0.8501524990797042
        },
        "Whole dataset (Slake)": {
            "bart": -3.88564173579216,
            "bert": 0.9946919506788254
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.4405304312705995,
            "bert": 0.8670772898197174
        },
        "Whole dataset (VCR)": {
            "bart": -2.9707524952292443,
            "bert": 0.9334049075841904
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5908963376283647,
            "bert": 0.9088056081533432
        },
        "Whole dataset (VQA)": {
            "bart": -4.358592412471771,
            "bert": 0.967412475347519
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.521451369524002,
            "bert": 0.9324011999368668
        },
        "Whole dataset (Winoground)": {
            "bart": -4.115933251380921,
            "bert": 0.9980040842294693
        },
        "random (POPE)": {
            "acc": 0.8859934853420195,
            "prec": 1.0,
            "rec": 0.7635135135135135,
            "f1": 0.8659003831417624
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.936,
            "rec": 0.7452229299363057,
            "f1": 0.8297872340425532
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9814814814814815,
            "rec": 0.803030303030303,
            "f1": 0.8833333333333332
        }
    },
    "prism-siglip-controlled+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6431490643149065
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25156838143036386
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.244
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5692684920310584
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2383673469387755
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2620689655172414
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.27976190476190477
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24519549370444002
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5715746421267893
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6391752577319587
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6833438885370487
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6493719279082468
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.22784810126582278
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.3106060606060606
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4855403348554033
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20216606498194944
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24731182795698925
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6888217522658611
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.225
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.25125628140703515
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.30864197530864196
        },
        "celebrity (MME)": {
            "acc": 0.6764705882352942,
            "prec": 0.9545454545454546,
            "rec": 0.37058823529411766,
            "f1": 0.5338983050847458
        },
        "posters (MME)": {
            "acc": 0.8299319727891157,
            "prec": 0.970873786407767,
            "rec": 0.6802721088435374,
            "f1": 0.8
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6904761904761905,
            "rec": 0.9666666666666667,
            "f1": 0.8055555555555556
        },
        "scene (MME)": {
            "acc": 0.8375,
            "prec": 0.8947368421052632,
            "rec": 0.765,
            "f1": 0.8247978436657682
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.7272727272727273,
            "rec": 0.6857142857142857,
            "f1": 0.7058823529411764
        },
        "artwork (MME)": {
            "acc": 0.71,
            "prec": 0.7258064516129032,
            "rec": 0.675,
            "f1": 0.6994818652849741
        },
        "landmark (MME)": {
            "acc": 0.765,
            "prec": 0.9568965517241379,
            "rec": 0.555,
            "f1": 0.7025316455696202
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.8275862068965517,
            "rec": 0.8,
            "f1": 0.8135593220338982
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.75,
            "rec": 0.6,
            "f1": 0.6666666666666665
        },
        "code_reasoning (MME)": {
            "acc": 0.425,
            "prec": 0.45454545454545453,
            "rec": 0.75,
            "f1": 0.5660377358490566
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8662790697674418
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5555555555555556
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4063926940639269
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.574468085106383
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.3687150837988827
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7247474747474747
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7051282051282052
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3220338983050847
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.375886524822695
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7960526315789473
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4148936170212766
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7424242424242424
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6634920634920635
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5388127853881278
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6524822695035462
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.45251396648044695
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8358585858585859
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7307692307692307
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.44680851063829785
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4574468085106383
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8207547169811321
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8939393939393939
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4533333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Agriculture (MMMU)": {
            "acc": 0.5666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27049180327868855
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3006535947712418
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20098039215686275
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8214285714285714
        },
        "Materials (ScienceQA)": {
            "acc": 0.8811188811188811
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.8125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.8607594936708861
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.5824873096446701
        },
        "3D Distance (CVBench)": {
            "acc": 0.53
        },
        "2D Relation (CVBench)": {
            "acc": 0.6323076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.7116666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.645058262348175,
            "bert": 0.7903962582349777
        },
        "Whole dataset (Enrico)": {
            "bart": -6.62453697681427,
            "bert": 0.9646989971399307
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.079945197105408,
            "bert": 0.9917664247751236
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2889651238918303,
            "bert": 0.8654610794782639
        },
        "Whole dataset (GQA)": {
            "bart": -3.6962203335762025,
            "bert": 0.9936356031894684
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.353808283805847,
            "bert": 0.9465228766202927
        },
        "Whole dataset (INAT)": {
            "bart": -5.94265302658081,
            "bert": 0.7924408298730851
        },
        "Whole dataset (IRFL)": {
            "bart": -4.22140382528305,
            "bert": 0.9986670821905136
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.93541175365448,
            "bert": 0.8506426495313645
        },
        "Whole dataset (Memotion)": {
            "bart": -4.562209601402283,
            "bert": 0.8904830676317215
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9434585583209993,
            "bert": 0.8490727424621582
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.510706765651703,
            "bert": 0.9983460932970047
        },
        "Whole dataset (NLVR)": {
            "bart": -3.539438226222992,
            "bert": 0.999238189458847
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.196232511997223,
            "bert": 0.9991863226890564
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.443071048259735,
            "bert": 0.8822238194942474
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1738046288490294,
            "bert": 0.9374566519260407
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901273334026337,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.304702520370483,
            "bert": 0.911097200512886
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.326113171577454,
            "bert": 0.9206238842010498
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.093426234722138,
            "bert": 0.8562580341100693
        },
        "Whole dataset (Slake)": {
            "bart": -4.193607692718506,
            "bert": 0.9947167634963989
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.575908712148666,
            "bert": 0.8393613648414612
        },
        "Whole dataset (VCR)": {
            "bart": -3.4224436882138254,
            "bert": 0.9188192868232727
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7718954282999038,
            "bert": 0.904228868484497
        },
        "Whole dataset (VQA)": {
            "bart": -4.0761272549629215,
            "bert": 0.9718006777763367
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.506941033601761,
            "bert": 0.9417434346675873
        },
        "Whole dataset (Winoground)": {
            "bart": -5.119071797132492,
            "bert": 0.9977494513988495
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9663865546218487,
            "rec": 0.777027027027027,
            "f1": 0.8614232209737828
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9111111111111111,
            "rec": 0.7834394904458599,
            "f1": 0.8424657534246577
        },
        "adversarial (POPE)": {
            "acc": 0.8846153846153846,
            "prec": 0.9805825242718447,
            "rec": 0.7651515151515151,
            "f1": 0.8595744680851064
        }
    },
    "prism-siglip-controlled+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.702516670251667
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6307385229540918
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5282308657465495
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.8
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6195341234164283
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3820408163265306
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5793103448275863
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2371031746031746
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4479787939032472
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6349693251533742
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6701030927835051
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7441418619379354
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7192790824685964
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9240506329113924
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.45708582834331335
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5098934550989346
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5812274368231047
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3313782991202346
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7673716012084593
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.375
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.44025157232704404
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.5678391959798995
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.6647058823529411,
            "prec": 0.9516129032258065,
            "rec": 0.34705882352941175,
            "f1": 0.5086206896551724
        },
        "posters (MME)": {
            "acc": 0.826530612244898,
            "prec": 0.9705882352941176,
            "rec": 0.673469387755102,
            "f1": 0.7951807228915663
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "scene (MME)": {
            "acc": 0.84,
            "prec": 0.8953488372093024,
            "rec": 0.77,
            "f1": 0.8279569892473118
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7142857142857143,
            "prec": 0.7205882352941176,
            "rec": 0.7,
            "f1": 0.7101449275362319
        },
        "artwork (MME)": {
            "acc": 0.7075,
            "prec": 0.7195767195767195,
            "rec": 0.68,
            "f1": 0.6992287917737788
        },
        "landmark (MME)": {
            "acc": 0.765,
            "prec": 0.9491525423728814,
            "rec": 0.56,
            "f1": 0.7044025157232704
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.8214285714285714,
            "rec": 0.7666666666666667,
            "f1": 0.793103448275862
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.7,
            "prec": 0.7857142857142857,
            "rec": 0.55,
            "f1": 0.6470588235294117
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48484848484848486,
            "rec": 0.8,
            "f1": 0.6037735849056605
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.872093023255814
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5492063492063493
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.6615384615384615
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4018264840182648
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5602836879432624
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.35195530726256985
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9116279069767442
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9533169533169533
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7272727272727273
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7243589743589743
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.36879432624113473
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.835
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7927631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.40425531914893614
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7924528301886793
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7310606060606061
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.38666666666666666
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6603174603174603
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5769230769230769
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5342465753424658
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6524822695035462
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.46368715083798884
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8308080808080808
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2937853107344633
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.44680851063829785
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8552631578947368
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.43617021276595747
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8066037735849056
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8939393939393939
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.4
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Agriculture (MMMU)": {
            "acc": 0.5666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.30327868852459017
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.11904761904761904
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.26143790849673204
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.25882352941176473
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18137254901960784
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3977272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8671328671328671
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.8125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.23943661971830985
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.5901015228426396
        },
        "3D Distance (CVBench)": {
            "acc": 0.535
        },
        "2D Relation (CVBench)": {
            "acc": 0.6384615384615384
        },
        "3D Depth (CVBench)": {
            "acc": 0.715
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.6307702267169955,
            "bert": 0.7944670361280441
        },
        "Whole dataset (Enrico)": {
            "bart": -6.630043118000031,
            "bert": 0.9669093304872513
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.006334506273269,
            "bert": 0.9917467612028122
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2923317623138426,
            "bert": 0.8659842997789383
        },
        "Whole dataset (GQA)": {
            "bart": -3.6812931060791017,
            "bert": 0.9936390155553818
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.247535458803177,
            "bert": 0.9524038070440293
        },
        "Whole dataset (INAT)": {
            "bart": -6.315373721122742,
            "bert": 0.7922744500637054
        },
        "Whole dataset (IRFL)": {
            "bart": -4.191546609401703,
            "bert": 0.9986739057302475
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.95210901260376,
            "bert": 0.8512189215421677
        },
        "Whole dataset (Memotion)": {
            "bart": -4.561024277210236,
            "bert": 0.8906562972068787
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.9159243255853653,
            "bert": 0.8509210288524628
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.525635373592377,
            "bert": 0.9983426815271378
        },
        "Whole dataset (NLVR)": {
            "bart": -3.4508906412124634,
            "bert": 0.9992330634593963
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.212468633651733,
            "bert": 0.9991930723190308
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.438807199001312,
            "bert": 0.8825629836320877
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1951424634456633,
            "bert": 0.9358917397260665
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901272956132889,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.179961249828339,
            "bert": 0.9113796436786652
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.650787115097046,
            "bert": 0.9223628729581833
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.073904700279236,
            "bert": 0.855007985830307
        },
        "Whole dataset (Slake)": {
            "bart": -4.177139358520508,
            "bert": 0.9947240734100342
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.579325268268585,
            "bert": 0.8405784702301026
        },
        "Whole dataset (VCR)": {
            "bart": -3.372176761329174,
            "bert": 0.9196517014503479
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.705931572318077,
            "bert": 0.9042213380336761
        },
        "Whole dataset (VQA)": {
            "bart": -4.102211247682572,
            "bert": 0.97179880797863
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.585032857656479,
            "bert": 0.940730242729187
        },
        "Whole dataset (Winoground)": {
            "bart": -4.963463656902313,
            "bert": 0.9977880591154098
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9663865546218487,
            "rec": 0.777027027027027,
            "f1": 0.8614232209737828
        },
        "popular (POPE)": {
            "acc": 0.8501628664495114,
            "prec": 0.9111111111111111,
            "rec": 0.7834394904458599,
            "f1": 0.8424657534246577
        },
        "adversarial (POPE)": {
            "acc": 0.8811188811188811,
            "prec": 0.9711538461538461,
            "rec": 0.7651515151515151,
            "f1": 0.8559322033898304
        }
    },
    "prism-siglip+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6663798666379867
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.21956087824351297
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24968632371392724
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.236
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5847977114834492
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.25142857142857145
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.31494252873563217
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26686507936507936
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25646123260437376
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6012269938650306
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6934768841038632
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6739486619333698
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5068493150684932
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24187725631768953
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2287390029325513
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6948640483383686
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.23270440251572327
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.65,
            "prec": 0.5962264150943396,
            "rec": 0.9294117647058824,
            "f1": 0.7264367816091953
        },
        "posters (MME)": {
            "acc": 0.8605442176870748,
            "prec": 0.8897058823529411,
            "rec": 0.8231292517006803,
            "f1": 0.8551236749116609
        },
        "position (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.5769230769230769,
            "rec": 1.0,
            "f1": 0.7317073170731707
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.8529411764705882,
            "rec": 0.87,
            "f1": 0.8613861386138614
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6428571428571429,
            "prec": 0.6111111111111112,
            "rec": 0.7857142857142857,
            "f1": 0.6875000000000001
        },
        "artwork (MME)": {
            "acc": 0.66,
            "prec": 0.6038961038961039,
            "rec": 0.93,
            "f1": 0.7322834645669292
        },
        "landmark (MME)": {
            "acc": 0.815,
            "prec": 0.7669491525423728,
            "rec": 0.905,
            "f1": 0.8302752293577982
        },
        "text_translation (MME)": {
            "acc": 0.475,
            "prec": 0.48,
            "rec": 0.6,
            "f1": 0.5333333333333332
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 0.9666666666666667,
            "rec": 0.9666666666666667,
            "f1": 0.9666666666666667
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.45454545454545453,
            "rec": 0.75,
            "f1": 0.5660377358490566
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7,
            "rec": 0.9333333333333333,
            "f1": 0.8
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8108108108108109,
            "rec": 1.0,
            "f1": 0.8955223880597014
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.7037037037037037,
            "rec": 0.95,
            "f1": 0.8085106382978724
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5079365079365079
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5769230769230769
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3972602739726027
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.4245810055865922
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9631449631449631
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7070707070707071
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5769230769230769
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3389830508474576
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3971631205673759
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7828947368421053
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6933962264150944
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.6931818181818182
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4533333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6412698412698413
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5068493150684932
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6312056737588653
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9488372093023256
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.851010101010101
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7051282051282052
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4011299435028249
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8519736842105263
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6063829787234043
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8113207547169812
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8636363636363636
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.52
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.95
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.5
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Physics (MMMU)": {
            "acc": 0.4
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5333333333333333
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.3
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.4
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.9
        },
        "Biology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.5666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2540983606557377
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3202614379084967
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2235294117647059
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.22549019607843138
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.684931506849315
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9839743589743589
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8061224489795918
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8727272727272727
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.5217391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6274509803921569
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.43661971830985913
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.6078680203045685
        },
        "3D Distance (CVBench)": {
            "acc": 0.56
        },
        "2D Relation (CVBench)": {
            "acc": 0.6
        },
        "3D Depth (CVBench)": {
            "acc": 0.7516666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.850620765686035,
            "bert": 0.8104594647884369
        },
        "Whole dataset (Enrico)": {
            "bart": -6.4600582432746885,
            "bert": 0.9897563052177429
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.043516886234284,
            "bert": 0.9973689723014831
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.339573620557785,
            "bert": 0.898662810921669
        },
        "Whole dataset (GQA)": {
            "bart": -3.4306939685344697,
            "bert": 0.9923322629928589
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.270197629928589,
            "bert": 0.9620837652683258
        },
        "Whole dataset (INAT)": {
            "bart": -5.915265398025513,
            "bert": 0.7880525809526443
        },
        "Whole dataset (IRFL)": {
            "bart": -4.451070642471313,
            "bert": 0.9986166244745255
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.890031888484955,
            "bert": 0.8696183824539184
        },
        "Whole dataset (Memotion)": {
            "bart": -4.706565036773681,
            "bert": 0.8961745125055313
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.023174446821213,
            "bert": 0.8334123510122299
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.798387891054153,
            "bert": 0.9982712113857269
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992925268411637
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.237563418149948,
            "bert": 0.9992351937294006
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.3708646523952486,
            "bert": 0.9299364161491394
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.9850795125961302,
            "bert": 0.9379026114940643
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.91612601518631,
            "bert": 0.8497170954942703
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.108228936195373,
            "bert": 0.901308633685112
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.481069631576538,
            "bert": 0.8886142790317535
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.954854545593261,
            "bert": 0.843456774353981
        },
        "Whole dataset (Slake)": {
            "bart": -3.811783332824707,
            "bert": 0.9917487317323684
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.018516145944595,
            "bert": 0.876511761546135
        },
        "Whole dataset (VCR)": {
            "bart": -3.1205951595306396,
            "bert": 0.9260074204206467
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.629605236649513,
            "bert": 0.9086411213874817
        },
        "Whole dataset (VQA)": {
            "bart": -4.120095363855362,
            "bert": 0.9738056951761246
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.75379318356514,
            "bert": 0.941435513496399
        },
        "Whole dataset (Winoground)": {
            "bart": -4.151528165340424,
            "bert": 0.9978843516111374
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9663865546218487,
            "rec": 0.777027027027027,
            "f1": 0.8614232209737828
        },
        "popular (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.937984496124031,
            "rec": 0.7707006369426752,
            "f1": 0.8461538461538463
        },
        "adversarial (POPE)": {
            "acc": 0.8951048951048951,
            "prec": 0.9903846153846154,
            "rec": 0.7803030303030303,
            "f1": 0.8728813559322034
        }
    },
    "prism-siglip+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7180038718003872
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.7045908183632734
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5872020075282308
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27944111776447106
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.802
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6375153248876175
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.3942857142857143
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6528735632183909
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.35615079365079366
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.43803843605036447
        },
        "Instance Location (SEED_2)": {
            "acc": 0.66359918200409
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.75965801139962
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7340251228836702
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5848303393213573
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5509893455098934
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5306859205776173
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.364613880742913
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.770392749244713
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.39166666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.6226415094339622
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.6848484848484848
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.542713567839196
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.32653061224489793
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.8176470588235294,
            "prec": 0.7596153846153846,
            "rec": 0.9294117647058824,
            "f1": 0.835978835978836
        },
        "posters (MME)": {
            "acc": 0.8707482993197279,
            "prec": 0.8263473053892215,
            "rec": 0.9387755102040817,
            "f1": 0.8789808917197451
        },
        "position (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.6170212765957447,
            "rec": 0.9666666666666667,
            "f1": 0.7532467532467533
        },
        "scene (MME)": {
            "acc": 0.875,
            "prec": 0.8537735849056604,
            "rec": 0.905,
            "f1": 0.8786407766990291
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6428571428571429,
            "prec": 0.6190476190476191,
            "rec": 0.7428571428571429,
            "f1": 0.6753246753246753
        },
        "artwork (MME)": {
            "acc": 0.6925,
            "prec": 0.6287625418060201,
            "rec": 0.94,
            "f1": 0.7535070140280562
        },
        "landmark (MME)": {
            "acc": 0.8025,
            "prec": 0.7574468085106383,
            "rec": 0.89,
            "f1": 0.8183908045977011
        },
        "text_translation (MME)": {
            "acc": 0.625,
            "prec": 0.5757575757575758,
            "rec": 0.95,
            "f1": 0.7169811320754716
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.45714285714285713,
            "rec": 0.8,
            "f1": 0.5818181818181818
        },
        "count (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.5454545454545454,
            "rec": 1.0,
            "f1": 0.7058823529411764
        },
        "color (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6060606060606061,
            "rec": 1.0,
            "f1": 0.7547169811320755
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.872093023255814
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.526984126984127
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.4076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5251141552511416
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5815602836879432
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.441340782122905
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9302325581395349
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9533169533169533
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6212121212121212
        },
        "ocr (MMBench_CN)": {
            "acc": 0.5897435897435898
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.43617021276595747
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.825
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8388157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7386363636363636
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6158730158730159
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5799086757990868
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4748603351955307
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9441860465116279
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.803030303030303
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.46808510638297873
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.865
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8289473684210527
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6808510638297872
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.54
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.7
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.5
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.32679738562091504
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.27058823529411763
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20588235294117646
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.42045454545454547
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7808219178082192
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8741258741258742
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8979591836734694
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.24390243902439024
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.43636363636363634
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5555555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6379310344827587
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.9347826086956522
        },
        "Engineering practices (ScienceQA)": {
            "acc": 1.0
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7948717948717948
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Maps (ScienceQA)": {
            "acc": 0.5869565217391305
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.13157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6274509803921569
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5070422535211268
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.5812182741116751
        },
        "3D Distance (CVBench)": {
            "acc": 0.5283333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6738461538461539
        },
        "3D Depth (CVBench)": {
            "acc": 0.67
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.713369417190552,
            "bert": 0.821834911108017
        },
        "Whole dataset (Enrico)": {
            "bart": -6.663526247739792,
            "bert": 0.9916419982910156
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.363432598114014,
            "bert": 0.9990896421670914
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.305881597995758,
            "bert": 0.8998832100629807
        },
        "Whole dataset (GQA)": {
            "bart": -3.5928345453739166,
            "bert": 0.9936612671613694
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.6874192011356355,
            "bert": 0.9556061124801636
        },
        "Whole dataset (INAT)": {
            "bart": -6.159677805900574,
            "bert": 0.7982641422748565
        },
        "Whole dataset (IRFL)": {
            "bart": -4.349439235925675,
            "bert": 0.998640866279602
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.8884386622905733,
            "bert": 0.8754404473304749
        },
        "Whole dataset (Memotion)": {
            "bart": -4.5207413697242735,
            "bert": 0.8988426953554154
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.5331814604997636,
            "bert": 0.8838182586431503
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.337301168441773,
            "bert": 0.998387753367424
        },
        "Whole dataset (NLVR)": {
            "bart": -3.519517297744751,
            "bert": 0.9992457509040833
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.202894787788391,
            "bert": 0.9992999994754791
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.343459075689316,
            "bert": 0.9270512145757676
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1174934875965117,
            "bert": 0.9385408020019531
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.188034479022026,
            "bert": 0.8223865139484405
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.006581336259842,
            "bert": 0.9060129272937775
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.2195853018760685,
            "bert": 0.8862114316225052
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.985204436779022,
            "bert": 0.8470293802022933
        },
        "Whole dataset (Slake)": {
            "bart": -3.9299199759960173,
            "bert": 0.9929649704694747
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.983082599639893,
            "bert": 0.8859251999855041
        },
        "Whole dataset (VCR)": {
            "bart": -2.8646117612719535,
            "bert": 0.9350053042173385
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.609519720673561,
            "bert": 0.9056476706266403
        },
        "Whole dataset (VQA)": {
            "bart": -3.7573678278923035,
            "bert": 0.9719380271434784
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.4353970229625705,
            "bert": 0.9334291106462479
        },
        "Whole dataset (Winoground)": {
            "bart": -4.06195651769638,
            "bert": 0.9980240488052368
        },
        "random (POPE)": {
            "acc": 0.8827361563517915,
            "prec": 0.9745762711864406,
            "rec": 0.777027027027027,
            "f1": 0.8646616541353384
        },
        "popular (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9333333333333333,
            "rec": 0.802547770700637,
            "f1": 0.863013698630137
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.972972972972973,
            "rec": 0.8181818181818182,
            "f1": 0.8888888888888891
        }
    },
    "prism-dinosiglip-controlled+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6551946655194666
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.21357285429141717
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2490589711417817
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.258
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5643645279934614
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23183673469387756
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2850574712643678
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2787698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25115970841616964
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5695296523517382
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6779607346421785
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.667394866193337
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.20253164556962025
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25149700598802394
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4855403348554033
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22382671480144403
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24731182795698925
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7099697885196374
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2641509433962264
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2515151515151515
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24623115577889448
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "celebrity (MME)": {
            "acc": 0.5676470588235294,
            "prec": 1.0,
            "rec": 0.13529411764705881,
            "f1": 0.23834196891191708
        },
        "posters (MME)": {
            "acc": 0.8061224489795918,
            "prec": 1.0,
            "rec": 0.6122448979591837,
            "f1": 0.759493670886076
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7222222222222222,
            "rec": 0.8666666666666667,
            "f1": 0.7878787878787877
        },
        "scene (MME)": {
            "acc": 0.85,
            "prec": 0.9487179487179487,
            "rec": 0.74,
            "f1": 0.8314606741573033
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7214285714285714,
            "prec": 0.7183098591549296,
            "rec": 0.7285714285714285,
            "f1": 0.7234042553191489
        },
        "artwork (MME)": {
            "acc": 0.705,
            "prec": 0.7662337662337663,
            "rec": 0.59,
            "f1": 0.6666666666666666
        },
        "landmark (MME)": {
            "acc": 0.69,
            "prec": 0.9418604651162791,
            "rec": 0.405,
            "f1": 0.5664335664335663
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2,
            "rec": 0.05,
            "f1": 0.08000000000000002
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.88,
            "rec": 0.7333333333333333,
            "f1": 0.8
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.8235294117647058,
            "rec": 0.9333333333333333,
            "f1": 0.8749999999999999
        },
        "OCR (MME)": {
            "acc": 0.8,
            "prec": 0.8,
            "rec": 0.8,
            "f1": 0.8000000000000002
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.46875,
            "rec": 0.75,
            "f1": 0.5769230769230769
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8313953488372093
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5333333333333333
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5769230769230769
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.410958904109589
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.3016759776536313
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6540404040404041
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7564102564102564
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3617021276595745
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.855
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4787234042553192
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8018867924528302
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7651515151515151
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.653968253968254
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5296803652968036
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.4692737430167598
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9877149877149877
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.75
        },
        "ocr (MMBench_EN)": {
            "acc": 0.75
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3502824858757062
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.86
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8618421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4148936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8490566037735849
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8977272727272727
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.43333333333333335
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.26229508196721313
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.19607843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.375
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7534246575342466
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8181818181818182
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8367346938775511
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.25806451612903225
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4090909090909091
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.603448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8909090909090909
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.6153846153846154
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.78125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3148148148148148
        },
        "Maps (ScienceQA)": {
            "acc": 0.5652173913043478
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7843137254901961
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.2535211267605634
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5789473684210527
        },
        "2D Count (CVBench)": {
            "acc": 0.6015228426395939
        },
        "3D Distance (CVBench)": {
            "acc": 0.48833333333333334
        },
        "2D Relation (CVBench)": {
            "acc": 0.6323076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.7133333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.775535700321197,
            "bert": 0.7853370386362076
        },
        "Whole dataset (Enrico)": {
            "bart": -6.493979141712189,
            "bert": 0.9822670519351959
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.975745913982391,
            "bert": 0.9987073451280594
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.272902715206146,
            "bert": 0.8661687487363815
        },
        "Whole dataset (GQA)": {
            "bart": -3.575064399242401,
            "bert": 0.9922998344898224
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.541443300247193,
            "bert": 0.8164271169900894
        },
        "Whole dataset (INAT)": {
            "bart": -5.948348264694214,
            "bert": 0.7939798069000245
        },
        "Whole dataset (IRFL)": {
            "bart": -4.009534465074539,
            "bert": 0.9987144875526428
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9719156074523925,
            "bert": 0.8510636395215988
        },
        "Whole dataset (Memotion)": {
            "bart": -4.607224488258362,
            "bert": 0.8971173173189163
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8555260145664216,
            "bert": 0.8550369888544083
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.01201033949852,
            "bert": 0.998208179473877
        },
        "Whole dataset (NLVR)": {
            "bart": -3.600691024065018,
            "bert": 0.9992794990539551
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.286268231868744,
            "bert": 0.9992554426193238
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.400160714387894,
            "bert": 0.8819871735572815
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.185430706143379,
            "bert": 0.9400226932764053
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901273334026337,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.458781495094299,
            "bert": 0.9081154805421829
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.346741733551025,
            "bert": 0.9245524340867997
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.061979908943176,
            "bert": 0.8588516271114349
        },
        "Whole dataset (Slake)": {
            "bart": -4.180940809249878,
            "bert": 0.9947160071134568
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.323903399705887,
            "bert": 0.8503265261650086
        },
        "Whole dataset (VCR)": {
            "bart": -3.291474598348141,
            "bert": 0.9237249511480331
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7447643226385114,
            "bert": 0.9047502136230469
        },
        "Whole dataset (VQA)": {
            "bart": -4.182856830358506,
            "bert": 0.9698023653030395
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.757115141153336,
            "bert": 0.9405782634019851
        },
        "Whole dataset (Winoground)": {
            "bart": -4.765938541889191,
            "bert": 0.997837261557579
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.975,
            "rec": 0.7905405405405406,
            "f1": 0.8731343283582089
        },
        "popular (POPE)": {
            "acc": 0.8664495114006515,
            "prec": 0.9264705882352942,
            "rec": 0.802547770700637,
            "f1": 0.8600682593856656
        },
        "adversarial (POPE)": {
            "acc": 0.916083916083916,
            "prec": 0.9736842105263158,
            "rec": 0.8409090909090909,
            "f1": 0.9024390243902439
        }
    },
    "prism-dinosiglip-controlled+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7330608733060874
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6586826347305389
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5514429109159348
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2874251497005988
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.808
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6350633428688189
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.38857142857142857
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5747126436781609
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2926587301587302
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4433399602385686
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6574642126789366
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7010309278350515
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.759341355288157
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.7400327689787002
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.49700598802395207
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.528158295281583
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5776173285198556
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.35581622678396874
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7885196374622356
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.4083333333333333
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5534591194968553
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7696969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.6582914572864321
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.7264705882352941,
            "prec": 0.8532110091743119,
            "rec": 0.5470588235294118,
            "f1": 0.6666666666666667
        },
        "posters (MME)": {
            "acc": 0.8843537414965986,
            "prec": 0.9185185185185185,
            "rec": 0.8435374149659864,
            "f1": 0.8794326241134752
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6744186046511628,
            "rec": 0.9666666666666667,
            "f1": 0.7945205479452055
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9,
            "rec": 0.81,
            "f1": 0.8526315789473685
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6891891891891891,
            "rec": 0.7285714285714285,
            "f1": 0.7083333333333334
        },
        "artwork (MME)": {
            "acc": 0.7075,
            "prec": 0.6707818930041153,
            "rec": 0.815,
            "f1": 0.7358916478555304
        },
        "landmark (MME)": {
            "acc": 0.8225,
            "prec": 0.9056603773584906,
            "rec": 0.72,
            "f1": 0.8022284122562674
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 1.0,
            "prec": 1.0,
            "rec": 1.0,
            "f1": 1.0
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.4375,
            "rec": 0.35,
            "f1": 0.38888888888888884
        },
        "count (MME)": {
            "acc": 0.85,
            "prec": 0.7692307692307693,
            "rec": 1.0,
            "f1": 0.8695652173913044
        },
        "color (MME)": {
            "acc": 0.85,
            "prec": 0.7692307692307693,
            "rec": 1.0,
            "f1": 0.8695652173913044
        },
        "OCR (MME)": {
            "acc": 0.75,
            "prec": 0.6785714285714286,
            "rec": 0.95,
            "f1": 0.7916666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.375,
            "prec": 0.38095238095238093,
            "rec": 0.4,
            "f1": 0.3902439024390244
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8604651162790697
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5841269841269842
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5153846153846153
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.6027397260273972
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.40782122905027934
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9813953488372092
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6515151515151515
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7756410256410257
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.480225988700565
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.450354609929078
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.79
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8453947368421053
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7452830188679245
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.32
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9428571428571428
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6190476190476191
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5923076923076923
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6438356164383562
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6666666666666666
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.553072625698324
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8257575757575758
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7884615384615384
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3898305084745763
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4716312056737589
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8585526315789473
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5319148936170213
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8632075471698113
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9053030303030303
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9285714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5
        },
        "History (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Materials (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2786885245901639
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.33986928104575165
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.4318181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.8461538461538461
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1774193548387097
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.5
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4722222222222222
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6206896551724138
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4666666666666667
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8478260869565217
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.8717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.78125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.6086956521739131
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.10526315789473684
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7058823529411765
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.8620689655172413
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4507042253521127
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5789473684210527
        },
        "2D Count (CVBench)": {
            "acc": 0.5685279187817259
        },
        "3D Distance (CVBench)": {
            "acc": 0.59
        },
        "2D Relation (CVBench)": {
            "acc": 0.7123076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.745
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.701021599769592,
            "bert": 0.804843184351921
        },
        "Whole dataset (Enrico)": {
            "bart": -5.8758935546875,
            "bert": 0.9600458461046218
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.156876187324524,
            "bert": 0.9987750399112701
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.265804671049118,
            "bert": 0.864529864192009
        },
        "Whole dataset (GQA)": {
            "bart": -3.837341831922531,
            "bert": 0.9936580795049668
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.003207607269287,
            "bert": 0.8013929867744446
        },
        "Whole dataset (INAT)": {
            "bart": -6.300882687568665,
            "bert": 0.7950534474849701
        },
        "Whole dataset (IRFL)": {
            "bart": -4.472905673980713,
            "bert": 0.9986035138368606
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.020539711713791,
            "bert": 0.8478774130344391
        },
        "Whole dataset (Memotion)": {
            "bart": -4.052528922557831,
            "bert": 0.9095307278633118
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.2464330583810805,
            "bert": 0.8999530476331711
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.444670273065567,
            "bert": 0.9983642303943634
        },
        "Whole dataset (NLVR)": {
            "bart": -3.1062703037261965,
            "bert": 0.9991056323051453
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.1578769874572754,
            "bert": 0.9992654395103454
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.351674622297287,
            "bert": 0.8818617236614227
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.012186095714569,
            "bert": 0.9411294287443162
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.299352032542228,
            "bert": 0.8262041866779327
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.173505357503891,
            "bert": 0.9069319170713425
        },
        "Whole dataset (Resisc45)": {
            "bart": -3.89923898935318,
            "bert": 0.9314260971546173
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.0810041737556455,
            "bert": 0.8600599712133408
        },
        "Whole dataset (Slake)": {
            "bart": -3.7408637082576752,
            "bert": 0.9961663365364075
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.517214823365212,
            "bert": 0.9206828862428665
        },
        "Whole dataset (VCR)": {
            "bart": -3.153262464404106,
            "bert": 0.9297837543487549
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.642492840886116,
            "bert": 0.9048012560606002
        },
        "Whole dataset (VQA)": {
            "bart": -4.026018344163894,
            "bert": 0.9760560530424118
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.783740390539169,
            "bert": 0.907768040895462
        },
        "Whole dataset (Winoground)": {
            "bart": -4.834843883514404,
            "bert": 0.9978194838762283
        },
        "random (POPE)": {
            "acc": 0.8827361563517915,
            "prec": 0.9745762711864406,
            "rec": 0.777027027027027,
            "f1": 0.8646616541353384
        },
        "popular (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9615384615384616,
            "rec": 0.7961783439490446,
            "f1": 0.8710801393728222
        },
        "adversarial (POPE)": {
            "acc": 0.9125874125874126,
            "prec": 0.9734513274336283,
            "rec": 0.8333333333333334,
            "f1": 0.8979591836734695
        }
    },
    "prism-dinosiglip+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6700365670036567
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2215568862275449
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25094102885821834
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.25
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5888843481814466
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24244897959183673
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2620689655172414
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25496031746031744
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24188204108681247
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6186094069529653
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6701030927835051
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6963267891070297
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6805024576734026
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.22784810126582278
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.23484848484848486
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2654690618762475
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5312024353120244
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20577617328519857
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.23069403714565004
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6918429003021148
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.19166666666666668
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.20603015075376885
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6735294117647059,
            "prec": 0.625531914893617,
            "rec": 0.8647058823529412,
            "f1": 0.725925925925926
        },
        "posters (MME)": {
            "acc": 0.8775510204081632,
            "prec": 0.9302325581395349,
            "rec": 0.8163265306122449,
            "f1": 0.8695652173913043
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "scene (MME)": {
            "acc": 0.87,
            "prec": 0.8663366336633663,
            "rec": 0.875,
            "f1": 0.8706467661691543
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6642857142857143,
            "prec": 0.6236559139784946,
            "rec": 0.8285714285714286,
            "f1": 0.7116564417177914
        },
        "artwork (MME)": {
            "acc": 0.72,
            "prec": 0.6527777777777778,
            "rec": 0.94,
            "f1": 0.7704918032786886
        },
        "landmark (MME)": {
            "acc": 0.755,
            "prec": 0.6758620689655173,
            "rec": 0.98,
            "f1": 0.8
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6666666666666666,
            "rec": 0.1,
            "f1": 0.1739130434782609
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 0.967741935483871,
            "rec": 1.0,
            "f1": 0.9836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.85,
            "f1": 0.6296296296296295
        },
        "count (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.6818181818181818,
            "rec": 1.0,
            "f1": 0.8108108108108109
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.6896551724137931,
            "rec": 1.0,
            "f1": 0.8163265306122449
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7441860465116279
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5365079365079365
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.47692307692307695
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4748858447488584
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9209302325581395
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7297979797979798
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6410256410256411
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3107344632768362
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4219858156028369
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7861842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9375
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7405660377358491
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.75
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.7015873015873015
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4846153846153846
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.547945205479452
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6927374301675978
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9441860465116279
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9754299754299754
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7878787878787878
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7115384615384616
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4463276836158192
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.85
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8651315789473685
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5531914893617021
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8349056603773585
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8977272727272727
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.4
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4666666666666667
        },
        "History (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.2459016393442623
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.18253968253968253
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3202614379084967
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.375
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.684931506849315
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.24193548387096775
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.7380952380952381
        },
        "Magnets (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9454545454545454
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2962962962962963
        },
        "Maps (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6470588235294118
        },
        "Classification (ScienceQA)": {
            "acc": 0.8354430379746836
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5526315789473685
        },
        "2D Count (CVBench)": {
            "acc": 0.5939086294416244
        },
        "3D Distance (CVBench)": {
            "acc": 0.5483333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.5984615384615385
        },
        "3D Depth (CVBench)": {
            "acc": 0.7566666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.777624447345733,
            "bert": 0.8294626981019974
        },
        "Whole dataset (Enrico)": {
            "bart": -6.557701984643936,
            "bert": 0.9811157745122909
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.822185678482056,
            "bert": 0.9990339094400406
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3933953523635862,
            "bert": 0.8984586244821549
        },
        "Whole dataset (GQA)": {
            "bart": -3.4695977210998534,
            "bert": 0.9937046778202057
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.048539053201676,
            "bert": 0.9974210047721863
        },
        "Whole dataset (INAT)": {
            "bart": -6.080071940422058,
            "bert": 0.7835792863368988
        },
        "Whole dataset (IRFL)": {
            "bart": -4.546379988193512,
            "bert": 0.9985968726873398
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9155831122398377,
            "bert": 0.8752518260478973
        },
        "Whole dataset (Memotion)": {
            "bart": -4.577948734760285,
            "bert": 0.8905272668600083
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.958517138361931,
            "bert": 0.8434890848398209
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.486587247848511,
            "bert": 0.9983536356687546
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892404556274414,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.2375645446777344,
            "bert": 0.9992351937294006
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.446994438171387,
            "bert": 0.9275172597169876
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.92467444896698,
            "bert": 0.9366748631000519
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.746044410467148,
            "bert": 0.8479057765007019
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.930260132551194,
            "bert": 0.9035949265956879
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.541126979589462,
            "bert": 0.8640365272760391
        },
        "Whole dataset (Screen2Words)": {
            "bart": -5.987937984466552,
            "bert": 0.8448791867494583
        },
        "Whole dataset (Slake)": {
            "bart": -3.6869675600528717,
            "bert": 0.9917625832557678
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.557673945426941,
            "bert": 0.817036971449852
        },
        "Whole dataset (VCR)": {
            "bart": -3.0228147849440576,
            "bert": 0.930490427017212
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.629257143735886,
            "bert": 0.9080974501371384
        },
        "Whole dataset (VQA)": {
            "bart": -4.261644238233567,
            "bert": 0.9742995053529739
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.3912524580955505,
            "bert": 0.9375240141153336
        },
        "Whole dataset (Winoground)": {
            "bart": -4.160134712457657,
            "bert": 0.9979481887817383
        },
        "random (POPE)": {
            "acc": 0.8729641693811075,
            "prec": 0.9739130434782609,
            "rec": 0.7567567567567568,
            "f1": 0.8517110266159698
        },
        "popular (POPE)": {
            "acc": 0.8631921824104235,
            "prec": 0.9389312977099237,
            "rec": 0.7834394904458599,
            "f1": 0.8541666666666667
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.9906542056074766,
            "rec": 0.803030303030303,
            "f1": 0.8870292887029287
        }
    },
    "prism-dinosiglip+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.7476876747687675
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6746506986027944
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.5671267252195734
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.31137724550898205
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.822
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6546791990192072
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.42448979591836733
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.6735632183908046
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.35714285714285715
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4552683896620278
        },
        "Instance Location (SEED_2)": {
            "acc": 0.689161554192229
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.7731958762886598
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7713742875237493
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.770617149098853
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9367088607594937
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.5808383233532934
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5646879756468798
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5703971119133574
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.3919843597262952
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7885196374622356
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.30833333333333335
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5660377358490566
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7696969696969697
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.628140703517588
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.3877551020408163
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "celebrity (MME)": {
            "acc": 0.7911764705882353,
            "prec": 0.7239819004524887,
            "rec": 0.9411764705882353,
            "f1": 0.8184143222506394
        },
        "posters (MME)": {
            "acc": 0.8707482993197279,
            "prec": 0.8385093167701864,
            "rec": 0.9183673469387755,
            "f1": 0.8766233766233766
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.8272727272727273,
            "rec": 0.91,
            "f1": 0.8666666666666667
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.6304347826086957,
            "rec": 0.8285714285714286,
            "f1": 0.7160493827160495
        },
        "artwork (MME)": {
            "acc": 0.595,
            "prec": 0.5533707865168539,
            "rec": 0.985,
            "f1": 0.7086330935251798
        },
        "landmark (MME)": {
            "acc": 0.7725,
            "prec": 0.6967509025270758,
            "rec": 0.965,
            "f1": 0.8092243186582809
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.5555555555555556,
            "rec": 1.0,
            "f1": 0.7142857142857143
        },
        "existence (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.8823529411764706,
            "rec": 1.0,
            "f1": 0.9375
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.4722222222222222,
            "rec": 0.85,
            "f1": 0.6071428571428571
        },
        "count (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5084745762711864,
            "rec": 1.0,
            "f1": 0.6741573033707865
        },
        "color (MME)": {
            "acc": 0.6833333333333333,
            "prec": 0.6122448979591837,
            "rec": 1.0,
            "f1": 0.759493670886076
        },
        "OCR (MME)": {
            "acc": 0.625,
            "prec": 0.5714285714285714,
            "rec": 1.0,
            "f1": 0.7272727272727273
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8953488372093024
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5587301587301587
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3923076923076923
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5114155251141552
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5957446808510638
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5307262569832403
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9534883720930233
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9508599508599509
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7045454545454546
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6217948717948718
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3785310734463277
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5212765957446809
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.875
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8486842105263158
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9545454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7216981132075472
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.8181818181818182
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9285714285714286
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9127906976744186
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6158730158730159
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5662100456621004
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6950354609929078
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5027932960893855
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9488372093023256
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.972972972972973
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8585858585858586
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8012820512820513
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3446327683615819
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5354609929078015
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.86
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.868421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8207547169811321
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9318181818181818
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4866666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9428571428571428
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.3
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6333333333333333
        },
        "History (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.4
        },
        "Manage (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.6666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29098360655737704
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.11904761904761904
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3660130718954248
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.19607843137254902
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.38636363636363635
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9903846153846154
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8741258741258742
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8775510204081632
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1774193548387097
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.17073170731707318
        },
        "Geography (ScienceQA)": {
            "acc": 0.7142857142857143
        },
        "Magnets (ScienceQA)": {
            "acc": 0.41818181818181815
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.21568627450980393
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.6724137931034483
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8260869565217391
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8909090909090909
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7692307692307693
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4074074074074074
        },
        "Maps (ScienceQA)": {
            "acc": 0.5869565217391305
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.13157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.7450980392156863
        },
        "Classification (ScienceQA)": {
            "acc": 0.810126582278481
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4647887323943662
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.6218274111675127
        },
        "3D Distance (CVBench)": {
            "acc": 0.5366666666666666
        },
        "2D Relation (CVBench)": {
            "acc": 0.72
        },
        "3D Depth (CVBench)": {
            "acc": 0.69
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.057552518844605,
            "bert": 0.7967037469148636
        },
        "Whole dataset (Enrico)": {
            "bart": -6.462761875391006,
            "bert": 0.9834822279214859
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.101870052814483,
            "bert": 0.9974024575948716
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.5491419887542723,
            "bert": 0.8891578900814057
        },
        "Whole dataset (GQA)": {
            "bart": -3.5968191707134247,
            "bert": 0.9936723685264588
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.707337553501129,
            "bert": 0.98089410841465
        },
        "Whole dataset (INAT)": {
            "bart": -6.139813523292542,
            "bert": 0.8006070041656494
        },
        "Whole dataset (IRFL)": {
            "bart": -4.453939491510392,
            "bert": 0.9986169838905334
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9027004873752595,
            "bert": 0.8744397872686386
        },
        "Whole dataset (Memotion)": {
            "bart": -4.534209706783295,
            "bert": 0.8972955858707428
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.355007579624653,
            "bert": 0.8939222538471222
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.459598881006241,
            "bert": 0.9983608186244964
        },
        "Whole dataset (NLVR)": {
            "bart": -3.495908441543579,
            "bert": 0.99926762342453
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.367443923950195,
            "bert": 0.9992891907691955
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.8014257621765135,
            "bert": 0.9115631049871444
        },
        "Whole dataset (OKVQA)": {
            "bart": -2.862458966374397,
            "bert": 0.9428015381097794
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.214697129130363,
            "bert": 0.8122423309087753
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.989703993797303,
            "bert": 0.9106657695770264
        },
        "Whole dataset (Resisc45)": {
            "bart": -5.336598629951477,
            "bert": 0.9111955511569977
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.005063853263855,
            "bert": 0.8492251116037369
        },
        "Whole dataset (Slake)": {
            "bart": -3.819920802116394,
            "bert": 0.9946561229228973
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.494857450723648,
            "bert": 0.8718997263908386
        },
        "Whole dataset (VCR)": {
            "bart": -2.8509448343515396,
            "bert": 0.9358307075500488
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.572567234635353,
            "bert": 0.9094801646471024
        },
        "Whole dataset (VQA)": {
            "bart": -4.080439535379409,
            "bert": 0.9761498886346817
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.703116754293442,
            "bert": 0.9355100500583649
        },
        "Whole dataset (Winoground)": {
            "bart": -4.214890961647034,
            "bert": 0.9963105928897857
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9741379310344828,
            "rec": 0.7635135135135135,
            "f1": 0.8560606060606061
        },
        "popular (POPE)": {
            "acc": 0.8534201954397395,
            "prec": 0.9375,
            "rec": 0.7643312101910829,
            "f1": 0.8421052631578948
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9814814814814815,
            "rec": 0.803030303030303,
            "f1": 0.8833333333333332
        }
    },
    "prism-dinosiglip-224px-controlled+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6293826629382663
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2435129740518962
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2547051442910916
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.256
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5443400081732734
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24326530612244898
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.26436781609195403
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2787698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.23591782637508282
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5603271983640081
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6701030927835051
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6678277390753642
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.638995084653195
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2634730538922156
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5038051750380518
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.19855595667870035
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2404692082111437
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6888217522658611
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2641509433962264
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21608040201005024
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.6294117647058823,
            "prec": 0.9230769230769231,
            "rec": 0.2823529411764706,
            "f1": 0.4324324324324324
        },
        "posters (MME)": {
            "acc": 0.8027210884353742,
            "prec": 0.9405940594059405,
            "rec": 0.6462585034013606,
            "f1": 0.7661290322580645
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6923076923076923,
            "rec": 0.9,
            "f1": 0.7826086956521738
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9235294117647059,
            "rec": 0.785,
            "f1": 0.8486486486486486
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6642857142857143,
            "prec": 0.6716417910447762,
            "rec": 0.6428571428571429,
            "f1": 0.6569343065693432
        },
        "artwork (MME)": {
            "acc": 0.725,
            "prec": 0.7122641509433962,
            "rec": 0.755,
            "f1": 0.7330097087378641
        },
        "landmark (MME)": {
            "acc": 0.7975,
            "prec": 0.8888888888888888,
            "rec": 0.68,
            "f1": 0.7705382436260623
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.8333333333333334,
            "rec": 0.8333333333333334,
            "f1": 0.8333333333333334
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.8333333333333334,
            "rec": 1.0,
            "f1": 0.9090909090909091
        },
        "OCR (MME)": {
            "acc": 0.675,
            "prec": 0.6666666666666666,
            "rec": 0.7,
            "f1": 0.6829268292682926
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.85,
            "f1": 0.6296296296296295
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8488372093023255
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5650793650793651
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5461538461538461
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3744292237442922
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.524822695035461
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.3407821229050279
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7095959595959596
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7115384615384616
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.36879432624113473
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.86
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7993421052631579
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9261363636363636
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.4148936170212766
        },
        "image_style (MMBench_CN)": {
            "acc": 0.8301886792452831
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7613636363636364
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8714285714285714
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9186046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6634920634920635
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.547945205479452
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6099290780141844
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.48044692737430167
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7702020202020202
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7051282051282052
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3559322033898305
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4326241134751773
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.9013157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.4787234042553192
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8443396226415094
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.42
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Pharmacy (MMMU)": {
            "acc": 0.4
        },
        "Public_Health (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Art_Theory (MMMU)": {
            "acc": 0.4666666666666667
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.6
        },
        "Literature (MMMU)": {
            "acc": 0.7333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.5
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.27459016393442626
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.10317460317460317
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15196078431372548
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3522727272727273
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7123287671232876
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.2564102564102564
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8775510204081632
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.36585365853658536
        },
        "Geography (ScienceQA)": {
            "acc": 0.6190476190476191
        },
        "Magnets (ScienceQA)": {
            "acc": 0.44545454545454544
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3055555555555556
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5862068965517241
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.6956521739130435
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9272727272727272
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.84375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.43478260869565216
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.21052631578947367
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5294117647058824
        },
        "Classification (ScienceQA)": {
            "acc": 0.8734177215189873
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7931034482758621
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5406091370558376
        },
        "3D Distance (CVBench)": {
            "acc": 0.515
        },
        "2D Relation (CVBench)": {
            "acc": 0.6415384615384615
        },
        "3D Depth (CVBench)": {
            "acc": 0.6816666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.61261257648468,
            "bert": 0.8136343419551849
        },
        "Whole dataset (Enrico)": {
            "bart": -6.557054700851441,
            "bert": 0.9886674761772156
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.265361788272858,
            "bert": 0.8352409416437149
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.348650811910629,
            "bert": 0.8653643167018891
        },
        "Whole dataset (GQA)": {
            "bart": -3.735335681438446,
            "bert": 0.9937016850709915
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.093935403823853,
            "bert": 0.8011117118597031
        },
        "Whole dataset (INAT)": {
            "bart": -5.923327269554139,
            "bert": 0.7937296384572983
        },
        "Whole dataset (IRFL)": {
            "bart": -4.218534976243973,
            "bert": 0.9986667227745056
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.923122982978821,
            "bert": 0.852292954325676
        },
        "Whole dataset (Memotion)": {
            "bart": -4.528283095359802,
            "bert": 0.8987309384346008
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.8781544119119644,
            "bert": 0.8541620486974716
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.44812348484993,
            "bert": 0.9983593809604645
        },
        "Whole dataset (NLVR)": {
            "bart": -3.616925961971283,
            "bert": 0.9992862486839295
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.132779883146286,
            "bert": 0.9992233180999756
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.442122392654419,
            "bert": 0.8820065194368363
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.2943475639820097,
            "bert": 0.9398072057962418
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.901273334026337,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.179159183502197,
            "bert": 0.9148843425512314
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.556547465324402,
            "bert": 0.8906545394659042
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.152257437705994,
            "bert": 0.8568437558412552
        },
        "Whole dataset (Slake)": {
            "bart": -4.170421924591064,
            "bert": 0.9947013872861862
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.678698773384094,
            "bert": 0.820182334780693
        },
        "Whole dataset (VCR)": {
            "bart": -3.2302043703198433,
            "bert": 0.9240897780656815
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.7327346283197405,
            "bert": 0.9042553567886352
        },
        "Whole dataset (VQA)": {
            "bart": -4.2393193173408505,
            "bert": 0.9711074024438858
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.688544710874558,
            "bert": 0.9415788114070892
        },
        "Whole dataset (Winoground)": {
            "bart": -4.358244190216064,
            "bert": 0.9979446464776993
        },
        "random (POPE)": {
            "acc": 0.8697068403908795,
            "prec": 0.9576271186440678,
            "rec": 0.7635135135135135,
            "f1": 0.849624060150376
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.9097744360902256,
            "rec": 0.7707006369426752,
            "f1": 0.8344827586206898
        },
        "adversarial (POPE)": {
            "acc": 0.8881118881118881,
            "prec": 0.9464285714285714,
            "rec": 0.803030303030303,
            "f1": 0.8688524590163934
        }
    },
    "prism-dinosiglip-224px+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6582060658206066
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.26223337515683814
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.22554890219560877
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.238
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.578259092766653
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3057471264367816
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26587301587301587
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25115970841616964
        },
        "Instance Location (SEED_2)": {
            "acc": 0.598159509202454
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6494845360824743
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6925269157694743
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6553795740032768
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5220700152207002
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20216606498194944
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.23362658846529813
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6827794561933535
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2389937106918239
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21105527638190955
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.538235294117647,
            "prec": 0.52,
            "rec": 0.9941176470588236,
            "f1": 0.6828282828282828
        },
        "posters (MME)": {
            "acc": 0.8843537414965986,
            "prec": 0.9185185185185185,
            "rec": 0.8435374149659864,
            "f1": 0.8794326241134752
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "scene (MME)": {
            "acc": 0.8525,
            "prec": 0.821917808219178,
            "rec": 0.9,
            "f1": 0.8591885441527446
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.6428571428571429,
            "rec": 0.7714285714285715,
            "f1": 0.7012987012987013
        },
        "artwork (MME)": {
            "acc": 0.635,
            "prec": 0.5833333333333334,
            "rec": 0.945,
            "f1": 0.7213740458015268
        },
        "landmark (MME)": {
            "acc": 0.6775,
            "prec": 0.6085626911314985,
            "rec": 0.995,
            "f1": 0.7552182163187857
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.5263157894736842,
            "rec": 0.5,
            "f1": 0.5128205128205129
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.4838709677419355,
            "rec": 0.75,
            "f1": 0.5882352941176471
        },
        "count (MME)": {
            "acc": 0.7,
            "prec": 0.625,
            "rec": 1.0,
            "f1": 0.7692307692307693
        },
        "color (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7317073170731707,
            "rec": 1.0,
            "f1": 0.8450704225352113
        },
        "OCR (MME)": {
            "acc": 0.65,
            "prec": 0.5882352941176471,
            "rec": 1.0,
            "f1": 0.7407407407407407
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5619047619047619
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.45384615384615384
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4657534246575342
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5083798882681564
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9348837209302325
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.7196969696969697
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6346153846153846
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3723404255319149
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.885
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7927631578947368
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9318181818181818
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.425531914893617
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7452830188679245
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7651515151515151
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.4533333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.7142857142857143
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5076923076923077
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5388127853881278
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6170212765957447
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.6759776536312849
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.958139534883721
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7651515151515151
        },
        "ocr (MMBench_EN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4689265536723164
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.42907801418439717
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.88
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.7993421052631579
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9772727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5212765957446809
        },
        "image_style (MMBench_EN)": {
            "acc": 0.8254716981132075
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8674242424242424
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.5066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Math (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4
        },
        "Sociology (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.7333333333333333
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.6333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.5333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.30327868852459017
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.22058823529411764
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6712328767123288
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5384615384615384
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9726027397260274
        },
        "States of matter (ScienceQA)": {
            "acc": 0.6785714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8251748251748252
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8571428571428571
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1774193548387097
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.14634146341463414
        },
        "Geography (ScienceQA)": {
            "acc": 0.6428571428571429
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4727272727272727
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8181818181818182
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.2777777777777778
        },
        "Maps (ScienceQA)": {
            "acc": 0.6521739130434783
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.8481012658227848
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.7586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5263157894736842
        },
        "2D Count (CVBench)": {
            "acc": 0.5761421319796954
        },
        "3D Distance (CVBench)": {
            "acc": 0.55
        },
        "2D Relation (CVBench)": {
            "acc": 0.6261538461538462
        },
        "3D Depth (CVBench)": {
            "acc": 0.7033333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7539138102531435,
            "bert": 0.8281898695230484
        },
        "Whole dataset (Enrico)": {
            "bart": -6.756681222915649,
            "bert": 0.9622122114896774
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.265878705978394,
            "bert": 0.9991715943813324
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.3365939557552338,
            "bert": 0.9003982073068619
        },
        "Whole dataset (GQA)": {
            "bart": -3.545997442007065,
            "bert": 0.9937297767400741
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.988774013519287,
            "bert": 0.9909733194112778
        },
        "Whole dataset (INAT)": {
            "bart": -6.218036756515503,
            "bert": 0.7842279016971588
        },
        "Whole dataset (IRFL)": {
            "bart": -4.167427091598511,
            "bert": 0.9986814481019973
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9092077159881593,
            "bert": 0.8747101372480393
        },
        "Whole dataset (Memotion)": {
            "bart": -4.5768087553977965,
            "bert": 0.9056267601251602
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.006279155015945,
            "bert": 0.8429103231430054
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.51357561469078,
            "bert": 0.9983464527130127
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.3460339069366456,
            "bert": 0.9992327582836151
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.432181715965271,
            "bert": 0.9267929708957672
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.1228399872779846,
            "bert": 0.9365183907747269
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.242049651145935,
            "bert": 0.862505528330803
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.939060964584351,
            "bert": 0.9064217782020569
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.5665930557250975,
            "bert": 0.9022749149799347
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.127656583786011,
            "bert": 0.8430296623706818
        },
        "Whole dataset (Slake)": {
            "bart": -3.8206481444835663,
            "bert": 0.9919207459688186
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.191627215147019,
            "bert": 0.8741505974531174
        },
        "Whole dataset (VCR)": {
            "bart": -3.29929574072361,
            "bert": 0.9221243131160736
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6264078933000565,
            "bert": 0.9061404657363892
        },
        "Whole dataset (VQA)": {
            "bart": -4.140479199886322,
            "bert": 0.9732871115207672
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.490853217840194,
            "bert": 0.93827891767025
        },
        "Whole dataset (Winoground)": {
            "bart": -4.100420280694961,
            "bert": 0.998113961815834
        },
        "random (POPE)": {
            "acc": 0.8631921824104235,
            "prec": 0.9568965517241379,
            "rec": 0.75,
            "f1": 0.8409090909090909
        },
        "popular (POPE)": {
            "acc": 0.8631921824104235,
            "prec": 0.9197080291970803,
            "rec": 0.802547770700637,
            "f1": 0.8571428571428571
        },
        "adversarial (POPE)": {
            "acc": 0.9055944055944056,
            "prec": 0.972972972972973,
            "rec": 0.8181818181818182,
            "f1": 0.8888888888888891
        }
    },
    "llama2-chat+13b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.703377070337707
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.6906187624750499
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.548306148055207
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.812
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.6076828769922354
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.36979591836734693
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.5724137931034483
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.39880952380952384
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.4347249834327369
        },
        "Instance Location (SEED_2)": {
            "acc": 0.6124744376278118
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.7517416086130463
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.699617695248498
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.9113924050632911
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.30303030303030304
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.49700598802395207
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5372907153729072
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.5126353790613718
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.31378299120234604
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.8066465256797583
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.375
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.5723270440251572
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.7848484848484848
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.628140703517588
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.32653061224489793
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.6794117647058824,
            "prec": 1.0,
            "rec": 0.3588235294117647,
            "f1": 0.5281385281385281
        },
        "posters (MME)": {
            "acc": 0.8401360544217688,
            "prec": 0.923728813559322,
            "rec": 0.7414965986394558,
            "f1": 0.8226415094339624
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "scene (MME)": {
            "acc": 0.86,
            "prec": 0.9337349397590361,
            "rec": 0.775,
            "f1": 0.8469945355191257
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.75,
            "prec": 0.8070175438596491,
            "rec": 0.6571428571428571,
            "f1": 0.7244094488188977
        },
        "artwork (MME)": {
            "acc": 0.7375,
            "prec": 0.7486910994764397,
            "rec": 0.715,
            "f1": 0.7314578005115088
        },
        "landmark (MME)": {
            "acc": 0.6625,
            "prec": 0.9850746268656716,
            "rec": 0.33,
            "f1": 0.4943820224719101
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 1.0,
            "rec": 0.05,
            "f1": 0.09523809523809523
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.35,
            "prec": 0.3125,
            "rec": 0.25,
            "f1": 0.2777777777777778
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8484848484848485,
            "rec": 0.9333333333333333,
            "f1": 0.888888888888889
        },
        "color (MME)": {
            "acc": 0.9333333333333333,
            "prec": 0.8823529411764706,
            "rec": 1.0,
            "f1": 0.9375
        },
        "OCR (MME)": {
            "acc": 0.775,
            "prec": 0.7391304347826086,
            "rec": 0.85,
            "f1": 0.7906976744186046
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.4,
            "f1": 0.4444444444444445
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.8546511627906976
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.6253968253968254
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.5753424657534246
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6382978723404256
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.49162011173184356
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.9069767441860465
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9680589680589681
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6919191919191919
        },
        "ocr (MMBench_CN)": {
            "acc": 0.7371794871794872
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.4180790960451977
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.5106382978723404
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.8223684210526315
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9488636363636364
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.6650943396226415
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7651515151515151
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.31333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.9071428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9302325581395349
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5538461538461539
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.6438356164383562
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6595744680851063
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.547486033519553
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9778869778869779
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.8434343434343434
        },
        "ocr (MMBench_EN)": {
            "acc": 0.8141025641025641
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4519774011299435
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.5212765957446809
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.89
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8388157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9715909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.5957446808510638
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7783018867924528
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.9242424242424242
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.47333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9142857142857143
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.4
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.6
        },
        "Accounting (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.2
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.5666666666666667
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29918032786885246
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3137254901960784
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23529411764705882
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.4318181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7397260273972602
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9935897435897436
        },
        "Designing experiments (ScienceQA)": {
            "acc": 1.0
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8111888111888111
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8469387755102041
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.20967741935483872
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.12195121951219512
        },
        "Geography (ScienceQA)": {
            "acc": 0.6904761904761905
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5277777777777778
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5344827586206896
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.782608695652174
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9636363636363636
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Maps (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.07894736842105263
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.5686274509803921
        },
        "Classification (ScienceQA)": {
            "acc": 0.7974683544303798
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.896551724137931
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5633802816901409
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.4473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.5786802030456852
        },
        "3D Distance (CVBench)": {
            "acc": 0.64
        },
        "2D Relation (CVBench)": {
            "acc": 0.703076923076923
        },
        "3D Depth (CVBench)": {
            "acc": 0.76
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.906414020061493,
            "bert": 0.8252823221683502
        },
        "Whole dataset (Enrico)": {
            "bart": -5.525970580577851,
            "bert": 0.9861544913053513
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -4.218310737609864,
            "bert": 0.9951759076118469
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.2064549136161804,
            "bert": 0.8738805079460144
        },
        "Whole dataset (GQA)": {
            "bart": -3.7106071972846983,
            "bert": 0.9922748565673828
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.707546154260635,
            "bert": 0.8189721971750259
        },
        "Whole dataset (INAT)": {
            "bart": -6.289048447608947,
            "bert": 0.7967571967840195
        },
        "Whole dataset (IRFL)": {
            "bart": -4.577990292310715,
            "bert": 0.9985744225978851
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9309868311882017,
            "bert": 0.8533126121759415
        },
        "Whole dataset (Memotion)": {
            "bart": -3.798046560287476,
            "bert": 0.9061203354597092
        },
        "Whole dataset (MMIMDB)": {
            "bart": -3.5242156982421875,
            "bert": 0.8832122886180878
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.459598881006241,
            "bert": 0.9983608186244964
        },
        "Whole dataset (NLVR)": {
            "bart": -3.469325475692749,
            "bert": 0.9991615080833435
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.2102687215805052,
            "bert": 0.999271377325058
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.2617146360874174,
            "bert": 0.8950580137968064
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.153268313407898,
            "bert": 0.9413365709781647
        },
        "Whole dataset (OpenPath)": {
            "bart": -4.805280660390854,
            "bert": 0.846593776345253
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.352979725599289,
            "bert": 0.9124990147352219
        },
        "Whole dataset (Resisc45)": {
            "bart": -4.120614074468612,
            "bert": 0.946690993309021
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.101057362556458,
            "bert": 0.8585738968849183
        },
        "Whole dataset (Slake)": {
            "bart": -3.9476285469532013,
            "bert": 0.9961809647083283
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.401103601455689,
            "bert": 0.9485621005296707
        },
        "Whole dataset (VCR)": {
            "bart": -3.261028724014759,
            "bert": 0.9212698435783386
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.490257633328438,
            "bert": 0.9068746149539948
        },
        "Whole dataset (VQA)": {
            "bart": -4.323632148504257,
            "bert": 0.9701798313856125
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.909621266126632,
            "bert": 0.9425585025548935
        },
        "Whole dataset (Winoground)": {
            "bart": -4.448400200605392,
            "bert": 0.9979189670085907
        },
        "random (POPE)": {
            "acc": 0.8794788273615635,
            "prec": 0.9826086956521739,
            "rec": 0.7635135135135135,
            "f1": 0.8593155893536122
        },
        "popular (POPE)": {
            "acc": 0.8436482084690554,
            "prec": 0.9097744360902256,
            "rec": 0.7707006369426752,
            "f1": 0.8344827586206898
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.9727272727272728,
            "rec": 0.8106060606060606,
            "f1": 0.8842975206611571
        }
    },
    "mistral-v0.1+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6575607657560766
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23952095808383234
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2659974905897114
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.248
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5586432366162648
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24979591836734694
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3471264367816092
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24503968253968253
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25579854208084823
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5552147239263804
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6703609879670678
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6176952484980884
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.1893939393939394
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2714570858283433
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4824961948249619
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23104693140794225
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.23655913978494625
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6737160120845922
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.19166666666666668
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23115577889447236
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.24691358024691357
        },
        "celebrity (MME)": {
            "acc": 0.6588235294117647,
            "prec": 0.6038461538461538,
            "rec": 0.9235294117647059,
            "f1": 0.7302325581395349
        },
        "posters (MME)": {
            "acc": 0.8231292517006803,
            "prec": 0.8740157480314961,
            "rec": 0.7551020408163265,
            "f1": 0.8102189781021897
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6444444444444445,
            "rec": 0.9666666666666667,
            "f1": 0.7733333333333334
        },
        "scene (MME)": {
            "acc": 0.845,
            "prec": 0.8285714285714286,
            "rec": 0.87,
            "f1": 0.848780487804878
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6642857142857143,
            "prec": 0.6292134831460674,
            "rec": 0.8,
            "f1": 0.7044025157232705
        },
        "artwork (MME)": {
            "acc": 0.615,
            "prec": 0.5741935483870968,
            "rec": 0.89,
            "f1": 0.6980392156862745
        },
        "landmark (MME)": {
            "acc": 0.83,
            "prec": 0.782051282051282,
            "rec": 0.915,
            "f1": 0.8433179723502305
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9833333333333333,
            "prec": 1.0,
            "rec": 0.9666666666666667,
            "f1": 0.983050847457627
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.4230769230769231,
            "rec": 0.55,
            "f1": 0.47826086956521735
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7631578947368421,
            "rec": 0.9666666666666667,
            "f1": 0.8529411764705883
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8571428571428571,
            "rec": 1.0,
            "f1": 0.923076923076923
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.7325581395348837
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5904761904761905
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.5230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4794520547945205
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.6028368794326241
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.41899441340782123
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8511627906976744
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9484029484029484
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.5580808080808081
        },
        "ocr (MMBench_CN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.384180790960452
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.4326241134751773
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.845
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7763157894736842
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.8920454545454546
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5851063829787234
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7358490566037735
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7348484848484849
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.32
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8428571428571429
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.9069767441860465
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6095238095238096
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5615384615384615
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.5068493150684932
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.6453900709219859
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.5921787709497207
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8604651162790697
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9557739557739557
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7626262626262627
        },
        "ocr (MMBench_EN)": {
            "acc": 0.7435897435897436
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3389830508474576
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.45390070921985815
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.83
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.8355263157894737
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6382978723404256
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7877358490566038
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8106060606060606
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.44666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9285714285714286
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.5
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Accounting (MMMU)": {
            "acc": 0.2
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.5
        },
        "Literature (MMMU)": {
            "acc": 0.8
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.4666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.25
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14705882352941177
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.2727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.726027397260274
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9198717948717948
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.8493150684931506
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.8391608391608392
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7959183673469388
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2926829268292683
        },
        "Geography (ScienceQA)": {
            "acc": 0.47619047619047616
        },
        "Magnets (ScienceQA)": {
            "acc": 0.45454545454545453
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.35294117647058826
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.24444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.717391304347826
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.8727272727272727
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.5897435897435898
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.90625
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.43478260869565216
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.18421052631578946
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.49019607843137253
        },
        "Classification (ScienceQA)": {
            "acc": 0.7721518987341772
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.6551724137931034
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.4225352112676056
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.5774111675126904
        },
        "3D Distance (CVBench)": {
            "acc": 0.5583333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.6369230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.7033333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.010326607227325,
            "bert": 0.7592189437150956
        },
        "Whole dataset (Enrico)": {
            "bart": -6.762264246940613,
            "bert": 0.8187324917316436
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.445790619850158,
            "bert": 0.8118132615089416
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.331624118089676,
            "bert": 0.8765212899446487
        },
        "Whole dataset (GQA)": {
            "bart": -3.6474301397800444,
            "bert": 0.9937328684329987
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.613762583732605,
            "bert": 0.7985093927383423
        },
        "Whole dataset (INAT)": {
            "bart": -5.980911855697632,
            "bert": 0.7919709515571595
        },
        "Whole dataset (IRFL)": {
            "bart": -4.328772929906845,
            "bert": 0.998643559217453
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.080261433124543,
            "bert": 0.8482839113473892
        },
        "Whole dataset (Memotion)": {
            "bart": -4.590451276302337,
            "bert": 0.8770296013355255
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.79128223657608,
            "bert": 0.7893569457530976
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.634437413215637,
            "bert": 0.8196184474229813
        },
        "Whole dataset (NLVR)": {
            "bart": -3.96760751247406,
            "bert": 0.9806391209363937
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.707864977121353,
            "bert": 0.9943166506290436
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.395124323964119,
            "bert": 0.9084745281934739
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.089918537735939,
            "bert": 0.9379903131723404
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.26074123620987,
            "bert": 0.8083086222410202
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.363347861766815,
            "bert": 0.9106439167261123
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.372421083450317,
            "bert": 0.8519249606132507
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.08743088722229,
            "bert": 0.8566168171167373
        },
        "Whole dataset (Slake)": {
            "bart": -3.887047522068024,
            "bert": 0.994385780096054
        },
        "Whole dataset (UCMerced)": {
            "bart": -4.244175095558166,
            "bert": 0.8967180246114731
        },
        "Whole dataset (VCR)": {
            "bart": -3.9472747194766997,
            "bert": 0.869454243183136
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.6217629039287567,
            "bert": 0.9067788511514664
        },
        "Whole dataset (VQA)": {
            "bart": -4.48926043510437,
            "bert": 0.9740155959129333
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.703870354890824,
            "bert": 0.9382223069667817
        },
        "Whole dataset (Winoground)": {
            "bart": -5.340716429948807,
            "bert": 0.9976927065849304
        },
        "random (POPE)": {
            "acc": 0.8762214983713354,
            "prec": 0.9824561403508771,
            "rec": 0.7567567567567568,
            "f1": 0.8549618320610687
        },
        "popular (POPE)": {
            "acc": 0.8566775244299675,
            "prec": 0.9448818897637795,
            "rec": 0.7643312101910829,
            "f1": 0.8450704225352114
        },
        "adversarial (POPE)": {
            "acc": 0.9020979020979021,
            "prec": 0.956140350877193,
            "rec": 0.8257575757575758,
            "f1": 0.8861788617886179
        }
    },
    "mistral-instruct-v0.1+7b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6528285652828565
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2694610778443114
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2578419071518193
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26147704590818366
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.244
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5627298733142624
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.25387755102040815
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3195402298850575
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2390873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2491716368455931
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5685071574642127
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6738442051931602
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6237028945931186
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4794520547945205
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24187725631768953
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2316715542521994
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6918429003021148
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.175
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22012578616352202
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.25125628140703515
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.061224489795918366
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.5441176470588235,
            "prec": 0.5252525252525253,
            "rec": 0.9176470588235294,
            "f1": 0.6680942184154175
        },
        "posters (MME)": {
            "acc": 0.8299319727891157,
            "prec": 0.8540145985401459,
            "rec": 0.7959183673469388,
            "f1": 0.823943661971831
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6666666666666666,
            "rec": 1.0,
            "f1": 0.8
        },
        "scene (MME)": {
            "acc": 0.8575,
            "prec": 0.8592964824120602,
            "rec": 0.855,
            "f1": 0.8571428571428571
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.6666666666666666,
            "rec": 0.7714285714285715,
            "f1": 0.7152317880794701
        },
        "artwork (MME)": {
            "acc": 0.595,
            "prec": 0.5568862275449101,
            "rec": 0.93,
            "f1": 0.6966292134831461
        },
        "landmark (MME)": {
            "acc": 0.8,
            "prec": 0.7419354838709677,
            "rec": 0.92,
            "f1": 0.8214285714285714
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.9666666666666667,
            "prec": 1.0,
            "rec": 0.9333333333333333,
            "f1": 0.9655172413793104
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.47368421052631576,
            "rec": 0.9,
            "f1": 0.6206896551724138
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.84375,
            "rec": 0.9,
            "f1": 0.870967741935484
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5135135135135135,
            "rec": 0.95,
            "f1": 0.6666666666666667
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.6569767441860465
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.5714285714285714
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.46153846153846156
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4885844748858447
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.5886524822695035
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.5363128491620112
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.8511627906976744
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.9705159705159705
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.6237373737373737
        },
        "ocr (MMBench_CN)": {
            "acc": 0.6923076923076923
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.3728813559322034
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.44680851063829785
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.82
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.7993421052631579
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.9431818181818182
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.5638297872340425
        },
        "image_style (MMBench_CN)": {
            "acc": 0.7547169811320755
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.7727272727272727
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.8857142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.7848837209302325
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.6349206349206349
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.5692307692307692
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.4885844748858447
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.7094972067039106
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.8883720930232558
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.9803439803439803
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.7323232323232324
        },
        "ocr (MMBench_EN)": {
            "acc": 0.717948717948718
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.4293785310734463
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.4858156028368794
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.87
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.819078947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.9602272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.6702127659574468
        },
        "image_style (MMBench_EN)": {
            "acc": 0.7264150943396226
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.8522727272727273
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.4066666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.9071428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.4
        },
        "Math (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Physics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.5
        },
        "Art_Theory (MMMU)": {
            "acc": 0.6
        },
        "History (MMMU)": {
            "acc": 0.4
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Accounting (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Literature (MMMU)": {
            "acc": 0.7666666666666667
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.4
        },
        "Computer_Science (MMMU)": {
            "acc": 0.3
        },
        "Agriculture (MMMU)": {
            "acc": 0.4
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29098360655737704
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.11904761904761904
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.2679738562091503
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.29411764705882354
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.3409090909090909
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.6986301369863014
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.9038461538461539
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.8904109589041096
        },
        "States of matter (ScienceQA)": {
            "acc": 0.75
        },
        "Materials (ScienceQA)": {
            "acc": 0.8321678321678322
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.7959183673469388
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1935483870967742
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.5238095238095238
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4818181818181818
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5833333333333334
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.8913043478260869
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9090909090909091
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.717948717948718
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.96875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.5434782608695652
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.15789473684210525
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.6078431372549019
        },
        "Classification (ScienceQA)": {
            "acc": 0.7721518987341772
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.9310344827586207
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.5915492957746479
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.6053299492385786
        },
        "3D Distance (CVBench)": {
            "acc": 0.5016666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.7015384615384616
        },
        "3D Depth (CVBench)": {
            "acc": 0.7483333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -5.193787891864776,
            "bert": 0.7604712665081024
        },
        "Whole dataset (Enrico)": {
            "bart": -6.669589298963547,
            "bert": 0.8390920895338059
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.32934877872467,
            "bert": 0.817403045296669
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.352442251443863,
            "bert": 0.8815590733289719
        },
        "Whole dataset (GQA)": {
            "bart": -3.970706287622452,
            "bert": 0.9936518847942353
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.120412015914917,
            "bert": 0.804689519405365
        },
        "Whole dataset (INAT)": {
            "bart": -5.975527520179749,
            "bert": 0.7899882131814957
        },
        "Whole dataset (IRFL)": {
            "bart": -4.529751256704331,
            "bert": 0.9985895073413849
        },
        "Whole dataset (MemeCaps)": {
            "bart": -3.9970836901664732,
            "bert": 0.848621831536293
        },
        "Whole dataset (Memotion)": {
            "bart": -5.158390393257141,
            "bert": 0.848419075012207
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.909701244831085,
            "bert": 0.786643733382225
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.383424401283264,
            "bert": 0.8206709653139115
        },
        "Whole dataset (NLVR)": {
            "bart": -3.6892396211624146,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.677365918159485,
            "bert": 0.9977895468473434
        },
        "Whole dataset (NoCaps)": {
            "bart": -2.4150730687379838,
            "bert": 0.9097941082715988
        },
        "Whole dataset (OKVQA)": {
            "bart": -3.195041160583496,
            "bert": 0.9378931802511216
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.303572800159454,
            "bert": 0.8034200936555862
        },
        "Whole dataset (PathVQA)": {
            "bart": -4.9077913546562195,
            "bert": 0.9098646414279937
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.296952729225159,
            "bert": 0.8122262513637543
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.148961338996887,
            "bert": 0.8593924593925476
        },
        "Whole dataset (Slake)": {
            "bart": -4.118187350034714,
            "bert": 0.9941803145408631
        },
        "Whole dataset (UCMerced)": {
            "bart": -5.643750879764557,
            "bert": 0.8149322128295898
        },
        "Whole dataset (VCR)": {
            "bart": -3.9831806874275206,
            "bert": 0.8722725015878677
        },
        "Whole dataset (VisualGenome)": {
            "bart": -3.5432140648365023,
            "bert": 0.9073249965906143
        },
        "Whole dataset (VQA)": {
            "bart": -4.54137002825737,
            "bert": 0.9723191142082215
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.791396251916885,
            "bert": 0.9415029865503312
        },
        "Whole dataset (Winoground)": {
            "bart": -4.4839951145648955,
            "bert": 0.9979128623008728
        },
        "random (POPE)": {
            "acc": 0.8892508143322475,
            "prec": 0.975,
            "rec": 0.7905405405405406,
            "f1": 0.8731343283582089
        },
        "popular (POPE)": {
            "acc": 0.8403908794788274,
            "prec": 0.9029850746268657,
            "rec": 0.7707006369426752,
            "f1": 0.8316151202749141
        },
        "adversarial (POPE)": {
            "acc": 0.8986013986013986,
            "prec": 0.963963963963964,
            "rec": 0.8106060606060606,
            "f1": 0.8806584362139919
        }
    },
    "phi-2+3b": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.25231232523123254
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23952095808383234
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24215809284818068
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2634730538922156
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.28
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.26808336738863914
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.22285714285714286
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2413793103448276
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.16964285714285715
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.23061630218687873
        },
        "Instance Location (SEED_2)": {
            "acc": 0.23210633946830267
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.29896907216494845
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.253324889170361
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.2348443473511742
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.17721518987341772
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.22727272727272727
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.24750499001996007
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.2222222222222222
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20938628158844766
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.22971652003910067
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.18731117824773413
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.275
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.27044025157232704
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.22424242424242424
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.1457286432160804
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.30612244897959184
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.25925925925925924
        },
        "celebrity (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "posters (MME)": {
            "acc": 0.42517006802721086,
            "prec": 0.23809523809523808,
            "rec": 0.06802721088435375,
            "f1": 0.10582010582010583
        },
        "position (MME)": {
            "acc": 0.55,
            "prec": 0.8,
            "rec": 0.13333333333333333,
            "f1": 0.2285714285714286
        },
        "scene (MME)": {
            "acc": 0.52,
            "prec": 0.5232558139534884,
            "rec": 0.45,
            "f1": 0.48387096774193555
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5428571428571428,
            "prec": 0.59375,
            "rec": 0.2714285714285714,
            "f1": 0.37254901960784315
        },
        "artwork (MME)": {
            "acc": 0.4775,
            "prec": 0.45054945054945056,
            "rec": 0.205,
            "f1": 0.281786941580756
        },
        "landmark (MME)": {
            "acc": 0.5125,
            "prec": 0.5163398692810458,
            "rec": 0.395,
            "f1": 0.44759206798866863
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.6166666666666667,
            "prec": 0.6129032258064516,
            "rec": 0.6333333333333333,
            "f1": 0.6229508196721313
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.2857142857142857,
            "rec": 0.1,
            "f1": 0.14814814814814817
        },
        "count (MME)": {
            "acc": 0.5166666666666667,
            "prec": 1.0,
            "rec": 0.03333333333333333,
            "f1": 0.06451612903225806
        },
        "color (MME)": {
            "acc": 0.5833333333333334,
            "prec": 0.7777777777777778,
            "rec": 0.23333333333333334,
            "f1": 0.35897435897435903
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.05,
            "f1": 0.09090909090909091
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.3488372093023256
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.30793650793650795
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.18461538461538463
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.4155251141552511
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.24822695035460993
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.13966480446927373
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.15348837209302327
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.26535626535626533
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_CN)": {
            "acc": 0.30128205128205127
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.29432624113475175
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.305
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.4144736842105263
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2784090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.3617021276595745
        },
        "image_style (MMBench_CN)": {
            "acc": 0.28773584905660377
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.30303030303030304
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.3466666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.3142857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.29069767441860467
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.3492063492063492
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.4
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.502283105022831
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.45390070921985815
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.26256983240223464
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.257985257985258
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2878787878787879
        },
        "ocr (MMBench_EN)": {
            "acc": 0.38461538461538464
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.41843971631205673
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.365
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.5888157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.3181818181818182
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.44680851063829785
        },
        "image_style (MMBench_EN)": {
            "acc": 0.330188679245283
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.3484848484848485
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.36
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.3357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Math (MMMU)": {
            "acc": 0.4
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.5333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3333333333333333
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.2
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.4
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Finance (MMMU)": {
            "acc": 0.3
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.3
        },
        "Literature (MMMU)": {
            "acc": 0.6
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.3333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.29508196721311475
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1349206349206349
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.3006535947712418
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.27058823529411763
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23039215686274508
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.36363636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.7671232876712328
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "State capitals (ScienceQA)": {
            "acc": 0.8237179487179487
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.9863013698630136
        },
        "States of matter (ScienceQA)": {
            "acc": 0.9285714285714286
        },
        "Materials (ScienceQA)": {
            "acc": 0.5244755244755245
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.8775510204081632
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.16129032258064516
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5853658536585366
        },
        "Geography (ScienceQA)": {
            "acc": 0.5
        },
        "Magnets (ScienceQA)": {
            "acc": 0.42727272727272725
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.5
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.1568627450980392
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.5172413793103449
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.7391304347826086
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.9818181818181818
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.7435897435897436
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.46875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.48148148148148145
        },
        "Maps (ScienceQA)": {
            "acc": 0.43478260869565216
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.23684210526315788
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.23529411764705882
        },
        "Classification (ScienceQA)": {
            "acc": 0.8860759493670886
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.9655172413793104
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.43661971830985913
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.5
        },
        "2D Count (CVBench)": {
            "acc": 0.2398477157360406
        },
        "3D Distance (CVBench)": {
            "acc": 0.56
        },
        "2D Relation (CVBench)": {
            "acc": 0.5107692307692308
        },
        "3D Depth (CVBench)": {
            "acc": 0.52
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.655291571617126,
            "bert": 0.8891218793392182
        },
        "Whole dataset (Enrico)": {
            "bart": -7.292895483970642,
            "bert": 0.07996644914150237
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.4335067129135135,
            "bert": 0.7311231487989426
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.081509773731232,
            "bert": 0.8412127321958542
        },
        "Whole dataset (GQA)": {
            "bart": -5.152695897817612,
            "bert": 0.9919118946790695
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.195514988899231,
            "bert": 0.7996758985519409
        },
        "Whole dataset (INAT)": {
            "bart": -7.247145810127258,
            "bert": 0.7309533524513244
        },
        "Whole dataset (IRFL)": {
            "bart": -4.87140911579132,
            "bert": 0.9985002595186233
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.071884958744049,
            "bert": 0.8481695544719696
        },
        "Whole dataset (Memotion)": {
            "bart": -4.704369874000549,
            "bert": 0.9005506199598312
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.031823735237122,
            "bert": 0.8680757975578308
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.474527488946915,
            "bert": 0.9983574068546295
        },
        "Whole dataset (NLVR)": {
            "bart": -3.0845229279994966,
            "bert": 0.9891825777292251
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.27960703253746,
            "bert": 0.999141765832901
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.640860505104065,
            "bert": 0.852006059885025
        },
        "Whole dataset (OKVQA)": {
            "bart": -4.777178502082824,
            "bert": 0.8857181602716446
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.2146962171792985,
            "bert": 0.8142396861314773
        },
        "Whole dataset (PathVQA)": {
            "bart": -5.771110072135925,
            "bert": 0.7781624686717987
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.309398078918457,
            "bert": 0.7362187653779984
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.775306839942932,
            "bert": 0.8413454514741897
        },
        "Whole dataset (Slake)": {
            "bart": -4.135871865749359,
            "bert": 0.9946532720327377
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.964226593971253,
            "bert": 0.9098296374082565
        },
        "Whole dataset (VCR)": {
            "bart": -3.99892398416996,
            "bert": 0.8955324685573578
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.155469873547554,
            "bert": 0.9091322451829911
        },
        "Whole dataset (VQA)": {
            "bart": -5.448735733032226,
            "bert": 0.960288496017456
        },
        "Whole dataset (VQARAD)": {
            "bart": -4.966671794652939,
            "bert": 0.8136343252658844
        },
        "Whole dataset (Winoground)": {
            "bart": -5.382633404731751,
            "bert": 0.9976821118593215
        },
        "random (POPE)": {
            "acc": 0.5179153094462541,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "popular (POPE)": {
            "acc": 0.48859934853420195,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "adversarial (POPE)": {
            "acc": 0.5384615384615384,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        }
    },
    "gemma-instruct+2b+clip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5166702516670252
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2616060225846926
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.276
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4854924397221087
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24571428571428572
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.31724137931034485
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26686507936507936
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.25182239893969516
        },
        "Instance Location (SEED_2)": {
            "acc": 0.4703476482617587
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6082474226804123
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6266624445851805
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5368651010376844
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.1893939393939394
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2554890219560878
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.3607305936073059
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24548736462093862
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.22776148582600195
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.622356495468278
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.175
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2389937106918239
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23618090452261306
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.20408163265306123
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.35802469135802467
        },
        "celebrity (MME)": {
            "acc": 0.5823529411764706,
            "prec": 0.7592592592592593,
            "rec": 0.2411764705882353,
            "f1": 0.36607142857142855
        },
        "posters (MME)": {
            "acc": 0.2687074829931973,
            "prec": 0.3440366972477064,
            "rec": 0.5102040816326531,
            "f1": 0.4109589041095891
        },
        "position (MME)": {
            "acc": 0.65,
            "prec": 0.6,
            "rec": 0.9,
            "f1": 0.7200000000000001
        },
        "scene (MME)": {
            "acc": 0.175,
            "prec": 0.20454545454545456,
            "rec": 0.225,
            "f1": 0.2142857142857143
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5928571428571429,
            "prec": 0.5764705882352941,
            "rec": 0.7,
            "f1": 0.632258064516129
        },
        "artwork (MME)": {
            "acc": 0.675,
            "prec": 0.6923076923076923,
            "rec": 0.63,
            "f1": 0.6596858638743456
        },
        "landmark (MME)": {
            "acc": 0.1875,
            "prec": 0.12574850299401197,
            "rec": 0.105,
            "f1": 0.11444141689373297
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.5384615384615384,
            "rec": 0.35,
            "f1": 0.4242424242424242
        },
        "existence (MME)": {
            "acc": 0.06666666666666667,
            "prec": 0.03571428571428571,
            "rec": 0.03333333333333333,
            "f1": 0.03448275862068965
        },
        "numerical_calculation (MME)": {
            "acc": 0.375,
            "prec": 0.42424242424242425,
            "rec": 0.7,
            "f1": 0.5283018867924527
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.7647058823529411,
            "rec": 0.8666666666666667,
            "f1": 0.8125
        },
        "color (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5714285714285714,
            "rec": 0.2,
            "f1": 0.29629629629629634
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.95,
            "f1": 0.6551724137931034
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.20930232558139536
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2222222222222222
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2557077625570776
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.19553072625698323
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.27525252525252525
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.26595744680851063
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2631578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25471698113207547
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2765151515151515
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.23333333333333334
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2571428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.1744186046511628
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2571428571428571
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2465753424657534
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.20567375886524822
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.24022346368715083
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2837209302325581
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2297979797979798
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2033898305084746
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.24468085106382978
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2631578947368421
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.22641509433962265
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.26515151515151514
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.26
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.22142857142857142
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.1
        },
        "Public_Health (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.3
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.20081967213114754
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.17647058823529413
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12745098039215685
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.19318181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "States of matter (ScienceQA)": {
            "acc": 0.5
        },
        "Materials (ScienceQA)": {
            "acc": 0.35664335664335667
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.336734693877551
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.1774193548387097
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.2619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.29310344827586204
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.4
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.3125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Classification (ScienceQA)": {
            "acc": 0.4050632911392405
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "2D Count (CVBench)": {
            "acc": 0.41624365482233505
        },
        "3D Distance (CVBench)": {
            "acc": 0.375
        },
        "2D Relation (CVBench)": {
            "acc": 0.2876923076923077
        },
        "3D Depth (CVBench)": {
            "acc": 0.285
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.842846074104309,
            "bert": 0.7422327595949173
        },
        "Whole dataset (Enrico)": {
            "bart": -7.144124202728271,
            "bert": 0.8195360332727433
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.502818179130554,
            "bert": 0.8024686443805694
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.9329800462722777,
            "bert": 0.9012306362390519
        },
        "Whole dataset (GQA)": {
            "bart": -5.5277635288238525,
            "bert": 0.993663592338562
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.343161659240723,
            "bert": 0.7938715809583664
        },
        "Whole dataset (INAT)": {
            "bart": -5.998351898193359,
            "bert": 0.7800591504573822
        },
        "Whole dataset (IRFL)": {
            "bart": -4.7282010936737064,
            "bert": 0.9985923826694488
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.39239468574524,
            "bert": 0.8514208370447158
        },
        "Whole dataset (Memotion)": {
            "bart": -4.729321157932281,
            "bert": 0.8930838245153427
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.6507493686676025,
            "bert": 0.7952792263031006
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.057378339767456,
            "bert": 0.8482258284091949
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.508800446987152,
            "bert": 0.9271547186374665
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.504065070152283,
            "bert": 0.9425538313388825
        },
        "Whole dataset (OpenPath)": {
            "bart": -7.342857072353363,
            "bert": 0.824109069108963
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.435267786979676,
            "bert": 0.9122408974170685
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.611848254203796,
            "bert": 0.8223088383674622
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.67193389415741,
            "bert": 0.854354720711708
        },
        "Whole dataset (Slake)": {
            "bart": -5.334671578407288,
            "bert": 0.9954354149103165
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.034532625675201,
            "bert": 0.7642142415046692
        },
        "Whole dataset (VCR)": {
            "bart": -5.6454249334335325,
            "bert": 0.8779624855518341
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.140605140924453,
            "bert": 0.9064147543907165
        },
        "Whole dataset (VQA)": {
            "bart": -7.053021421432495,
            "bert": 0.9735762602090836
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.297602562904358,
            "bert": 0.941164749264717
        },
        "Whole dataset (Winoground)": {
            "bart": -3.6950954723358156,
            "bert": 0.99802365899086
        },
        "random (POPE)": {
            "acc": 0.504885993485342,
            "prec": 0.4861111111111111,
            "rec": 0.47297297297297297,
            "f1": 0.4794520547945206
        },
        "popular (POPE)": {
            "acc": 0.498371335504886,
            "prec": 0.5094339622641509,
            "rec": 0.5159235668789809,
            "f1": 0.5126582278481012
        },
        "adversarial (POPE)": {
            "acc": 0.47202797202797203,
            "prec": 0.43537414965986393,
            "rec": 0.48484848484848486,
            "f1": 0.4587813620071684
        }
    },
    "gemma-instruct+2b+siglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.39621423962142394
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23353293413173654
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2685069008782936
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.274
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.46710257458111976
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24081632653061225
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2735632183908046
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2569444444444444
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.26308813783962887
        },
        "Instance Location (SEED_2)": {
            "acc": 0.4233128834355828
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5567010309278351
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.5772640911969601
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.4620425996723102
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.20253164556962025
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2196969696969697
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25349301397205587
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.365296803652968
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2563176895306859
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.23949169110459434
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.5558912386706949
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.25157232704402516
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24120603015075376
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.5235294117647059,
            "prec": 0.6,
            "rec": 0.1411764705882353,
            "f1": 0.22857142857142856
        },
        "posters (MME)": {
            "acc": 0.37755102040816324,
            "prec": 0.42436974789915966,
            "rec": 0.6870748299319728,
            "f1": 0.5246753246753247
        },
        "position (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5142857142857142,
            "rec": 0.6,
            "f1": 0.5538461538461538
        },
        "scene (MME)": {
            "acc": 0.155,
            "prec": 0.21008403361344538,
            "rec": 0.25,
            "f1": 0.228310502283105
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6,
            "prec": 0.5897435897435898,
            "rec": 0.6571428571428571,
            "f1": 0.6216216216216216
        },
        "artwork (MME)": {
            "acc": 0.5825,
            "prec": 0.5587188612099644,
            "rec": 0.785,
            "f1": 0.6528066528066528
        },
        "landmark (MME)": {
            "acc": 0.2725,
            "prec": 0.32567049808429116,
            "rec": 0.425,
            "f1": 0.368763557483731
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.1,
            "f1": 0.16666666666666669
        },
        "existence (MME)": {
            "acc": 0.06666666666666667,
            "prec": 0.11764705882352941,
            "rec": 0.13333333333333333,
            "f1": 0.125
        },
        "numerical_calculation (MME)": {
            "acc": 0.35,
            "prec": 0.35,
            "rec": 0.35,
            "f1": 0.35
        },
        "count (MME)": {
            "acc": 0.7,
            "prec": 0.6764705882352942,
            "rec": 0.7666666666666667,
            "f1": 0.71875
        },
        "color (MME)": {
            "acc": 0.55,
            "prec": 0.5306122448979592,
            "rec": 0.8666666666666667,
            "f1": 0.6582278481012658
        },
        "OCR (MME)": {
            "acc": 0.45,
            "prec": 0.42857142857142855,
            "rec": 0.3,
            "f1": 0.3529411764705882
        },
        "code_reasoning (MME)": {
            "acc": 0.25,
            "prec": 0.3333333333333333,
            "rec": 0.5,
            "f1": 0.4
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22674418604651161
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2420091324200913
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.3049645390070922
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.18435754189944134
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26976744186046514
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.25552825552825553
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.255050505050505
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2765957446808511
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.28289473684210525
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25943396226415094
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.23863636363636365
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.24
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.25
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.23255813953488372
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2761904761904762
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.26153846153846155
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2694063926940639
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2122905027932961
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2837209302325581
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.26044226044226043
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.26262626262626265
        },
        "ocr (MMBench_EN)": {
            "acc": 0.22435897435897437
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.24293785310734464
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2198581560283688
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.24013157894736842
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_EN)": {
            "acc": 0.23113207547169812
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.25757575757575757
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.19333333333333333
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.24285714285714285
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Sociology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.26666666666666666
        },
        "History (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.1
        },
        "Accounting (MMMU)": {
            "acc": 0.3
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1721311475409836
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1830065359477124
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15294117647058825
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.18181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3287671232876712
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.2867132867132867
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.32653061224489793
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.21428571428571427
        },
        "Magnets (ScienceQA)": {
            "acc": 0.3181818181818182
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3275862068965517
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.4936708860759494
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.27586206896551724
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.39436619718309857
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "2D Count (CVBench)": {
            "acc": 0.4035532994923858
        },
        "3D Distance (CVBench)": {
            "acc": 0.35333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.31076923076923074
        },
        "3D Depth (CVBench)": {
            "acc": 0.29
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7946776151657104,
            "bert": 0.7504360920190811
        },
        "Whole dataset (Enrico)": {
            "bart": -7.189329110383987,
            "bert": 0.8129383379220962
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.1809776759147645,
            "bert": 0.9656421411037445
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.073573095798492,
            "bert": 0.8712378120422364
        },
        "Whole dataset (GQA)": {
            "bart": -5.39421368598938,
            "bert": 0.9936353039741516
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.406053838729858,
            "bert": 0.8009548586606979
        },
        "Whole dataset (INAT)": {
            "bart": -6.0140201234817505,
            "bert": 0.7850462377071381
        },
        "Whole dataset (IRFL)": {
            "bart": -4.778724584579468,
            "bert": 0.9986002844572067
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.379671013355255,
            "bert": 0.8558306390047073
        },
        "Whole dataset (Memotion)": {
            "bart": -4.7991596865654,
            "bert": 0.8847128754854202
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.817214756011963,
            "bert": 0.7852167314291001
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.832930660247802,
            "bert": 0.9983574068546295
        },
        "Whole dataset (NLVR)": {
            "bart": -3.958522517681122,
            "bert": 0.9815809041261673
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.394119579792022,
            "bert": 0.9684545677900315
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.682897598743439,
            "bert": 0.8921028655767441
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.5493964147567745,
            "bert": 0.9429338651895524
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.941356720924378,
            "bert": 0.8154561239480972
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.483130249977112,
            "bert": 0.9109144341945649
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.998622660636902,
            "bert": 0.764800032377243
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.771955189704895,
            "bert": 0.853041089773178
        },
        "Whole dataset (Slake)": {
            "bart": -5.231590447425842,
            "bert": 0.9953682613372803
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.511878576278686,
            "bert": 0.7648134016990662
        },
        "Whole dataset (VCR)": {
            "bart": -5.841908824443817,
            "bert": 0.8777348929643631
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.175679697990417,
            "bert": 0.9078992527723312
        },
        "Whole dataset (VQA)": {
            "bart": -6.951199197769165,
            "bert": 0.9746129053831101
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.183648028373718,
            "bert": 0.9407155191898346
        },
        "Whole dataset (Winoground)": {
            "bart": -3.687049918174744,
            "bert": 0.9929156458377838
        },
        "random (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.48322147651006714,
            "rec": 0.4864864864864865,
            "f1": 0.48484848484848486
        },
        "popular (POPE)": {
            "acc": 0.504885993485342,
            "prec": 0.5165562913907285,
            "rec": 0.4968152866242038,
            "f1": 0.5064935064935066
        },
        "adversarial (POPE)": {
            "acc": 0.493006993006993,
            "prec": 0.45323741007194246,
            "rec": 0.4772727272727273,
            "f1": 0.46494464944649444
        }
    },
    "gemma-instruct+2b+dinosiglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.38739513873951387
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.26223337515683814
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.272
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4384961176951369
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.25551020408163266
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.24597701149425288
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25595238095238093
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2504970178926441
        },
        "Instance Location (SEED_2)": {
            "acc": 0.401840490797546
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5154639175257731
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.5614312856238125
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.4587657018022938
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.17424242424242425
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2375249500998004
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.3531202435312024
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22021660649819494
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.22678396871945258
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6012084592145015
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.16666666666666666
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2138364779874214
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2636363636363636
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.22110552763819097
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.24489795918367346
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.6411764705882353,
            "prec": 0.6578947368421053,
            "rec": 0.5882352941176471,
            "f1": 0.6211180124223602
        },
        "posters (MME)": {
            "acc": 0.391156462585034,
            "prec": 0.4148936170212766,
            "rec": 0.5306122448979592,
            "f1": 0.4656716417910448
        },
        "position (MME)": {
            "acc": 0.5166666666666667,
            "prec": 0.5084745762711864,
            "rec": 1.0,
            "f1": 0.6741573033707865
        },
        "scene (MME)": {
            "acc": 0.175,
            "prec": 0.1782178217821782,
            "rec": 0.18,
            "f1": 0.17910447761194032
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5714285714285714,
            "prec": 0.543859649122807,
            "rec": 0.8857142857142857,
            "f1": 0.673913043478261
        },
        "artwork (MME)": {
            "acc": 0.575,
            "prec": 0.5619834710743802,
            "rec": 0.68,
            "f1": 0.6153846153846154
        },
        "landmark (MME)": {
            "acc": 0.215,
            "prec": 0.16071428571428573,
            "rec": 0.135,
            "f1": 0.14673913043478262
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.5263157894736842,
            "rec": 1.0,
            "f1": 0.6896551724137931
        },
        "existence (MME)": {
            "acc": 0.15,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.47368421052631576,
            "rec": 0.9,
            "f1": 0.6206896551724138
        },
        "count (MME)": {
            "acc": 0.6333333333333333,
            "prec": 0.5833333333333334,
            "rec": 0.9333333333333333,
            "f1": 0.7179487179487181
        },
        "color (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.23255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.25396825396825395
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.27692307692307694
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2511415525114155
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.24822695035460993
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.18994413407821228
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.20465116279069767
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_CN)": {
            "acc": 0.22435897435897437
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.20212765957446807
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.255
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2631578947368421
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24528301886792453
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.26
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.25
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.21511627906976744
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.24126984126984127
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.27692307692307694
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.273972602739726
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22905027932960895
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2186046511627907
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.1807909604519774
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.1950354609929078
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2598684210526316
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2215909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_EN)": {
            "acc": 0.20754716981132076
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.2
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Sociology (MMMU)": {
            "acc": 0.1
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.2
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.1
        },
        "Design (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1762295081967213
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.20634920634920634
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.19318181818181818
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3150684931506849
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.32653061224489793
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3709677419354839
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.2619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.33636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3275862068965517
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.32727272727272727
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Maps (ScienceQA)": {
            "acc": 0.41304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Classification (ScienceQA)": {
            "acc": 0.379746835443038
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.42258883248730966
        },
        "3D Distance (CVBench)": {
            "acc": 0.355
        },
        "2D Relation (CVBench)": {
            "acc": 0.32461538461538464
        },
        "3D Depth (CVBench)": {
            "acc": 0.2783333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.899351007938385,
            "bert": 0.7426817387342453
        },
        "Whole dataset (Enrico)": {
            "bart": -7.0034751701354985,
            "bert": 0.9344979280233383
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.433316667079925,
            "bert": 0.8356495612859726
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.9196767020225525,
            "bert": 0.8964285707473755
        },
        "Whole dataset (GQA)": {
            "bart": -5.659855394363404,
            "bert": 0.9935967880487442
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.775521278381348,
            "bert": 0.8046491885185242
        },
        "Whole dataset (INAT)": {
            "bart": -5.98798330783844,
            "bert": 0.7899366396665574
        },
        "Whole dataset (IRFL)": {
            "bart": -4.902775602340698,
            "bert": 0.9985577231645584
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.426693034172058,
            "bert": 0.8517718285322189
        },
        "Whole dataset (Memotion)": {
            "bart": -4.627305541038513,
            "bert": 0.8925557315349579
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.67755588054657,
            "bert": 0.7710163241624832
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.427725067138672,
            "bert": 0.9192391210794448
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5058889293670656,
            "bert": 0.919140642285347
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.526294159889221,
            "bert": 0.9403897041082382
        },
        "Whole dataset (OpenPath)": {
            "bart": -7.167839155197144,
            "bert": 0.8331092566251754
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.538990120887757,
            "bert": 0.9166830754280091
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.419939593076706,
            "bert": 0.8324914795160293
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.715880522727966,
            "bert": 0.8589444714784622
        },
        "Whole dataset (Slake)": {
            "bart": -5.3669951105117795,
            "bert": 0.9946211421489716
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.159765267372132,
            "bert": 0.7603579241037369
        },
        "Whole dataset (VCR)": {
            "bart": -5.630829820632934,
            "bert": 0.8923090952634811
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.193367846012116,
            "bert": 0.9061369270086288
        },
        "Whole dataset (VQA)": {
            "bart": -7.044387679100037,
            "bert": 0.9740831243991852
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.2625386428833005,
            "bert": 0.9429870527982712
        },
        "Whole dataset (Winoground)": {
            "bart": -3.8208464431762694,
            "bert": 0.9979918748140335
        },
        "random (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.4906832298136646,
            "rec": 0.5337837837837838,
            "f1": 0.511326860841424
        },
        "popular (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.5192307692307693,
            "rec": 0.5159235668789809,
            "f1": 0.5175718849840256
        },
        "adversarial (POPE)": {
            "acc": 0.48951048951048953,
            "prec": 0.4492753623188406,
            "rec": 0.4696969696969697,
            "f1": 0.45925925925925926
        }
    },
    "gemma-instruct+8b+clip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.46547644654764464
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24550898203592814
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24153074027603513
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.266
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.47691050265631385
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24571428571428572
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.335632183908046
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2390873015873016
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24519549370444002
        },
        "Instance Location (SEED_2)": {
            "acc": 0.48568507157464214
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6082474226804123
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.5987967067764408
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5155652648825778
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.26582278481012656
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4322678843226788
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2527075812274368
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24535679374389052
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.5921450151057401
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.25157232704402516
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.29393939393939394
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.25125628140703515
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.32098765432098764
        },
        "celebrity (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.4647058823529412,
            "f1": 0.4817073170731707
        },
        "posters (MME)": {
            "acc": 0.23469387755102042,
            "prec": 0.3010204081632653,
            "rec": 0.4013605442176871,
            "f1": 0.34402332361516036
        },
        "position (MME)": {
            "acc": 0.6166666666666667,
            "prec": 0.6,
            "rec": 0.7,
            "f1": 0.6461538461538462
        },
        "scene (MME)": {
            "acc": 0.2,
            "prec": 0.24789915966386555,
            "rec": 0.295,
            "f1": 0.2694063926940639
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.5428571428571428,
            "prec": 0.5483870967741935,
            "rec": 0.4857142857142857,
            "f1": 0.5151515151515151
        },
        "artwork (MME)": {
            "acc": 0.5475,
            "prec": 0.5333333333333333,
            "rec": 0.76,
            "f1": 0.6268041237113401
        },
        "landmark (MME)": {
            "acc": 0.365,
            "prec": 0.4171779141104294,
            "rec": 0.68,
            "f1": 0.5171102661596958
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.08333333333333333,
            "prec": 0.12121212121212122,
            "rec": 0.13333333333333333,
            "f1": 0.126984126984127
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.47058823529411764,
            "rec": 0.8,
            "f1": 0.5925925925925927
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.875,
            "rec": 0.7,
            "f1": 0.7777777777777777
        },
        "color (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6923076923076923,
            "rec": 0.6,
            "f1": 0.6428571428571429
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.85,
            "f1": 0.6296296296296295
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.27906976744186046
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.273015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2831050228310502
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.22695035460992907
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2011173184357542
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2803030303030303
        },
        "ocr (MMBench_CN)": {
            "acc": 0.28846153846153844
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.288135593220339
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.25886524822695034
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.28289473684210525
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.25
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25943396226415094
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2689393939393939
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.26666666666666666
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.2441860465116279
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2507936507936508
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2922374429223744
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.22695035460992907
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2122905027932961
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.27906976744186046
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24324324324324326
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2676767676767677
        },
        "ocr (MMBench_EN)": {
            "acc": 0.24358974358974358
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2542372881355932
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3262411347517731
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.22
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.27631578947368424
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.24431818181818182
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.18085106382978725
        },
        "image_style (MMBench_EN)": {
            "acc": 0.25
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.26136363636363635
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.26
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.21428571428571427
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.2
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.2
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.2
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.4
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18032786885245902
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1830065359477124
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.10588235294117647
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.17045454545454544
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3150684931506849
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "State capitals (ScienceQA)": {
            "acc": 0.4326923076923077
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3013698630136986
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.35664335664335667
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.5
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2926829268292683
        },
        "Geography (ScienceQA)": {
            "acc": 0.21428571428571427
        },
        "Magnets (ScienceQA)": {
            "acc": 0.3181818181818182
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.2413793103448276
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5185185185185185
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Classification (ScienceQA)": {
            "acc": 0.3924050632911392
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.5517241379310345
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.40482233502538073
        },
        "3D Distance (CVBench)": {
            "acc": 0.3616666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.3169230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.315
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.85090651512146,
            "bert": 0.7406131941080093
        },
        "Whole dataset (Enrico)": {
            "bart": -7.266197834014893,
            "bert": 0.8258696037530899
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.640443305969239,
            "bert": 0.8053448176383973
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.041585009098053,
            "bert": 0.8828988188505172
        },
        "Whole dataset (GQA)": {
            "bart": -5.908989758491516,
            "bert": 0.9808239024877549
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.452647070884705,
            "bert": 0.7904114925861359
        },
        "Whole dataset (INAT)": {
            "bart": -5.988065204620361,
            "bert": 0.7790151077508927
        },
        "Whole dataset (IRFL)": {
            "bart": -5.160015211105347,
            "bert": 0.998581964969635
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.37665730714798,
            "bert": 0.8458543938398361
        },
        "Whole dataset (Memotion)": {
            "bart": -4.837347390651703,
            "bert": 0.8611404585838318
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.849720735549926,
            "bert": 0.8238064002990723
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.332187619209289,
            "bert": 0.9382247227430344
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.610636222362518,
            "bert": 0.9062812447547912
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.53510998249054,
            "bert": 0.9013585126399994
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.227304110527038,
            "bert": 0.8138262552022933
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.58856568813324,
            "bert": 0.891578471660614
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.3865782737731935,
            "bert": 0.7907729023694992
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.687249126434327,
            "bert": 0.851925739645958
        },
        "Whole dataset (Slake)": {
            "bart": -5.859195637702942,
            "bert": 0.9782725584506988
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.763192176818848,
            "bert": 0.7913643008470536
        },
        "Whole dataset (VCR)": {
            "bart": -5.654758777618408,
            "bert": 0.8616618502140045
        },
        "Whole dataset (VisualGenome)": {
            "bart": -4.965309417247772,
            "bert": 0.890529026389122
        },
        "Whole dataset (VQA)": {
            "bart": -6.949340124130249,
            "bert": 0.9073449218273163
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.424630475044251,
            "bert": 0.9263426119089126
        },
        "Whole dataset (Winoground)": {
            "bart": -5.162190132141113,
            "bert": 0.9977225166559219
        },
        "random (POPE)": {
            "acc": 0.49185667752442996,
            "prec": 0.47297297297297297,
            "rec": 0.47297297297297297,
            "f1": 0.47297297297297297
        },
        "popular (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.5128205128205128,
            "rec": 0.5095541401273885,
            "f1": 0.5111821086261981
        },
        "adversarial (POPE)": {
            "acc": 0.4755244755244755,
            "prec": 0.4357142857142857,
            "rec": 0.4621212121212121,
            "f1": 0.44852941176470584
        }
    },
    "gemma-instruct+8b+siglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.43278124327812434
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2465495608531995
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2654690618762475
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.25
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5010216591744994
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2571428571428571
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.28045977011494255
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25595238095238093
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2379058979456594
        },
        "Instance Location (SEED_2)": {
            "acc": 0.43149284253578735
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5670103092783505
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.5905636478784041
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5117422173675588
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4140030441400304
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.26353790613718414
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24926686217008798
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6193353474320241
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.21666666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.34591194968553457
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.20100502512562815
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.3333333333333333
        },
        "celebrity (MME)": {
            "acc": 0.5823529411764706,
            "prec": 0.5729166666666666,
            "rec": 0.6470588235294118,
            "f1": 0.6077348066298343
        },
        "posters (MME)": {
            "acc": 0.35034013605442177,
            "prec": 0.3942307692307692,
            "rec": 0.5578231292517006,
            "f1": 0.46197183098591554
        },
        "position (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.6923076923076923,
            "rec": 0.6,
            "f1": 0.6428571428571429
        },
        "scene (MME)": {
            "acc": 0.2025,
            "prec": 0.26294820717131473,
            "rec": 0.33,
            "f1": 0.2926829268292683
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6142857142857143,
            "prec": 0.6739130434782609,
            "rec": 0.44285714285714284,
            "f1": 0.5344827586206895
        },
        "artwork (MME)": {
            "acc": 0.62,
            "prec": 0.6846153846153846,
            "rec": 0.445,
            "f1": 0.5393939393939394
        },
        "landmark (MME)": {
            "acc": 0.3425,
            "prec": 0.39805825242718446,
            "rec": 0.615,
            "f1": 0.48330058939096265
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6,
            "rec": 0.15,
            "f1": 0.24
        },
        "existence (MME)": {
            "acc": 0.06666666666666667,
            "prec": 0.09375,
            "rec": 0.1,
            "f1": 0.09677419354838711
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.3333333333333333,
            "rec": 0.2,
            "f1": 0.25
        },
        "count (MME)": {
            "acc": 0.7,
            "prec": 0.75,
            "rec": 0.6,
            "f1": 0.6666666666666665
        },
        "color (MME)": {
            "acc": 0.75,
            "prec": 0.7777777777777778,
            "rec": 0.7,
            "f1": 0.7368421052631577
        },
        "OCR (MME)": {
            "acc": 0.4,
            "prec": 0.3888888888888889,
            "rec": 0.35,
            "f1": 0.36842105263157887
        },
        "code_reasoning (MME)": {
            "acc": 0.65,
            "prec": 0.6875,
            "rec": 0.55,
            "f1": 0.6111111111111112
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.20348837209302326
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.24761904761904763
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2076923076923077
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2968036529680365
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.20567375886524822
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.22346368715083798
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24324324324324326
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.23484848484848486
        },
        "ocr (MMBench_CN)": {
            "acc": 0.2564102564102564
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.35106382978723405
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.24342105263157895
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_CN)": {
            "acc": 0.27358490566037735
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.29545454545454547
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.2733333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2571428571428571
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.1686046511627907
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2571428571428571
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2785388127853881
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.20567375886524822
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.18435754189944134
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.28837209302325584
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2398989898989899
        },
        "ocr (MMBench_EN)": {
            "acc": 0.24358974358974358
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3107344632768362
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.33687943262411346
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.21
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.28289473684210525
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2215909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2358490566037736
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2689393939393939
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.2866666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.22857142857142856
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.3
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.4666666666666667
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.3
        },
        "Design (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Literature (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.19262295081967212
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14705882352941177
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.20454545454545456
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3814102564102564
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "States of matter (ScienceQA)": {
            "acc": 0.42857142857142855
        },
        "Materials (ScienceQA)": {
            "acc": 0.32167832167832167
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4838709677419355
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.16666666666666666
        },
        "Magnets (ScienceQA)": {
            "acc": 0.33636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.28888888888888886
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.2608695652173913
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.48717948717948717
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.3125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.46296296296296297
        },
        "Maps (ScienceQA)": {
            "acc": 0.2826086956521739
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Classification (ScienceQA)": {
            "acc": 0.3670886075949367
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.36619718309859156
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.39847715736040606
        },
        "3D Distance (CVBench)": {
            "acc": 0.3616666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.30153846153846153
        },
        "3D Depth (CVBench)": {
            "acc": 0.29833333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.829740271568299,
            "bert": 0.7615319430828095
        },
        "Whole dataset (Enrico)": {
            "bart": -7.064537291526794,
            "bert": 0.8175503188371658
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.510458149909973,
            "bert": 0.8062956869602204
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.9446174359321593,
            "bert": 0.8957909256219864
        },
        "Whole dataset (GQA)": {
            "bart": -5.869927644729614,
            "bert": 0.9934491664171219
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.594406542778015,
            "bert": 0.7861225038766861
        },
        "Whole dataset (INAT)": {
            "bart": -6.042014408111572,
            "bert": 0.7817644453048707
        },
        "Whole dataset (IRFL)": {
            "bart": -4.86718071937561,
            "bert": 0.9985464096069336
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.4015614032745365,
            "bert": 0.8450417536497116
        },
        "Whole dataset (Memotion)": {
            "bart": -4.730349080562592,
            "bert": 0.8840078282356262
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.9313650536537175,
            "bert": 0.8493493819236755
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.956430735588074,
            "bert": 0.9945408087968827
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992874920368194
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5445998525619506,
            "bert": 0.9182474732398986
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.558158669471741,
            "bert": 0.9373579478263855
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.797065253257752,
            "bert": 0.8105428141355514
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.7927806854248045,
            "bert": 0.9136236375570297
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.243720607757568,
            "bert": 0.8132669937610626
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.661763234138489,
            "bert": 0.8542686480283738
        },
        "Whole dataset (Slake)": {
            "bart": -6.098499212265015,
            "bert": 0.9922842282056809
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.717454504966736,
            "bert": 0.7857868582010269
        },
        "Whole dataset (VCR)": {
            "bart": -5.777270512580872,
            "bert": 0.8652501046657562
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.13925053358078,
            "bert": 0.9080042791366577
        },
        "Whole dataset (VQA)": {
            "bart": -7.1232352590560915,
            "bert": 0.9710416781902313
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.222148408889771,
            "bert": 0.9371640658378602
        },
        "Whole dataset (Winoground)": {
            "bart": -4.662055082321167,
            "bert": 0.9978500127792358
        },
        "random (POPE)": {
            "acc": 0.4755700325732899,
            "prec": 0.4557823129251701,
            "rec": 0.4527027027027027,
            "f1": 0.45423728813559316
        },
        "popular (POPE)": {
            "acc": 0.48534201954397393,
            "prec": 0.4968944099378882,
            "rec": 0.5095541401273885,
            "f1": 0.5031446540880504
        },
        "adversarial (POPE)": {
            "acc": 0.506993006993007,
            "prec": 0.46715328467153283,
            "rec": 0.48484848484848486,
            "f1": 0.47583643122676583
        }
    },
    "gemma-instruct+8b+dinosiglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.42374704237470423
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23952095808383234
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.26097867001254704
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2754491017964072
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.26
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.4801798120147119
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2473469387755102
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2827586206896552
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2787698412698413
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2531477799867462
        },
        "Instance Location (SEED_2)": {
            "acc": 0.4887525562372188
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5567010309278351
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.5953134895503484
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.4893500819224468
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4200913242009132
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.2779783393501805
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.21505376344086022
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6012084592145015
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.225
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2893081761006289
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2613065326633166
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.1836734693877551
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.38271604938271603
        },
        "celebrity (MME)": {
            "acc": 0.5558823529411765,
            "prec": 0.7317073170731707,
            "rec": 0.17647058823529413,
            "f1": 0.2843601895734597
        },
        "posters (MME)": {
            "acc": 0.391156462585034,
            "prec": 0.43388429752066116,
            "rec": 0.7142857142857143,
            "f1": 0.5398457583547559
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.7407407407407407,
            "rec": 0.6666666666666666,
            "f1": 0.7017543859649122
        },
        "scene (MME)": {
            "acc": 0.2,
            "prec": 0.2637795275590551,
            "rec": 0.335,
            "f1": 0.29515418502202645
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6214285714285714,
            "prec": 0.6888888888888889,
            "rec": 0.44285714285714284,
            "f1": 0.5391304347826087
        },
        "artwork (MME)": {
            "acc": 0.575,
            "prec": 0.5925925925925926,
            "rec": 0.48,
            "f1": 0.5303867403314917
        },
        "landmark (MME)": {
            "acc": 0.365,
            "prec": 0.41818181818181815,
            "rec": 0.69,
            "f1": 0.520754716981132
        },
        "text_translation (MME)": {
            "acc": 0.475,
            "prec": 0.3333333333333333,
            "rec": 0.05,
            "f1": 0.08695652173913045
        },
        "existence (MME)": {
            "acc": 0.08333333333333333,
            "prec": 0.0967741935483871,
            "rec": 0.1,
            "f1": 0.09836065573770492
        },
        "numerical_calculation (MME)": {
            "acc": 0.45,
            "prec": 0.42857142857142855,
            "rec": 0.3,
            "f1": 0.3529411764705882
        },
        "count (MME)": {
            "acc": 0.7,
            "prec": 0.875,
            "rec": 0.4666666666666667,
            "f1": 0.608695652173913
        },
        "color (MME)": {
            "acc": 0.5666666666666667,
            "prec": 0.6666666666666666,
            "rec": 0.26666666666666666,
            "f1": 0.3809523809523809
        },
        "OCR (MME)": {
            "acc": 0.35,
            "prec": 0.3125,
            "rec": 0.25,
            "f1": 0.2777777777777778
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5217391304347826,
            "rec": 0.6,
            "f1": 0.5581395348837209
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.19186046511627908
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.29523809523809524
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2648401826484018
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.20567375886524822
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2122905027932961
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.29767441860465116
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.2334152334152334
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2601010101010101
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.2994350282485876
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.3333333333333333
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.255
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.27631578947368424
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25943396226415094
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.25757575757575757
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.29333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.19186046511627908
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2857142857142857
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3105022831050228
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.18439716312056736
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.20670391061452514
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2651162790697674
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.2334152334152334
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.25252525252525254
        },
        "ocr (MMBench_EN)": {
            "acc": 0.24358974358974358
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.3107344632768362
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.3333333333333333
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.255
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.26973684210526316
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2783018867924528
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.23484848484848486
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.32666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.22142857142857142
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.06666666666666667
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.22540983606557377
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.12698412698412698
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.11764705882352941
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.13725490196078433
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.125
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3835616438356164
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.36538461538461536
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3698630136986301
        },
        "States of matter (ScienceQA)": {
            "acc": 0.39285714285714285
        },
        "Materials (ScienceQA)": {
            "acc": 0.3356643356643357
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.45161290322580644
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3170731707317073
        },
        "Geography (ScienceQA)": {
            "acc": 0.21428571428571427
        },
        "Magnets (ScienceQA)": {
            "acc": 0.2909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.29411764705882354
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.20689655172413793
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.4050632911392405
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "2D Count (CVBench)": {
            "acc": 0.41878172588832485
        },
        "3D Distance (CVBench)": {
            "acc": 0.3566666666666667
        },
        "2D Relation (CVBench)": {
            "acc": 0.32153846153846155
        },
        "3D Depth (CVBench)": {
            "acc": 0.29333333333333333
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7986985468864445,
            "bert": 0.7544950813055038
        },
        "Whole dataset (Enrico)": {
            "bart": -7.071822981834412,
            "bert": 0.8316735523939133
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.295492396354676,
            "bert": 0.93513607442379
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.001734561920166,
            "bert": 0.8837996429204941
        },
        "Whole dataset (GQA)": {
            "bart": -5.966128368377685,
            "bert": 0.9862990516424179
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -7.40968816280365,
            "bert": 0.7979100614786148
        },
        "Whole dataset (INAT)": {
            "bart": -6.027133631706238,
            "bert": 0.7858906918764115
        },
        "Whole dataset (IRFL)": {
            "bart": -4.94469259262085,
            "bert": 0.9985122919082642
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.3797044801712035,
            "bert": 0.8431830966472625
        },
        "Whole dataset (Memotion)": {
            "bart": -4.8216561627388,
            "bert": 0.8878756195306778
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.7963121342659,
            "bert": 0.8218790459632873
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.362988300323487,
            "bert": 0.8603694540262222
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9994716823101044
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993326097726822
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.630938675403595,
            "bert": 0.8965339267253876
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.4922922372817995,
            "bert": 0.8816759300231933
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.873279762268067,
            "bert": 0.7985800069570541
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.614795227050781,
            "bert": 0.8303084307909012
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.291522316932678,
            "bert": 0.8169899386167526
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.715647807121277,
            "bert": 0.8523217910528182
        },
        "Whole dataset (Slake)": {
            "bart": -6.074223484992981,
            "bert": 0.9324317598342895
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.836752352714538,
            "bert": 0.8184329557418824
        },
        "Whole dataset (VCR)": {
            "bart": -5.727873296737671,
            "bert": 0.8705201011896133
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.056147801876068,
            "bert": 0.8993321621417999
        },
        "Whole dataset (VQA)": {
            "bart": -6.969184551239014,
            "bert": 0.9256627005338669
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.88975338935852,
            "bert": 0.8342992287874221
        },
        "Whole dataset (Winoground)": {
            "bart": -4.848104925155639,
            "bert": 0.9977993726730346
        },
        "random (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.49032258064516127,
            "rec": 0.5135135135135135,
            "f1": 0.5016501650165015
        },
        "popular (POPE)": {
            "acc": 0.5146579804560261,
            "prec": 0.5256410256410257,
            "rec": 0.5222929936305732,
            "f1": 0.523961661341853
        },
        "adversarial (POPE)": {
            "acc": 0.479020979020979,
            "prec": 0.4405594405594406,
            "rec": 0.4772727272727273,
            "f1": 0.45818181818181813
        }
    },
    "llama2-chat+7b+clip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6321789632178964
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24843161856963614
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.29141716566866266
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.244
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5341234164282795
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2310204081632653
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.26666666666666666
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.27976190476190477
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2531477799867462
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5316973415132924
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6706776440785307
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6340797378481704
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25948103792415167
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4809741248097412
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24548736462093862
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.25219941348973607
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.7009063444108762
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.21666666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.18238993710691823
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2787878787878788
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.20603015075376885
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.6647058823529411,
            "prec": 0.9666666666666667,
            "rec": 0.3411764705882353,
            "f1": 0.5043478260869566
        },
        "posters (MME)": {
            "acc": 0.20408163265306123,
            "prec": 0.2835820895522388,
            "rec": 0.3877551020408163,
            "f1": 0.3275862068965517
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6666666666666666,
            "rec": 0.8,
            "f1": 0.7272727272727272
        },
        "scene (MME)": {
            "acc": 0.1575,
            "prec": 0.2157676348547718,
            "rec": 0.26,
            "f1": 0.235827664399093
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6714285714285714,
            "prec": 0.7068965517241379,
            "rec": 0.5857142857142857,
            "f1": 0.640625
        },
        "artwork (MME)": {
            "acc": 0.73,
            "prec": 0.7211538461538461,
            "rec": 0.75,
            "f1": 0.7352941176470588
        },
        "landmark (MME)": {
            "acc": 0.405,
            "prec": 0.44751381215469616,
            "rec": 0.81,
            "f1": 0.5765124555160143
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.03333333333333333,
            "prec": 0.0625,
            "rec": 0.06666666666666667,
            "f1": 0.06451612903225808
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.78125,
            "rec": 0.8333333333333334,
            "f1": 0.8064516129032259
        },
        "color (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8108108108108109,
            "rec": 1.0,
            "f1": 0.8955223880597014
        },
        "OCR (MME)": {
            "acc": 0.3,
            "prec": 0.3,
            "rec": 0.3,
            "f1": 0.3
        },
        "code_reasoning (MME)": {
            "acc": 0.45,
            "prec": 0.47058823529411764,
            "rec": 0.8,
            "f1": 0.5925925925925927
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.2441860465116279
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.28888888888888886
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2694063926940639
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2553191489361702
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.16759776536312848
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24078624078624078
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.26515151515151514
        },
        "ocr (MMBench_CN)": {
            "acc": 0.1987179487179487
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24858757062146894
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.25886524822695034
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.26973684210526316
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_CN)": {
            "acc": 0.25
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.21333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.20930232558139536
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.30158730158730157
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2968036529680365
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.17318435754189945
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.24494949494949494
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21153846153846154
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.23728813559322035
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.24822695035460993
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.24342105263157895
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.23863636363636365
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_EN)": {
            "acc": 0.20754716981132076
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.25
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.20666666666666667
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.25
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.1
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Math (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Pharmacy (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.1
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.1
        },
        "Agriculture (MMMU)": {
            "acc": 0.1
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.13524590163934427
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15079365079365079
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1830065359477124
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.14215686274509803
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.20454545454545456
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3424657534246575
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.375
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.273972602739726
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.3356643356643357
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3387096774193548
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.4146341463414634
        },
        "Geography (ScienceQA)": {
            "acc": 0.30952380952380953
        },
        "Magnets (ScienceQA)": {
            "acc": 0.37272727272727274
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.39655172413793105
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4444444444444444
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.41304347826086957
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Classification (ScienceQA)": {
            "acc": 0.4050632911392405
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.323943661971831
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.4149746192893401
        },
        "3D Distance (CVBench)": {
            "acc": 0.375
        },
        "2D Relation (CVBench)": {
            "acc": 0.32769230769230767
        },
        "3D Depth (CVBench)": {
            "acc": 0.32166666666666666
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.4788665676116945,
            "bert": 0.8166891199350357
        },
        "Whole dataset (Enrico)": {
            "bart": -6.9095976710319515,
            "bert": 0.9783386361598968
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.799462108612061,
            "bert": 0.9915937435626984
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.962291955947876,
            "bert": 0.8815801149606705
        },
        "Whole dataset (GQA)": {
            "bart": -5.765295977592468,
            "bert": 0.9936464607715607
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.848619055747986,
            "bert": 0.8022129482030869
        },
        "Whole dataset (INAT)": {
            "bart": -5.92171088218689,
            "bert": 0.7956847214698791
        },
        "Whole dataset (IRFL)": {
            "bart": -4.804013013839722,
            "bert": 0.9986694157123566
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.43589946269989,
            "bert": 0.8501733535528183
        },
        "Whole dataset (Memotion)": {
            "bart": -4.527449867725372,
            "bert": 0.9015156865119934
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.441395633220672,
            "bert": 0.8470949023962021
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -5.010958442687988,
            "bert": 0.9982482248544693
        },
        "Whole dataset (NLVR)": {
            "bart": -3.309732702970505,
            "bert": 0.9991968798637391
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.136611211299896,
            "bert": 0.9992257535457612
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.56658456325531,
            "bert": 0.9048742574453353
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.54078999042511,
            "bert": 0.9385451477766037
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.398101422786713,
            "bert": 0.8523909968137741
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.763255038261414,
            "bert": 0.9144657462835312
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.648782181739807,
            "bert": 0.9382546061277389
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.755790271759033,
            "bert": 0.8554284930229187
        },
        "Whole dataset (Slake)": {
            "bart": -5.497061109542846,
            "bert": 0.995474078655243
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.746898010969162,
            "bert": 0.9115905088186264
        },
        "Whole dataset (VCR)": {
            "bart": -5.515665473937989,
            "bert": 0.9179053753614426
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.069362057447433,
            "bert": 0.9043929272890091
        },
        "Whole dataset (VQA)": {
            "bart": -7.048803205490112,
            "bert": 0.9738224333524704
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.19350299835205,
            "bert": 0.9416162949800492
        },
        "Whole dataset (Winoground)": {
            "bart": -4.638519973754883,
            "bert": 0.9978697645664215
        },
        "random (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.48366013071895425,
            "rec": 0.5,
            "f1": 0.49169435215946844
        },
        "popular (POPE)": {
            "acc": 0.4755700325732899,
            "prec": 0.4878048780487805,
            "rec": 0.5095541401273885,
            "f1": 0.4984423676012461
        },
        "adversarial (POPE)": {
            "acc": 0.4965034965034965,
            "prec": 0.45714285714285713,
            "rec": 0.48484848484848486,
            "f1": 0.4705882352941177
        }
    },
    "llama2-chat+7b+siglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6403527640352764
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.23552894211576847
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.24843161856963614
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27345309381237526
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.254
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5729464650592563
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2310204081632653
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.27816091954022987
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.27976190476190477
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24652087475149106
        },
        "Instance Location (SEED_2)": {
            "acc": 0.549079754601227
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6185567010309279
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.668777707409753
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6357181867831786
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.29545454545454547
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4855403348554033
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.21660649819494585
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24633431085043989
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6827794561933535
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2389937106918239
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2636363636363636
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21105527638190955
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.38271604938271603
        },
        "celebrity (MME)": {
            "acc": 0.6352941176470588,
            "prec": 0.9791666666666666,
            "rec": 0.27647058823529413,
            "f1": 0.4311926605504587
        },
        "posters (MME)": {
            "acc": 0.14625850340136054,
            "prec": 0.22340425531914893,
            "rec": 0.2857142857142857,
            "f1": 0.2507462686567164
        },
        "position (MME)": {
            "acc": 0.7666666666666667,
            "prec": 0.7105263157894737,
            "rec": 0.9,
            "f1": 0.7941176470588235
        },
        "scene (MME)": {
            "acc": 0.155,
            "prec": 0.2125,
            "rec": 0.255,
            "f1": 0.2318181818181818
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6571428571428571,
            "prec": 0.6774193548387096,
            "rec": 0.6,
            "f1": 0.6363636363636364
        },
        "artwork (MME)": {
            "acc": 0.71,
            "prec": 0.6875,
            "rec": 0.77,
            "f1": 0.7264150943396227
        },
        "landmark (MME)": {
            "acc": 0.33,
            "prec": 0.396969696969697,
            "rec": 0.655,
            "f1": 0.49433962264150944
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.03333333333333333,
            "prec": 0.0625,
            "rec": 0.06666666666666667,
            "f1": 0.06451612903225808
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.4,
            "rec": 0.1,
            "f1": 0.16000000000000003
        },
        "count (MME)": {
            "acc": 0.8,
            "prec": 0.78125,
            "rec": 0.8333333333333334,
            "f1": 0.8064516129032259
        },
        "color (MME)": {
            "acc": 0.8666666666666667,
            "prec": 0.7894736842105263,
            "rec": 1.0,
            "f1": 0.8823529411764706
        },
        "OCR (MME)": {
            "acc": 0.325,
            "prec": 0.3157894736842105,
            "rec": 0.3,
            "f1": 0.3076923076923077
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.20930232558139536
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.273015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.26153846153846155
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.3242009132420091
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.1787709497206704
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.27906976744186046
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.26262626262626265
        },
        "ocr (MMBench_CN)": {
            "acc": 0.22435897435897437
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23728813559322035
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.24468085106382978
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2730263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.18085106382978725
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24056603773584906
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.24621212121212122
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.18
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.24285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.18604651162790697
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.273015873015873
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.24615384615384617
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3105022831050228
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2765957446808511
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2011173184357542
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24078624078624078
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2474747474747475
        },
        "ocr (MMBench_EN)": {
            "acc": 0.25
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.1864406779661017
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.24
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.28289473684210525
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2215909090909091
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2169811320754717
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Public_Health (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.2
        },
        "Art_Theory (MMMU)": {
            "acc": 0.3
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.1
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.2
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1885245901639344
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.15873015873015872
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.16993464052287582
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.20098039215686275
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.20454545454545456
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3287671232876712
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.5128205128205128
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3942307692307692
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.273972602739726
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.3006993006993007
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3673469387755102
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4032258064516129
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3170731707317073
        },
        "Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.35294117647058826
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.4
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.375
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Classification (ScienceQA)": {
            "acc": 0.3670886075949367
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.29577464788732394
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "2D Count (CVBench)": {
            "acc": 0.38578680203045684
        },
        "3D Distance (CVBench)": {
            "acc": 0.365
        },
        "2D Relation (CVBench)": {
            "acc": 0.30615384615384617
        },
        "3D Depth (CVBench)": {
            "acc": 0.3016666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.384813015460968,
            "bert": 0.8217548114061356
        },
        "Whole dataset (Enrico)": {
            "bart": -6.869243786334992,
            "bert": 0.9813282269239426
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.605696449279785,
            "bert": 0.99705382168293
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.096731133460999,
            "bert": 0.8674213868379593
        },
        "Whole dataset (GQA)": {
            "bart": -5.383829102516175,
            "bert": 0.9936249488592148
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.920601539611816,
            "bert": 0.7990917098522187
        },
        "Whole dataset (INAT)": {
            "bart": -5.925120310783386,
            "bert": 0.7959511953592301
        },
        "Whole dataset (IRFL)": {
            "bart": -4.806881847381592,
            "bert": 0.99870461165905
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.417710964679718,
            "bert": 0.8498684537410736
        },
        "Whole dataset (Memotion)": {
            "bart": -4.434683666229248,
            "bert": 0.9043404740095139
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.4342405581474305,
            "bert": 0.8492189353704452
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.871978816986084,
            "bert": 0.9983638709783554
        },
        "Whole dataset (NLVR)": {
            "bart": -3.454360021352768,
            "bert": 0.9992354989051819
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.505553414821625,
            "bert": 0.9992392528057098
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.760721142292023,
            "bert": 0.8833142614364624
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.576518797874451,
            "bert": 0.9400765067338943
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.940881659984589,
            "bert": 0.8612395519018173
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.834103946685791,
            "bert": 0.9105034708976746
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.575395154953003,
            "bert": 0.930059231519699
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.812316431999206,
            "bert": 0.8580605065822602
        },
        "Whole dataset (Slake)": {
            "bart": -5.7240390920639035,
            "bert": 0.995456428527832
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.80285085439682,
            "bert": 0.8935227340459824
        },
        "Whole dataset (VCR)": {
            "bart": -5.531171023845673,
            "bert": 0.9192172384262085
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.123949165344238,
            "bert": 0.9034819757938385
        },
        "Whole dataset (VQA)": {
            "bart": -7.033416447639465,
            "bert": 0.9766450172662735
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.3416478633880615,
            "bert": 0.9400686800479889
        },
        "Whole dataset (Winoground)": {
            "bart": -4.467983179092407,
            "bert": 0.9978943657875061
        },
        "random (POPE)": {
            "acc": 0.48534201954397393,
            "prec": 0.4666666666666667,
            "rec": 0.47297297297297297,
            "f1": 0.4697986577181208
        },
        "popular (POPE)": {
            "acc": 0.495114006514658,
            "prec": 0.5060975609756098,
            "rec": 0.5286624203821656,
            "f1": 0.5171339563862929
        },
        "adversarial (POPE)": {
            "acc": 0.4755244755244755,
            "prec": 0.43661971830985913,
            "rec": 0.4696969696969697,
            "f1": 0.4525547445255474
        }
    },
    "llama2-chat+7b+dinosiglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.4570875457087546
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.22554890219560877
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.25407779171894607
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.29740518962075846
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.24
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.49693502247650184
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23020408163265307
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.296551724137931
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.26785714285714285
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.23591782637508282
        },
        "Instance Location (SEED_2)": {
            "acc": 0.49897750511247446
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5773195876288659
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6285623812539582
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5707263790278536
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.24050632911392406
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.2878787878787879
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2435129740518962
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4277016742770167
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23826714801444043
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2375366568914956
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6646525679758308
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.20833333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.3
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.22613065326633167
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.14285714285714285
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.5352941176470588,
            "prec": 0.875,
            "rec": 0.08235294117647059,
            "f1": 0.15053763440860213
        },
        "posters (MME)": {
            "acc": 0.2857142857142857,
            "prec": 0.3574660633484163,
            "rec": 0.5374149659863946,
            "f1": 0.42934782608695654
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.7777777777777778,
            "rec": 0.7,
            "f1": 0.7368421052631577
        },
        "scene (MME)": {
            "acc": 0.19,
            "prec": 0.26865671641791045,
            "rec": 0.36,
            "f1": 0.3076923076923077
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6357142857142857,
            "prec": 0.6862745098039216,
            "rec": 0.5,
            "f1": 0.5785123966942148
        },
        "artwork (MME)": {
            "acc": 0.71,
            "prec": 0.7210526315789474,
            "rec": 0.685,
            "f1": 0.7025641025641026
        },
        "landmark (MME)": {
            "acc": 0.4375,
            "prec": 0.4666666666666667,
            "rec": 0.875,
            "f1": 0.608695652173913
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.06666666666666667,
            "prec": 0.11764705882352941,
            "rec": 0.13333333333333333,
            "f1": 0.125
        },
        "numerical_calculation (MME)": {
            "acc": 0.55,
            "prec": 0.6666666666666666,
            "rec": 0.2,
            "f1": 0.30769230769230765
        },
        "count (MME)": {
            "acc": 0.75,
            "prec": 0.8571428571428571,
            "rec": 0.6,
            "f1": 0.7058823529411764
        },
        "color (MME)": {
            "acc": 0.6666666666666666,
            "prec": 0.7777777777777778,
            "rec": 0.4666666666666667,
            "f1": 0.5833333333333334
        },
        "OCR (MME)": {
            "acc": 0.3,
            "prec": 0.3333333333333333,
            "rec": 0.4,
            "f1": 0.3636363636363636
        },
        "code_reasoning (MME)": {
            "acc": 0.55,
            "prec": 0.5416666666666666,
            "rec": 0.65,
            "f1": 0.5909090909090908
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22093023255813954
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.26666666666666666
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.3230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2831050228310502
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2198581560283688
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.1787709497206704
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.27906976744186046
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.25252525252525254
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.22598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.25177304964539005
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.245
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2730263157894737
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.19886363636363635
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.18085106382978725
        },
        "image_style (MMBench_CN)": {
            "acc": 0.23113207547169812
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.26136363636363635
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.19333333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.24285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.22093023255813954
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2698412698412698
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2785388127853881
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.18994413407821228
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2837209302325581
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24324324324324326
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.24494949494949494
        },
        "ocr (MMBench_EN)": {
            "acc": 0.23717948717948717
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.1977401129943503
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.245
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2565789473684211
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.24431818181818182
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_EN)": {
            "acc": 0.22641509433962265
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.22142857142857142
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.2
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.3
        },
        "Sociology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art_Theory (MMMU)": {
            "acc": 0.26666666666666666
        },
        "History (MMMU)": {
            "acc": 0.2
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Economics (MMMU)": {
            "acc": 0.2
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.2
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.3
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.2
        },
        "Literature (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Biology (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18442622950819673
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.14285714285714285
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1503267973856209
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.15294117647058825
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.1568627450980392
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.25
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3287671232876712
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "State capitals (ScienceQA)": {
            "acc": 0.36217948717948717
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.273972602739726
        },
        "States of matter (ScienceQA)": {
            "acc": 0.39285714285714285
        },
        "Materials (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.5121951219512195
        },
        "Geography (ScienceQA)": {
            "acc": 0.30952380952380953
        },
        "Magnets (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.3125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Classification (ScienceQA)": {
            "acc": 0.43037974683544306
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.352112676056338
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.47368421052631576
        },
        "2D Count (CVBench)": {
            "acc": 0.4073604060913706
        },
        "3D Distance (CVBench)": {
            "acc": 0.37333333333333335
        },
        "2D Relation (CVBench)": {
            "acc": 0.3169230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.31333333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.263539865016937,
            "bert": 0.8476117271184921
        },
        "Whole dataset (Enrico)": {
            "bart": -7.011052225828171,
            "bert": 0.9870393520593643
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.907411911487579,
            "bert": 0.8524546647071838
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.024909403324127,
            "bert": 0.8710661965608597
        },
        "Whole dataset (GQA)": {
            "bart": -5.937898015975952,
            "bert": 0.9936527967453003
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.796285886764526,
            "bert": 0.8005739158391952
        },
        "Whole dataset (INAT)": {
            "bart": -5.931800049543381,
            "bert": 0.794322339296341
        },
        "Whole dataset (IRFL)": {
            "bart": -4.840192337036132,
            "bert": 0.9985710108280181
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.400904109477997,
            "bert": 0.8511968284845353
        },
        "Whole dataset (Memotion)": {
            "bart": -4.563945958614349,
            "bert": 0.8953976953029632
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.447320921421051,
            "bert": 0.8381107383966446
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.773216228485108,
            "bert": 0.9983710539340973
        },
        "Whole dataset (NLVR)": {
            "bart": -3.0831562805175783,
            "bert": 0.9989935791492462
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.1963768863677977,
            "bert": 0.9991946959495545
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.6654800963401795,
            "bert": 0.8897190606594085
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.541340823173523,
            "bert": 0.941123948097229
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.915811538696289,
            "bert": 0.8542553126811981
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.454475932121277,
            "bert": 0.9112174040079117
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.553730063438415,
            "bert": 0.9206198644638062
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.832717943191528,
            "bert": 0.853060639500618
        },
        "Whole dataset (Slake)": {
            "bart": -5.697570338249206,
            "bert": 0.9954461938142777
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.712836861610413,
            "bert": 0.8255262649059296
        },
        "Whole dataset (VCR)": {
            "bart": -5.554749615192414,
            "bert": 0.8970830529928208
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.079059901237488,
            "bert": 0.9044627696275711
        },
        "Whole dataset (VQA)": {
            "bart": -7.030260744094849,
            "bert": 0.9783145987987518
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.266771860122681,
            "bert": 0.9407205963134766
        },
        "Whole dataset (Winoground)": {
            "bart": -4.641388807296753,
            "bert": 0.9978352874517441
        },
        "random (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.4900662251655629,
            "rec": 0.5,
            "f1": 0.4949832775919732
        },
        "popular (POPE)": {
            "acc": 0.5146579804560261,
            "prec": 0.5256410256410257,
            "rec": 0.5222929936305732,
            "f1": 0.523961661341853
        },
        "adversarial (POPE)": {
            "acc": 0.486013986013986,
            "prec": 0.44680851063829785,
            "rec": 0.4772727272727273,
            "f1": 0.46153846153846156
        }
    },
    "llama3-instruct+8b+clip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6246504624650463
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2590966122961104
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.27944111776447106
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.24
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5480179812014712
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.22857142857142856
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.30344827586206896
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2847222222222222
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2485089463220676
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5664621676891616
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6804123711340206
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6760607979734009
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6499180775532496
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.21518987341772153
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.249500998003992
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.5038051750380518
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20577617328519857
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24535679374389052
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6555891238670695
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.23333333333333334
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.2515151515151515
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2814070351758794
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.20408163265306123
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7970588235294118,
            "prec": 0.7393364928909952,
            "rec": 0.9176470588235294,
            "f1": 0.8188976377952756
        },
        "posters (MME)": {
            "acc": 0.15306122448979592,
            "prec": 0.16,
            "rec": 0.16326530612244897,
            "f1": 0.1616161616161616
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6444444444444445,
            "rec": 0.9666666666666667,
            "f1": 0.7733333333333334
        },
        "scene (MME)": {
            "acc": 0.115,
            "prec": 0.13679245283018868,
            "rec": 0.145,
            "f1": 0.1407766990291262
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7357142857142858,
            "prec": 0.6941176470588235,
            "rec": 0.8428571428571429,
            "f1": 0.7612903225806451
        },
        "artwork (MME)": {
            "acc": 0.6075,
            "prec": 0.5609065155807366,
            "rec": 0.99,
            "f1": 0.7160940325497287
        },
        "landmark (MME)": {
            "acc": 0.2475,
            "prec": 0.028037383177570093,
            "rec": 0.015,
            "f1": 0.019543973941368076
        },
        "text_translation (MME)": {
            "acc": 0.6,
            "prec": 0.6666666666666666,
            "rec": 0.4,
            "f1": 0.5
        },
        "existence (MME)": {
            "acc": 0.05,
            "prec": 0.034482758620689655,
            "rec": 0.03333333333333333,
            "f1": 0.03389830508474576
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.4857142857142857,
            "rec": 0.85,
            "f1": 0.6181818181818183
        },
        "count (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.7631578947368421,
            "rec": 0.9666666666666667,
            "f1": 0.8529411764705883
        },
        "color (MME)": {
            "acc": 0.8,
            "prec": 0.725,
            "rec": 0.9666666666666667,
            "f1": 0.8285714285714285
        },
        "OCR (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.48717948717948717,
            "rec": 0.95,
            "f1": 0.6440677966101694
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.20348837209302326
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.29523809523809524
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2230769230769231
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2876712328767123
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.24581005586592178
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26976744186046514
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.26515151515151514
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2907801418439716
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.3026315789473684
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24528301886792453
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.23484848484848486
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.2733333333333333
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.20930232558139536
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.29523809523809524
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2785388127853881
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2695035460992908
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.2011173184357542
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2651162790697674
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.2334152334152334
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2474747474747475
        },
        "ocr (MMBench_EN)": {
            "acc": 0.23717948717948717
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.22033898305084745
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.24113475177304963
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.27960526315789475
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.20454545454545456
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.26595744680851063
        },
        "image_style (MMBench_EN)": {
            "acc": 0.22641509433962265
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.26136363636363635
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.22857142857142856
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Math (MMMU)": {
            "acc": 0.2
        },
        "Pharmacy (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.2
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.16666666666666666
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Geography (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Chemistry (MMMU)": {
            "acc": 0.2
        },
        "Electronics (MMMU)": {
            "acc": 0.0
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Manage (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.1
        },
        "Marketing (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.2
        },
        "Literature (MMMU)": {
            "acc": 0.2
        },
        "Biology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.16666666666666666
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18442622950819673
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.18181818181818182
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.273972602739726
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.36538461538461536
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.2602739726027397
        },
        "States of matter (ScienceQA)": {
            "acc": 0.39285714285714285
        },
        "Materials (ScienceQA)": {
            "acc": 0.32867132867132864
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.37755102040816324
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.43548387096774194
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.2619047619047619
        },
        "Magnets (ScienceQA)": {
            "acc": 0.33636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.43103448275862066
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.4
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.41304347826086957
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.1875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.5370370370370371
        },
        "Maps (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Classification (ScienceQA)": {
            "acc": 0.4177215189873418
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3448275862068966
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.36619718309859156
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.39473684210526316
        },
        "2D Count (CVBench)": {
            "acc": 0.42258883248730966
        },
        "3D Distance (CVBench)": {
            "acc": 0.37666666666666665
        },
        "2D Relation (CVBench)": {
            "acc": 0.3046153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.29833333333333334
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.805185618400574,
            "bert": 0.7686372816562652
        },
        "Whole dataset (Enrico)": {
            "bart": -7.133774981498719,
            "bert": 0.9767230361700058
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.912836165428161,
            "bert": 0.9952694869041443
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.934589066505432,
            "bert": 0.8910276073217392
        },
        "Whole dataset (GQA)": {
            "bart": -5.20392674446106,
            "bert": 0.9936433392763138
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.55544909954071,
            "bert": 0.8022668886184693
        },
        "Whole dataset (INAT)": {
            "bart": -6.0292117261886595,
            "bert": 0.7869196051359176
        },
        "Whole dataset (IRFL)": {
            "bart": -4.7465829753875735,
            "bert": 0.9986189579963685
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.449219634532929,
            "bert": 0.8508744293451309
        },
        "Whole dataset (Memotion)": {
            "bart": -4.3967698240280155,
            "bert": 0.8968474072217941
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.607713258266449,
            "bert": 0.8505342888832093
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.842746548652649,
            "bert": 0.9966597068309784
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5756925201416014,
            "bert": 0.9197150671482086
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.500995669364929,
            "bert": 0.9458074790239334
        },
        "Whole dataset (OpenPath)": {
            "bart": -7.045802793502808,
            "bert": 0.8291384136676788
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.586766939163208,
            "bert": 0.9106304103136063
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.672520117759705,
            "bert": 0.9099518364667892
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.7496633005142215,
            "bert": 0.8620811170339584
        },
        "Whole dataset (Slake)": {
            "bart": -5.61407361984253,
            "bert": 0.9965922611951828
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.651424981355667,
            "bert": 0.9264709281921387
        },
        "Whole dataset (VCR)": {
            "bart": -6.02413685798645,
            "bert": 0.8690136277675629
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.1274914395809175,
            "bert": 0.9048829364776612
        },
        "Whole dataset (VQA)": {
            "bart": -6.973018846511841,
            "bert": 0.972731391787529
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.033428936004639,
            "bert": 0.9367630249261856
        },
        "Whole dataset (Winoground)": {
            "bart": -3.8897518157958983,
            "bert": 0.9980263519287109
        },
        "random (POPE)": {
            "acc": 0.504885993485342,
            "prec": 0.4863013698630137,
            "rec": 0.4797297297297297,
            "f1": 0.48299319727891155
        },
        "popular (POPE)": {
            "acc": 0.48534201954397393,
            "prec": 0.4968944099378882,
            "rec": 0.5095541401273885,
            "f1": 0.5031446540880504
        },
        "adversarial (POPE)": {
            "acc": 0.4755244755244755,
            "prec": 0.4383561643835616,
            "rec": 0.48484848484848486,
            "f1": 0.460431654676259
        }
    },
    "llama3-instruct+8b+siglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6222843622284362
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.22554890219560877
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2559598494353827
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.24151696606786427
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.236
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5651818553330609
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2522448979591837
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.30114942528735633
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.2837301587301587
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2584493041749503
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5715746421267893
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6811272957568081
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6477334789732386
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.26582278481012656
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2435129740518962
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4992389649923896
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.20938628158844766
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.22776148582600195
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6888217522658611
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.175
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.2641509433962264
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.21818181818181817
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.24120603015075376
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.2653061224489796
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.20987654320987653
        },
        "celebrity (MME)": {
            "acc": 0.8441176470588235,
            "prec": 0.8502994011976048,
            "rec": 0.8352941176470589,
            "f1": 0.8427299703264095
        },
        "posters (MME)": {
            "acc": 0.1564625850340136,
            "prec": 0.19760479041916168,
            "rec": 0.22448979591836735,
            "f1": 0.21019108280254778
        },
        "position (MME)": {
            "acc": 0.75,
            "prec": 0.6923076923076923,
            "rec": 0.9,
            "f1": 0.7826086956521738
        },
        "scene (MME)": {
            "acc": 0.1125,
            "prec": 0.14285714285714285,
            "rec": 0.155,
            "f1": 0.14868105515587532
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7214285714285714,
            "prec": 0.691358024691358,
            "rec": 0.8,
            "f1": 0.7417218543046357
        },
        "artwork (MME)": {
            "acc": 0.66,
            "prec": 0.6052631578947368,
            "rec": 0.92,
            "f1": 0.7301587301587301
        },
        "landmark (MME)": {
            "acc": 0.1825,
            "prec": 0.06206896551724138,
            "rec": 0.045,
            "f1": 0.052173913043478265
        },
        "text_translation (MME)": {
            "acc": 0.55,
            "prec": 0.6,
            "rec": 0.3,
            "f1": 0.4
        },
        "existence (MME)": {
            "acc": 0.05,
            "prec": 0.09090909090909091,
            "rec": 0.1,
            "f1": 0.09523809523809525
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.45454545454545453,
            "rec": 0.25,
            "f1": 0.3225806451612903
        },
        "count (MME)": {
            "acc": 0.8833333333333333,
            "prec": 0.8285714285714286,
            "rec": 0.9666666666666667,
            "f1": 0.8923076923076922
        },
        "color (MME)": {
            "acc": 0.9,
            "prec": 0.875,
            "rec": 0.9333333333333333,
            "f1": 0.9032258064516129
        },
        "OCR (MME)": {
            "acc": 0.475,
            "prec": 0.46153846153846156,
            "rec": 0.3,
            "f1": 0.3636363636363637
        },
        "code_reasoning (MME)": {
            "acc": 0.35,
            "prec": 0.375,
            "rec": 0.45,
            "f1": 0.4090909090909091
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.19767441860465115
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.273015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.18461538461538463
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2831050228310502
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.24022346368715083
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.28837209302325584
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.255050505050505
        },
        "ocr (MMBench_CN)": {
            "acc": 0.2692307692307692
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.1807909604519774
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2978723404255319
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.27631578947368424
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.20454545454545456
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.22340425531914893
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24056603773584906
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.25757575757575757
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.20666666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2642857142857143
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.19767441860465115
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2920634920634921
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23846153846153847
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2831050228310502
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.28368794326241137
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.19553072625698323
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.24494949494949494
        },
        "ocr (MMBench_EN)": {
            "acc": 0.27564102564102566
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2598870056497175
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2801418439716312
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.27631578947368424
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_EN)": {
            "acc": 0.23113207547169812
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2571428571428571
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.2
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.2
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.2
        },
        "Sociology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art_Theory (MMMU)": {
            "acc": 0.23333333333333334
        },
        "History (MMMU)": {
            "acc": 0.3
        },
        "Materials (MMMU)": {
            "acc": 0.3
        },
        "Geography (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Chemistry (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.4
        },
        "Psychology (MMMU)": {
            "acc": 0.2
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.4
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Music (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Finance (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.2
        },
        "Design (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Literature (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.1885245901639344
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1746031746031746
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.22875816993464052
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.2
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.20454545454545456
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3287671232876712
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.3076923076923077
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3974358974358974
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.2602739726027397
        },
        "States of matter (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Materials (ScienceQA)": {
            "acc": 0.3706293706293706
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.32653061224489793
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3902439024390244
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.2549019607843137
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.3125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.4074074074074074
        },
        "Maps (ScienceQA)": {
            "acc": 0.32608695652173914
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Classification (ScienceQA)": {
            "acc": 0.4177215189873418
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.38028169014084506
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2631578947368421
        },
        "2D Count (CVBench)": {
            "acc": 0.4035532994923858
        },
        "3D Distance (CVBench)": {
            "acc": 0.355
        },
        "2D Relation (CVBench)": {
            "acc": 0.3169230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.2866666666666667
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.763906674385071,
            "bert": 0.7934872144460678
        },
        "Whole dataset (Enrico)": {
            "bart": -6.9619982409477235,
            "bert": 0.9788361579179764
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.846303772926331,
            "bert": 0.9588178408145904
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.909580888748169,
            "bert": 0.9037001252174377
        },
        "Whole dataset (GQA)": {
            "bart": -5.509910101890564,
            "bert": 0.9936585998535157
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -4.95838089466095,
            "bert": 0.8503827404975891
        },
        "Whole dataset (INAT)": {
            "bart": -6.056732196807861,
            "bert": 0.7858314126729965
        },
        "Whole dataset (IRFL)": {
            "bart": -4.671355495452881,
            "bert": 0.9985889708995819
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.41177128314972,
            "bert": 0.8502461445331574
        },
        "Whole dataset (Memotion)": {
            "bart": -4.114907243251801,
            "bert": 0.9047465938329696
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.48348700761795,
            "bert": 0.84724458694458
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.842746548652649,
            "bert": 0.9967331063747406
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5464731669425964,
            "bert": 0.9325703293085098
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.503778758049012,
            "bert": 0.9391826272010804
        },
        "Whole dataset (OpenPath)": {
            "bart": -7.992671031951904,
            "bert": 0.8544965171813965
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.344660243988037,
            "bert": 0.90889220058918
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.8023119640350345,
            "bert": 0.9133066266775132
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.7368369388580325,
            "bert": 0.8632154273986816
        },
        "Whole dataset (Slake)": {
            "bart": -5.738975348472596,
            "bert": 0.9932976269721985
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.799890379905701,
            "bert": 0.9173833113908768
        },
        "Whole dataset (VCR)": {
            "bart": -5.682575211524964,
            "bert": 0.8971442872285843
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.111216475963593,
            "bert": 0.9077528160810471
        },
        "Whole dataset (VQA)": {
            "bart": -7.049264817237854,
            "bert": 0.9758879762887954
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.239649558067322,
            "bert": 0.9405032074451447
        },
        "Whole dataset (Winoground)": {
            "bart": -3.9465974140167237,
            "bert": 0.9980646002292634
        },
        "random (POPE)": {
            "acc": 0.46579804560260585,
            "prec": 0.44285714285714284,
            "rec": 0.4189189189189189,
            "f1": 0.4305555555555556
        },
        "popular (POPE)": {
            "acc": 0.50814332247557,
            "prec": 0.5185185185185185,
            "rec": 0.535031847133758,
            "f1": 0.5266457680250783
        },
        "adversarial (POPE)": {
            "acc": 0.47202797202797203,
            "prec": 0.43448275862068964,
            "rec": 0.4772727272727273,
            "f1": 0.4548736462093863
        }
    },
    "llama3-instruct+8b+dinosiglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5855022585502259
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.22355289421157684
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2685069008782936
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2315369261477046
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.236
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5561912545974663
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23510204081632652
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.3057471264367816
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24603174603174602
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.24983432736911862
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5623721881390593
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6597938144329897
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6583280557314756
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.624795193883124
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4885844748858447
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.24548736462093862
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2482893450635386
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6646525679758308
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.2
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.22424242424242424
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.2562814070351759
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.7676470588235295,
            "prec": 0.7570621468926554,
            "rec": 0.788235294117647,
            "f1": 0.7723342939481268
        },
        "posters (MME)": {
            "acc": 0.20748299319727892,
            "prec": 0.23125,
            "rec": 0.25170068027210885,
            "f1": 0.241042345276873
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6590909090909091,
            "rec": 0.9666666666666667,
            "f1": 0.7837837837837838
        },
        "scene (MME)": {
            "acc": 0.1375,
            "prec": 0.15639810426540285,
            "rec": 0.165,
            "f1": 0.16058394160583944
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6928571428571428,
            "prec": 0.6708860759493671,
            "rec": 0.7571428571428571,
            "f1": 0.7114093959731544
        },
        "artwork (MME)": {
            "acc": 0.6225,
            "prec": 0.5749235474006116,
            "rec": 0.94,
            "f1": 0.713472485768501
        },
        "landmark (MME)": {
            "acc": 0.17,
            "prec": 0.10714285714285714,
            "rec": 0.09,
            "f1": 0.09782608695652174
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "existence (MME)": {
            "acc": 0.016666666666666666,
            "prec": 0.03225806451612903,
            "rec": 0.03333333333333333,
            "f1": 0.03278688524590164
        },
        "numerical_calculation (MME)": {
            "acc": 0.425,
            "prec": 0.38461538461538464,
            "rec": 0.25,
            "f1": 0.30303030303030304
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7428571428571429,
            "rec": 0.8666666666666667,
            "f1": 0.8
        },
        "color (MME)": {
            "acc": 0.75,
            "prec": 0.6829268292682927,
            "rec": 0.9333333333333333,
            "f1": 0.7887323943661972
        },
        "OCR (MME)": {
            "acc": 0.45,
            "prec": 0.42857142857142855,
            "rec": 0.3,
            "f1": 0.3529411764705882
        },
        "code_reasoning (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 1.0,
            "f1": 0.6666666666666666
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22093023255813954
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.273015873015873
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2876712328767123
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2695035460992908
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2011173184357542
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.29767441860465116
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.255050505050505
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.20903954802259886
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2695035460992908
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.23
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.2598684210526316
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.18085106382978725
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24056603773584906
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.25
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.18
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.24285714285714285
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.20930232558139536
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2920634920634921
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2602739726027397
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.3120567375886525
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.18435754189944134
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.27906976744186046
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23095823095823095
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.25
        },
        "ocr (MMBench_EN)": {
            "acc": 0.23076923076923078
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.21468926553672316
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2375886524822695
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.24671052631578946
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.19148936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2358490566037736
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.24285714285714285
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.2
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Sociology (MMMU)": {
            "acc": 0.3
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.2
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Art (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Manage (MMMU)": {
            "acc": 0.3
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Music (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Biology (MMMU)": {
            "acc": 0.3
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Computer_Science (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Agriculture (MMMU)": {
            "acc": 0.03333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18442622950819673
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1984126984126984
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1895424836601307
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.21176470588235294
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.23863636363636365
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3150684931506849
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3814102564102564
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.2602739726027397
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.3356643356643357
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.32653061224489793
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3225806451612903
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.24390243902439024
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.2909090909090909
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.27450980392156865
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.37777777777777777
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.28125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.42592592592592593
        },
        "Maps (ScienceQA)": {
            "acc": 0.3695652173913043
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3684210526315789
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Classification (ScienceQA)": {
            "acc": 0.3924050632911392
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.3793103448275862
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.28169014084507044
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.42105263157894735
        },
        "2D Count (CVBench)": {
            "acc": 0.37944162436548223
        },
        "3D Distance (CVBench)": {
            "acc": 0.3433333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.3169230769230769
        },
        "3D Depth (CVBench)": {
            "acc": 0.31333333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.817237942218781,
            "bert": 0.7695100855827331
        },
        "Whole dataset (Enrico)": {
            "bart": -6.4967978167533875,
            "bert": 0.9854679071903228
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -5.651264262199402,
            "bert": 0.9862868952751159
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.920702664852142,
            "bert": 0.901376650929451
        },
        "Whole dataset (GQA)": {
            "bart": -5.679879727363587,
            "bert": 0.9936032199859619
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -3.9865180110931395,
            "bert": 0.951272469162941
        },
        "Whole dataset (INAT)": {
            "bart": -6.15656174659729,
            "bert": 0.771566488146782
        },
        "Whole dataset (IRFL)": {
            "bart": -4.6748087692260745,
            "bert": 0.9986886310577393
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.44527941942215,
            "bert": 0.8504176431894303
        },
        "Whole dataset (Memotion)": {
            "bart": -4.033123626708984,
            "bert": 0.9085055100917816
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.509569652080536,
            "bert": 0.8825241446495056
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.870569815635681,
            "bert": 0.9951088309288025
        },
        "Whole dataset (NLVR)": {
            "bart": -3.5207868957519532,
            "bert": 0.9992846250534058
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.52507404088974,
            "bert": 0.9316787612438202
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.461676058769226,
            "bert": 0.9391717296838761
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.206233053207398,
            "bert": 0.8424942284822464
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.495111427307129,
            "bert": 0.9069839698076249
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.799505882263183,
            "bert": 0.9178423708677292
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.775448436737061,
            "bert": 0.860700996518135
        },
        "Whole dataset (Slake)": {
            "bart": -5.663246474266052,
            "bert": 0.9946827870607376
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.703203313350677,
            "bert": 0.9280874556303025
        },
        "Whole dataset (VCR)": {
            "bart": -5.784484536647796,
            "bert": 0.8982565611600876
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.15296795129776,
            "bert": 0.9072894316911697
        },
        "Whole dataset (VQA)": {
            "bart": -7.081474113464355,
            "bert": 0.9786337864398956
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.2517543172836305,
            "bert": 0.9402368724346161
        },
        "Whole dataset (Winoground)": {
            "bart": -3.778929452896118,
            "bert": 0.9980198878049851
        },
        "random (POPE)": {
            "acc": 0.4788273615635179,
            "prec": 0.4594594594594595,
            "rec": 0.4594594594594595,
            "f1": 0.4594594594594595
        },
        "popular (POPE)": {
            "acc": 0.4820846905537459,
            "prec": 0.4934210526315789,
            "rec": 0.47770700636942676,
            "f1": 0.4854368932038835
        },
        "adversarial (POPE)": {
            "acc": 0.46503496503496505,
            "prec": 0.42758620689655175,
            "rec": 0.4696969696969697,
            "f1": 0.4476534296028881
        }
    },
    "mistral-instruct-v0.2+7b+clip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.6478812647881265
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.2654690618762475
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2603513174404015
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.23952095808383234
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.238
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5586432366162648
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.24571428571428572
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2942528735632184
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25396825396825395
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.27037773359840955
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5603271983640081
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.6288659793814433
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.675427485750475
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.6253413435281268
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.17424242424242425
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2894211576846307
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4977168949771689
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.22743682310469315
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.260019550342131
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6797583081570997
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.175
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.24528301886792453
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.23030303030303031
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23115577889447236
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.12244897959183673
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2962962962962963
        },
        "celebrity (MME)": {
            "acc": 0.6764705882352942,
            "prec": 0.6229508196721312,
            "rec": 0.8941176470588236,
            "f1": 0.7342995169082125
        },
        "posters (MME)": {
            "acc": 0.13945578231292516,
            "prec": 0.1513157894736842,
            "rec": 0.1564625850340136,
            "f1": 0.15384615384615383
        },
        "position (MME)": {
            "acc": 0.7166666666666667,
            "prec": 0.6444444444444445,
            "rec": 0.9666666666666667,
            "f1": 0.7733333333333334
        },
        "scene (MME)": {
            "acc": 0.1375,
            "prec": 0.16589861751152074,
            "rec": 0.18,
            "f1": 0.1726618705035971
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.7,
            "prec": 0.6627906976744186,
            "rec": 0.8142857142857143,
            "f1": 0.7307692307692307
        },
        "artwork (MME)": {
            "acc": 0.635,
            "prec": 0.59,
            "rec": 0.885,
            "f1": 0.708
        },
        "landmark (MME)": {
            "acc": 0.185,
            "prec": 0.22123893805309736,
            "rec": 0.25,
            "f1": 0.2347417840375587
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6666666666666666,
            "rec": 0.1,
            "f1": 0.1739130434782609
        },
        "existence (MME)": {
            "acc": 0.03333333333333333,
            "prec": 0.0625,
            "rec": 0.06666666666666667,
            "f1": 0.06451612903225808
        },
        "numerical_calculation (MME)": {
            "acc": 0.4,
            "prec": 0.4444444444444444,
            "rec": 0.8,
            "f1": 0.5714285714285714
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7878787878787878,
            "rec": 0.8666666666666667,
            "f1": 0.8253968253968254
        },
        "color (MME)": {
            "acc": 0.9166666666666666,
            "prec": 0.8787878787878788,
            "rec": 0.9666666666666667,
            "f1": 0.9206349206349207
        },
        "OCR (MME)": {
            "acc": 0.525,
            "prec": 0.5714285714285714,
            "rec": 0.2,
            "f1": 0.29629629629629634
        },
        "code_reasoning (MME)": {
            "acc": 0.525,
            "prec": 0.5128205128205128,
            "rec": 1.0,
            "f1": 0.6779661016949152
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.23255813953488372
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2984126984126984
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2153846153846154
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2876712328767123
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.19148936170212766
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.19553072625698323
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26046511627906976
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24078624078624078
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.25757575757575757
        },
        "ocr (MMBench_CN)": {
            "acc": 0.21153846153846154
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24858757062146894
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.28368794326241137
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.23
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.29276315789473684
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.20454545454545456
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "image_style (MMBench_CN)": {
            "acc": 0.20754716981132076
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2765151515151515
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.24666666666666667
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.25
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.19186046511627908
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2984126984126984
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.24615384615384617
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2831050228310502
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.24822695035460993
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22905027932960895
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2744186046511628
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23587223587223588
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.24242424242424243
        },
        "ocr (MMBench_EN)": {
            "acc": 0.21794871794871795
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.28368794326241137
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.23
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2565789473684211
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.22727272727272727
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.19148936170212766
        },
        "image_style (MMBench_EN)": {
            "acc": 0.2358490566037736
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.25333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.25
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Math (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Pharmacy (MMMU)": {
            "acc": 0.3
        },
        "Public_Health (MMMU)": {
            "acc": 0.2
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Sociology (MMMU)": {
            "acc": 0.2
        },
        "Art_Theory (MMMU)": {
            "acc": 0.26666666666666666
        },
        "History (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.03333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Electronics (MMMU)": {
            "acc": 0.1
        },
        "Economics (MMMU)": {
            "acc": 0.4
        },
        "Art (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Accounting (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Psychology (MMMU)": {
            "acc": 0.1
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Manage (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Music (MMMU)": {
            "acc": 0.43333333333333335
        },
        "Finance (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Marketing (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.19672131147540983
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.1984126984126984
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.24836601307189543
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16470588235294117
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18627450980392157
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.1590909090909091
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3561643835616438
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.4358974358974359
        },
        "State capitals (ScienceQA)": {
            "acc": 0.38782051282051283
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3150684931506849
        },
        "States of matter (ScienceQA)": {
            "acc": 0.32142857142857145
        },
        "Materials (ScienceQA)": {
            "acc": 0.32867132867132864
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.336734693877551
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.41935483870967744
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.3170731707317073
        },
        "Geography (ScienceQA)": {
            "acc": 0.47619047619047616
        },
        "Magnets (ScienceQA)": {
            "acc": 0.33636363636363636
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.26666666666666666
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.34782608695652173
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.36363636363636365
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.38461538461538464
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.1875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Classification (ScienceQA)": {
            "acc": 0.3670886075949367
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "2D Count (CVBench)": {
            "acc": 0.3883248730964467
        },
        "3D Distance (CVBench)": {
            "acc": 0.35333333333333333
        },
        "2D Relation (CVBench)": {
            "acc": 0.30615384615384617
        },
        "3D Depth (CVBench)": {
            "acc": 0.30833333333333335
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.7247490978240965,
            "bert": 0.7771976828575134
        },
        "Whole dataset (Enrico)": {
            "bart": -7.226881318092346,
            "bert": 0.8155084383487702
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.3422941303253175,
            "bert": 0.8103580224514008
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.008602173328399,
            "bert": 0.8848088955879212
        },
        "Whole dataset (GQA)": {
            "bart": -5.535493350028991,
            "bert": 0.9923813688755035
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -5.892513365745544,
            "bert": 0.8001007580757141
        },
        "Whole dataset (INAT)": {
            "bart": -5.950835614204407,
            "bert": 0.7930067813396454
        },
        "Whole dataset (IRFL)": {
            "bart": -4.5071408081054685,
            "bert": 0.9986613368988038
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.436444485187531,
            "bert": 0.849782303571701
        },
        "Whole dataset (Memotion)": {
            "bart": -4.967589061260224,
            "bert": 0.8595340955257416
        },
        "Whole dataset (MMIMDB)": {
            "bart": -5.030960946083069,
            "bert": 0.7569816744327545
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.6162693977355955,
            "bert": 0.806529438495636
        },
        "Whole dataset (NLVR)": {
            "bart": -4.681194844245911,
            "bert": 0.9539119881391526
        },
        "Whole dataset (NLVR2)": {
            "bart": -4.742853126525879,
            "bert": 0.9614698672294617
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.5114824318885804,
            "bert": 0.922508465051651
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.541580972671508,
            "bert": 0.9376634377241134
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.740706882476807,
            "bert": 0.8072511780261994
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.24279568195343,
            "bert": 0.9122159910202027
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.330772137641906,
            "bert": 0.7946678572893142
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.783227624893189,
            "bert": 0.8599604278802871
        },
        "Whole dataset (Slake)": {
            "bart": -5.2707976770401,
            "bert": 0.9939226883649827
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.582086691856384,
            "bert": 0.8031490713357925
        },
        "Whole dataset (VCR)": {
            "bart": -5.5986784958839415,
            "bert": 0.8816261392831802
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.073474713563919,
            "bert": 0.9058665597438812
        },
        "Whole dataset (VQA)": {
            "bart": -7.067245769500732,
            "bert": 0.9731064081192017
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.252588763236999,
            "bert": 0.9394842565059662
        },
        "Whole dataset (Winoground)": {
            "bart": -3.737012462615967,
            "bert": 0.9980304825305939
        },
        "random (POPE)": {
            "acc": 0.504885993485342,
            "prec": 0.487012987012987,
            "rec": 0.5067567567567568,
            "f1": 0.49668874172185434
        },
        "popular (POPE)": {
            "acc": 0.48859934853420195,
            "prec": 0.5,
            "rec": 0.5222929936305732,
            "f1": 0.5109034267912773
        },
        "adversarial (POPE)": {
            "acc": 0.48951048951048953,
            "prec": 0.4520547945205479,
            "rec": 0.5,
            "f1": 0.474820143884892
        }
    },
    "mistral-instruct-v0.2+7b+siglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.601419660141966
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.2659974905897114
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.2634730538922156
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.226
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5537392725786677
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.23755102040816325
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.28735632183908044
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.25992063492063494
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2524850894632207
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5286298568507157
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5979381443298969
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6554781507283091
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5936646641179684
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.25316455696202533
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.25757575757575757
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.2654690618762475
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4490106544901065
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23826714801444043
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.2404692082111437
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6555891238670695
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.24166666666666667
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.27044025157232704
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.21608040201005024
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.16326530612244897
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.2839506172839506
        },
        "celebrity (MME)": {
            "acc": 0.6764705882352942,
            "prec": 0.6376146788990825,
            "rec": 0.8176470588235294,
            "f1": 0.7164948453608248
        },
        "posters (MME)": {
            "acc": 0.18027210884353742,
            "prec": 0.22674418604651161,
            "rec": 0.2653061224489796,
            "f1": 0.24451410658307207
        },
        "position (MME)": {
            "acc": 0.7333333333333333,
            "prec": 0.6590909090909091,
            "rec": 0.9666666666666667,
            "f1": 0.7837837837837838
        },
        "scene (MME)": {
            "acc": 0.145,
            "prec": 0.13402061855670103,
            "rec": 0.13,
            "f1": 0.1319796954314721
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.65,
            "prec": 0.6206896551724138,
            "rec": 0.7714285714285715,
            "f1": 0.6878980891719746
        },
        "artwork (MME)": {
            "acc": 0.64,
            "prec": 0.6196581196581197,
            "rec": 0.725,
            "f1": 0.6682027649769585
        },
        "landmark (MME)": {
            "acc": 0.235,
            "prec": 0.20224719101123595,
            "rec": 0.18,
            "f1": 0.1904761904761905
        },
        "text_translation (MME)": {
            "acc": 0.525,
            "prec": 0.6666666666666666,
            "rec": 0.1,
            "f1": 0.1739130434782609
        },
        "existence (MME)": {
            "acc": 0.03333333333333333,
            "prec": 0.03333333333333333,
            "rec": 0.03333333333333333,
            "f1": 0.03333333333333333
        },
        "numerical_calculation (MME)": {
            "acc": 0.5,
            "prec": 0.5,
            "rec": 0.2,
            "f1": 0.28571428571428575
        },
        "count (MME)": {
            "acc": 0.8166666666666667,
            "prec": 0.7567567567567568,
            "rec": 0.9333333333333333,
            "f1": 0.835820895522388
        },
        "color (MME)": {
            "acc": 0.8333333333333334,
            "prec": 0.75,
            "rec": 1.0,
            "f1": 0.8571428571428571
        },
        "OCR (MME)": {
            "acc": 0.45,
            "prec": 0.4166666666666667,
            "rec": 0.25,
            "f1": 0.3125
        },
        "code_reasoning (MME)": {
            "acc": 0.4,
            "prec": 0.4444444444444444,
            "rec": 0.8,
            "f1": 0.5714285714285714
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.23837209302325582
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2984126984126984
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.2
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2694063926940639
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.24113475177304963
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.2681564245810056
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.31627906976744186
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.24815724815724816
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.25
        },
        "ocr (MMBench_CN)": {
            "acc": 0.20512820512820512
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.23163841807909605
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.22695035460992907
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.25
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.27960526315789475
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.24468085106382978
        },
        "image_style (MMBench_CN)": {
            "acc": 0.24528301886792453
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.26136363636363635
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.22
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.2357142857142857
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.18023255813953487
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2920634920634921
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.2153846153846154
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.3059360730593607
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.2624113475177305
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.22346368715083798
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.31627906976744186
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.23832923832923833
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.25
        },
        "ocr (MMBench_EN)": {
            "acc": 0.20512820512820512
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2711864406779661
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2553191489361702
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.2565789473684211
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.2159090909090909
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.21226415094339623
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.24242424242424243
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.22666666666666666
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.2357142857142857
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Math (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Physics (MMMU)": {
            "acc": 0.3
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Sociology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art_Theory (MMMU)": {
            "acc": 0.2
        },
        "History (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.2
        },
        "Geography (MMMU)": {
            "acc": 0.06666666666666667
        },
        "Chemistry (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Art (MMMU)": {
            "acc": 0.2
        },
        "Accounting (MMMU)": {
            "acc": 0.3333333333333333
        },
        "Psychology (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.2
        },
        "Music (MMMU)": {
            "acc": 0.4
        },
        "Finance (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Literature (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Biology (MMMU)": {
            "acc": 0.2
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.3
        },
        "Computer_Science (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18032786885245902
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.16666666666666666
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.21568627450980393
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.12941176470588237
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.16176470588235295
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.17045454545454544
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.3287671232876712
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3974358974358974
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.3150684931506849
        },
        "States of matter (ScienceQA)": {
            "acc": 0.35714285714285715
        },
        "Materials (ScienceQA)": {
            "acc": 0.32167832167832167
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.336734693877551
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.4032258064516129
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.2682926829268293
        },
        "Geography (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Magnets (ScienceQA)": {
            "acc": 0.4
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.3333333333333333
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.39215686274509803
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.3103448275862069
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.2222222222222222
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.4
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.21875
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.30434782608695654
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.3157894736842105
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.3137254901960784
        },
        "Classification (ScienceQA)": {
            "acc": 0.4177215189873418
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.4482758620689655
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.3819796954314721
        },
        "3D Distance (CVBench)": {
            "acc": 0.355
        },
        "2D Relation (CVBench)": {
            "acc": 0.3046153846153846
        },
        "3D Depth (CVBench)": {
            "acc": 0.31833333333333336
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.82110132932663,
            "bert": 0.7609650534391403
        },
        "Whole dataset (Enrico)": {
            "bart": -7.504788975715638,
            "bert": 0.9180655026435852
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.224163584709167,
            "bert": 0.8215032351016999
        },
        "Whole dataset (Flickr30k)": {
            "bart": -4.01908864736557,
            "bert": 0.8779397994279862
        },
        "Whole dataset (GQA)": {
            "bart": -5.566888356208802,
            "bert": 0.9936851465702057
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.184362578392029,
            "bert": 0.7990586322546005
        },
        "Whole dataset (INAT)": {
            "bart": -5.986921663284302,
            "bert": 0.792687623500824
        },
        "Whole dataset (IRFL)": {
            "bart": -4.755189476013183,
            "bert": 0.9986548727750778
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.4131901788711545,
            "bert": 0.8482878106832504
        },
        "Whole dataset (Memotion)": {
            "bart": -5.1842290616035465,
            "bert": 0.8489167779684067
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.964044070243835,
            "bert": 0.8041308510303498
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -6.521610221862793,
            "bert": 0.8141968506574631
        },
        "Whole dataset (NLVR)": {
            "bart": -3.327454763650894,
            "bert": 0.9992384660243988
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.6431289398670197,
            "bert": 0.9988000410795211
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.6330269980430603,
            "bert": 0.907218171954155
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.53195716381073,
            "bert": 0.9404986822605133
        },
        "Whole dataset (OpenPath)": {
            "bart": -5.718090720176697,
            "bert": 0.8055515289306641
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.3245842123031615,
            "bert": 0.9116422784328461
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.610309567451477,
            "bert": 0.8356959199905396
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.7271614933013915,
            "bert": 0.8575555747747421
        },
        "Whole dataset (Slake)": {
            "bart": -5.232503790855407,
            "bert": 0.9941191238164901
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.8095721745491025,
            "bert": 0.8911070847511291
        },
        "Whole dataset (VCR)": {
            "bart": -5.541742978096008,
            "bert": 0.8919106882810592
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.135419549942017,
            "bert": 0.9057083058357239
        },
        "Whole dataset (VQA)": {
            "bart": -7.167308168411255,
            "bert": 0.9759121978282929
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.205714659690857,
            "bert": 0.9406867641210556
        },
        "Whole dataset (Winoground)": {
            "bart": -4.488649454116821,
            "bert": 0.9978742545843124
        },
        "random (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.48344370860927155,
            "rec": 0.49324324324324326,
            "f1": 0.48829431438127097
        },
        "popular (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.5126582278481012,
            "rec": 0.5159235668789809,
            "f1": 0.5142857142857142
        },
        "adversarial (POPE)": {
            "acc": 0.506993006993007,
            "prec": 0.4666666666666667,
            "rec": 0.4772727272727273,
            "f1": 0.4719101123595506
        }
    },
    "mistral-instruct-v0.2+7b+dinosiglip": {
        "Instance Attributes (SEED_2)": {
            "acc": 0.5566788556678856
        },
        "Emotion Recognition (SEED_2)": {
            "acc": 0.21956087824351297
        },
        "Global Video Understanding (SEED_2)": {
            "acc": 0.266624843161857
        },
        "Chart Understanding (SEED_2)": {
            "acc": 0.26746506986027946
        },
        "Landmark Recognition (SEED_2)": {
            "acc": 0.264
        },
        "Instances Counting (SEED_2)": {
            "acc": 0.5214548426644872
        },
        "Action Prediction (SEED_2)": {
            "acc": 0.2416326530612245
        },
        "Text Understanding (SEED_2)": {
            "acc": 0.2896551724137931
        },
        "Text-to-Image Generation (SEED_2)": {
            "acc": 0.24702380952380953
        },
        "Action Recognition (SEED_2)": {
            "acc": 0.2485089463220676
        },
        "Instance Location (SEED_2)": {
            "acc": 0.5184049079754601
        },
        "Instance Interaction (SEED_2)": {
            "acc": 0.5876288659793815
        },
        "Scene Understanding (SEED_2)": {
            "acc": 0.6323622545915136
        },
        "Instance Identity (SEED_2)": {
            "acc": 0.5838339705079192
        },
        "Text-Image Creation (SEED_2)": {
            "acc": 0.22784810126582278
        },
        "Visual Mathematics (SEED_2)": {
            "acc": 0.26515151515151514
        },
        "Difference Spotting (SEED_2)": {
            "acc": 0.25748502994011974
        },
        "Spatial Relation (SEED_2)": {
            "acc": 0.4596651445966514
        },
        "Science Knowledge (SEED_2)": {
            "acc": 0.23826714801444043
        },
        "Procedure Understanding (SEED_2)": {
            "acc": 0.24242424242424243
        },
        "Visual Reasoning (SEED_2)": {
            "acc": 0.6737160120845922
        },
        "In-Context Captioning (SEED_2)": {
            "acc": 0.15
        },
        "Meme Comprehension (SEED_2)": {
            "acc": 0.22641509433962265
        },
        "Celebrity Recognition (SEED_2)": {
            "acc": 0.24545454545454545
        },
        "Visual Referring Expression (SEED_2)": {
            "acc": 0.23618090452261306
        },
        "Interleaved Image-Text Analysis (SEED_2)": {
            "acc": 0.22448979591836735
        },
        "Next Image Prediction (SEED_2)": {
            "acc": 0.345679012345679
        },
        "celebrity (MME)": {
            "acc": 0.5882352941176471,
            "prec": 0.5535714285714286,
            "rec": 0.9117647058823529,
            "f1": 0.6888888888888888
        },
        "posters (MME)": {
            "acc": 0.30612244897959184,
            "prec": 0.35233160621761656,
            "rec": 0.46258503401360546,
            "f1": 0.4
        },
        "position (MME)": {
            "acc": 0.7,
            "prec": 0.6428571428571429,
            "rec": 0.9,
            "f1": 0.75
        },
        "scene (MME)": {
            "acc": 0.1225,
            "prec": 0.12807881773399016,
            "rec": 0.13,
            "f1": 0.12903225806451615
        },
        "commonsense_reasoning (MME)": {
            "acc": 0.6571428571428571,
            "prec": 0.627906976744186,
            "rec": 0.7714285714285715,
            "f1": 0.6923076923076923
        },
        "artwork (MME)": {
            "acc": 0.62,
            "prec": 0.5923076923076923,
            "rec": 0.77,
            "f1": 0.6695652173913044
        },
        "landmark (MME)": {
            "acc": 0.225,
            "prec": 0.27459016393442626,
            "rec": 0.335,
            "f1": 0.30180180180180183
        },
        "text_translation (MME)": {
            "acc": 0.5,
            "prec": 0.0,
            "rec": 0.0,
            "f1": 0.0
        },
        "existence (MME)": {
            "acc": 0.016666666666666666,
            "prec": 0.03225806451612903,
            "rec": 0.03333333333333333,
            "f1": 0.03278688524590164
        },
        "numerical_calculation (MME)": {
            "acc": 0.475,
            "prec": 0.4666666666666667,
            "rec": 0.35,
            "f1": 0.4
        },
        "count (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.717948717948718,
            "rec": 0.9333333333333333,
            "f1": 0.8115942028985509
        },
        "color (MME)": {
            "acc": 0.7833333333333333,
            "prec": 0.7428571428571429,
            "rec": 0.8666666666666667,
            "f1": 0.8
        },
        "OCR (MME)": {
            "acc": 0.425,
            "prec": 0.4,
            "rec": 0.3,
            "f1": 0.34285714285714286
        },
        "code_reasoning (MME)": {
            "acc": 0.475,
            "prec": 0.4857142857142857,
            "rec": 0.85,
            "f1": 0.6181818181818183
        },
        "social_relation (MMBench_CN)": {
            "acc": 0.22674418604651161
        },
        "object_localization (MMBench_CN)": {
            "acc": 0.2634920634920635
        },
        "future_prediction (MMBench_CN)": {
            "acc": 0.23076923076923078
        },
        "physical_property_reasoning (MMBench_CN)": {
            "acc": 0.2648401826484018
        },
        "attribute_comparison (MMBench_CN)": {
            "acc": 0.2127659574468085
        },
        "nature_relation (MMBench_CN)": {
            "acc": 0.329608938547486
        },
        "action_recognition (MMBench_CN)": {
            "acc": 0.26976744186046514
        },
        "image_scene (MMBench_CN)": {
            "acc": 0.25061425061425063
        },
        "celebrity_recognition (MMBench_CN)": {
            "acc": 0.2702020202020202
        },
        "ocr (MMBench_CN)": {
            "acc": 0.23717948717948717
        },
        "spatial_relationship (MMBench_CN)": {
            "acc": 0.24858757062146894
        },
        "structuralized_imagetext_understanding (MMBench_CN)": {
            "acc": 0.2801418439716312
        },
        "image_emotion (MMBench_CN)": {
            "acc": 0.235
        },
        "function_reasoning (MMBench_CN)": {
            "acc": 0.28289473684210525
        },
        "identity_reasoning (MMBench_CN)": {
            "acc": 0.21022727272727273
        },
        "physical_relation (MMBench_CN)": {
            "acc": 0.1702127659574468
        },
        "image_style (MMBench_CN)": {
            "acc": 0.21226415094339623
        },
        "attribute_recognition (MMBench_CN)": {
            "acc": 0.2537878787878788
        },
        "image_quality (MMBench_CN)": {
            "acc": 0.21333333333333335
        },
        "image_topic (MMBench_CN)": {
            "acc": 0.22857142857142856
        },
        "social_relation (MMBench_EN)": {
            "acc": 0.20348837209302326
        },
        "object_localization (MMBench_EN)": {
            "acc": 0.2698412698412698
        },
        "future_prediction (MMBench_EN)": {
            "acc": 0.24615384615384617
        },
        "physical_property_reasoning (MMBench_EN)": {
            "acc": 0.2876712328767123
        },
        "attribute_comparison (MMBench_EN)": {
            "acc": 0.22695035460992907
        },
        "nature_relation (MMBench_EN)": {
            "acc": 0.1787709497206704
        },
        "action_recognition (MMBench_EN)": {
            "acc": 0.2930232558139535
        },
        "image_scene (MMBench_EN)": {
            "acc": 0.24324324324324326
        },
        "celebrity_recognition (MMBench_EN)": {
            "acc": 0.2474747474747475
        },
        "ocr (MMBench_EN)": {
            "acc": 0.27564102564102566
        },
        "spatial_relationship (MMBench_EN)": {
            "acc": 0.2655367231638418
        },
        "structuralized_imagetext_understanding (MMBench_EN)": {
            "acc": 0.2801418439716312
        },
        "image_emotion (MMBench_EN)": {
            "acc": 0.225
        },
        "function_reasoning (MMBench_EN)": {
            "acc": 0.27960526315789475
        },
        "identity_reasoning (MMBench_EN)": {
            "acc": 0.20454545454545456
        },
        "physical_relation (MMBench_EN)": {
            "acc": 0.23404255319148937
        },
        "image_style (MMBench_EN)": {
            "acc": 0.20754716981132076
        },
        "attribute_recognition (MMBench_EN)": {
            "acc": 0.25757575757575757
        },
        "image_quality (MMBench_EN)": {
            "acc": 0.21333333333333335
        },
        "image_topic (MMBench_EN)": {
            "acc": 0.24285714285714285
        },
        "Mechanical_Engineering (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Basic_Medical_Science (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Math (MMMU)": {
            "acc": 0.1
        },
        "Pharmacy (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Public_Health (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Physics (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Energy_and_Power (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Sociology (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Art_Theory (MMMU)": {
            "acc": 0.36666666666666664
        },
        "History (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Materials (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Geography (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Chemistry (MMMU)": {
            "acc": 0.1
        },
        "Electronics (MMMU)": {
            "acc": 0.13333333333333333
        },
        "Economics (MMMU)": {
            "acc": 0.3
        },
        "Art (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Accounting (MMMU)": {
            "acc": 0.5
        },
        "Psychology (MMMU)": {
            "acc": 0.3
        },
        "Architecture_and_Engineering (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Manage (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Clinical_Medicine (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Music (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Finance (MMMU)": {
            "acc": 0.26666666666666666
        },
        "Marketing (MMMU)": {
            "acc": 0.16666666666666666
        },
        "Design (MMMU)": {
            "acc": 0.36666666666666664
        },
        "Literature (MMMU)": {
            "acc": 0.1
        },
        "Biology (MMMU)": {
            "acc": 0.1
        },
        "Diagnostics_and_Laboratory_Medicine (MMMU)": {
            "acc": 0.23333333333333334
        },
        "Computer_Science (MMMU)": {
            "acc": 0.2
        },
        "Agriculture (MMMU)": {
            "acc": 0.13333333333333333
        },
        "\u6280\u672f\u4e0e\u5de5\u7a0b (CMMMU)": {
            "acc": 0.18442622950819673
        },
        "\u5546\u4e1a (CMMMU)": {
            "acc": 0.18253968253968253
        },
        "\u5065\u5eb7\u4e0e\u533b\u5b66 (CMMMU)": {
            "acc": 0.1895424836601307
        },
        "\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 (CMMMU)": {
            "acc": 0.18823529411764706
        },
        "\u79d1\u5b66 (CMMMU)": {
            "acc": 0.23039215686274508
        },
        "\u827a\u672f\u4e0e\u8bbe\u8ba1 (CMMMU)": {
            "acc": 0.22727272727272727
        },
        "Ecosystems (ScienceQA)": {
            "acc": 0.2328767123287671
        },
        "English colonies in North America (ScienceQA)": {
            "acc": 0.46153846153846156
        },
        "State capitals (ScienceQA)": {
            "acc": 0.3942307692307692
        },
        "Designing experiments (ScienceQA)": {
            "acc": 0.273972602739726
        },
        "States of matter (ScienceQA)": {
            "acc": 0.4642857142857143
        },
        "Materials (ScienceQA)": {
            "acc": 0.3356643356643357
        },
        "Adaptations (ScienceQA)": {
            "acc": 0.3877551020408163
        },
        "Velocity, acceleration, and forces (ScienceQA)": {
            "acc": 0.3870967741935484
        },
        "Particle motion and energy (ScienceQA)": {
            "acc": 0.34146341463414637
        },
        "Geography (ScienceQA)": {
            "acc": 0.2857142857142857
        },
        "Magnets (ScienceQA)": {
            "acc": 0.34545454545454546
        },
        "Astronomy (ScienceQA)": {
            "acc": 0.4166666666666667
        },
        "Oceania: geography (ScienceQA)": {
            "acc": 0.4117647058823529
        },
        "Weather and climate (ScienceQA)": {
            "acc": 0.4827586206896552
        },
        "The Americas: geography (ScienceQA)": {
            "acc": 0.5111111111111111
        },
        "Classification and scientific names (ScienceQA)": {
            "acc": 0.391304347826087
        },
        "Engineering practices (ScienceQA)": {
            "acc": 0.38181818181818183
        },
        "Atoms and molecules (ScienceQA)": {
            "acc": 0.41025641025641024
        },
        "Scientific names (ScienceQA)": {
            "acc": 0.125
        },
        "Solutions (ScienceQA)": {
            "acc": 0.3888888888888889
        },
        "Maps (ScienceQA)": {
            "acc": 0.2391304347826087
        },
        "Genes to traits (ScienceQA)": {
            "acc": 0.34210526315789475
        },
        "Physical Geography (ScienceQA)": {
            "acc": 0.37254901960784315
        },
        "Classification (ScienceQA)": {
            "acc": 0.3924050632911392
        },
        "Rocks and minerals (ScienceQA)": {
            "acc": 0.41379310344827586
        },
        "Basic economic principles (ScienceQA)": {
            "acc": 0.30985915492957744
        },
        "Colonial America (ScienceQA)": {
            "acc": 0.2894736842105263
        },
        "2D Count (CVBench)": {
            "acc": 0.3895939086294416
        },
        "3D Distance (CVBench)": {
            "acc": 0.35
        },
        "2D Relation (CVBench)": {
            "acc": 0.32461538461538464
        },
        "3D Depth (CVBench)": {
            "acc": 0.31
        },
        "Whole dataset (DECIMER)": {
            "bart": -4.643286542892456,
            "bert": 0.7699072068929672
        },
        "Whole dataset (Enrico)": {
            "bart": -7.595220646858215,
            "bert": 0.9132247596979142
        },
        "Whole dataset (FaceEmotion)": {
            "bart": -6.327345850467682,
            "bert": 0.8647164136171341
        },
        "Whole dataset (Flickr30k)": {
            "bart": -3.983147535324097,
            "bert": 0.8762887370586395
        },
        "Whole dataset (GQA)": {
            "bart": -5.58413649559021,
            "bert": 0.9937129771709442
        },
        "Whole dataset (HatefulMemes)": {
            "bart": -6.083436408042908,
            "bert": 0.8007804572582244
        },
        "Whole dataset (INAT)": {
            "bart": -5.987882385253906,
            "bert": 0.7922792494297027
        },
        "Whole dataset (IRFL)": {
            "bart": -4.670771055221557,
            "bert": 0.9985767614841461
        },
        "Whole dataset (MemeCaps)": {
            "bart": -4.4273175168037415,
            "bert": 0.8497138631343841
        },
        "Whole dataset (Memotion)": {
            "bart": -4.561908392906189,
            "bert": 0.9000741440057755
        },
        "Whole dataset (MMIMDB)": {
            "bart": -4.551118395328522,
            "bert": 0.821994041800499
        },
        "Whole dataset (NewYorkerCartoon)": {
            "bart": -4.860753927230835,
            "bert": 0.9967331063747406
        },
        "Whole dataset (NLVR)": {
            "bart": -3.7235507106781007,
            "bert": 0.981741349697113
        },
        "Whole dataset (NLVR2)": {
            "bart": -3.7453905296325685,
            "bert": 0.9993045687675476
        },
        "Whole dataset (NoCaps)": {
            "bart": -3.623836491107941,
            "bert": 0.9076088064908981
        },
        "Whole dataset (OKVQA)": {
            "bart": -6.504324526786804,
            "bert": 0.9435853385925292
        },
        "Whole dataset (OpenPath)": {
            "bart": -6.58864164352417,
            "bert": 0.8251305425167084
        },
        "Whole dataset (PathVQA)": {
            "bart": -6.3642102813720705,
            "bert": 0.9142903774976731
        },
        "Whole dataset (Resisc45)": {
            "bart": -6.498260374069214,
            "bert": 0.8191652899980545
        },
        "Whole dataset (Screen2Words)": {
            "bart": -6.81128221988678,
            "bert": 0.8609997075796127
        },
        "Whole dataset (Slake)": {
            "bart": -5.5609093761444095,
            "bert": 0.9896251076459884
        },
        "Whole dataset (UCMerced)": {
            "bart": -6.726863868236542,
            "bert": 0.8773936659097672
        },
        "Whole dataset (VCR)": {
            "bart": -5.781501891613007,
            "bert": 0.8860620826482772
        },
        "Whole dataset (VisualGenome)": {
            "bart": -5.132479386329651,
            "bert": 0.90520791888237
        },
        "Whole dataset (VQA)": {
            "bart": -7.099404840469361,
            "bert": 0.9741754657030106
        },
        "Whole dataset (VQARAD)": {
            "bart": -6.423404035568237,
            "bert": 0.9373928064107895
        },
        "Whole dataset (Winoground)": {
            "bart": -3.737012462615967,
            "bert": 0.9980130642652512
        },
        "random (POPE)": {
            "acc": 0.49185667752442996,
            "prec": 0.47333333333333333,
            "rec": 0.4797297297297297,
            "f1": 0.47651006711409394
        },
        "popular (POPE)": {
            "acc": 0.501628664495114,
            "prec": 0.5126582278481012,
            "rec": 0.5159235668789809,
            "f1": 0.5142857142857142
        },
        "adversarial (POPE)": {
            "acc": 0.4965034965034965,
            "prec": 0.4583333333333333,
            "rec": 0.5,
            "f1": 0.4782608695652174
        }
    }
}